{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.models.optical_flow.raft import ResidualBlock\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:32.949052Z",
     "start_time": "2024-06-19T06:31:32.944257Z"
    }
   },
   "id": "685462a85f2ae1e",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.070037Z",
     "start_time": "2024-06-19T06:31:33.066091Z"
    }
   },
   "id": "8fb89e7978c49f98",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bdd2924b375a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA1(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(576, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "            \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.084989Z",
     "start_time": "2024-06-19T06:31:33.079511Z"
    }
   },
   "id": "8e067a677d103ee6",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA2(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=12, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(300, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.093121Z",
     "start_time": "2024-06-19T06:31:33.086998Z"
    }
   },
   "id": "4f398a6ac2c01f29",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99360d229c0dbc75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1, 5, 5])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization_weights = torch.tensor([\n",
    "    [[\n",
    "        [1, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 1, 0], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [0, 1, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 1]\n",
    "    ]],\n",
    "    [[\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [0, 0, 1, 0, 0]\n",
    "    ]],\n",
    "    [[\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [1, 0, 0, 0, 1], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [0, 1, 1, 1, 0]\n",
    "    ]],\n",
    "    [[\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [0, 1, 0, 1, 0], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [1, 1, 0, 1, 1]\n",
    "    ]]], dtype=torch.float32)\n",
    "\n",
    "initialization_biases = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "initialization_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.105454Z",
     "start_time": "2024-06-19T06:31:33.095300Z"
    }
   },
   "id": "3e5ae64e8c459822",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[ 0.0275,  0.0715,  0.1256, -0.1479, -0.1567],\n",
      "          [-0.0226,  0.0100, -0.1715, -0.1725, -0.1208],\n",
      "          [-0.0613,  0.0349,  0.0108,  0.1250, -0.1452],\n",
      "          [ 0.1867,  0.1224,  0.1494, -0.0769,  0.1487],\n",
      "          [ 0.1965,  0.1156, -0.1832,  0.0777, -0.0907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1958, -0.1091, -0.0394,  0.0192,  0.1636],\n",
      "          [ 0.0413,  0.0136, -0.0527,  0.0598, -0.0349],\n",
      "          [-0.0775,  0.1340,  0.0160,  0.0956, -0.0025],\n",
      "          [-0.0257,  0.1061,  0.1439, -0.0596,  0.0039],\n",
      "          [-0.0475, -0.0550, -0.1336,  0.0186, -0.1608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1140,  0.0929, -0.1603, -0.1158, -0.0963],\n",
      "          [ 0.0612,  0.1323, -0.0751, -0.0008,  0.1730],\n",
      "          [-0.0210,  0.0880,  0.1101,  0.0027, -0.0185],\n",
      "          [-0.1288,  0.1883,  0.1363, -0.1108, -0.1461],\n",
      "          [-0.1278,  0.1464, -0.0103,  0.1723,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0337, -0.1431, -0.0953, -0.1032, -0.0572],\n",
      "          [ 0.1921, -0.1182, -0.0249, -0.1109,  0.0914],\n",
      "          [-0.1189,  0.0667,  0.0767,  0.0571, -0.1452],\n",
      "          [ 0.0160, -0.1395, -0.0432,  0.0421,  0.0813],\n",
      "          [ 0.1571, -0.1986,  0.1900, -0.0524,  0.0951]]]])), ('conv1.bias', tensor([-0.0427,  0.0574,  0.0915, -0.0145])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n"
     ]
    }
   ],
   "source": [
    "net_a1_hf = NetA1(10)\n",
    "net_a1_ht = NetA1(10)\n",
    "net_a1_dt = NetA1(10)\n",
    "\n",
    "#set conv1 initialization of net_a1_hf\n",
    "net_a1_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a1_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a1_ht.load_state_dict(net_a1_hf.state_dict())\n",
    "for name, param in net_a1_hf.state_dict().items():\n",
    "    if \"conv1\" not in name:\n",
    "        net_a1_dt.state_dict()[name].copy_(param)\n",
    "\n",
    "#set conv1 initialization\n",
    "#net_a1_dt.conv1.load_state_dict(net_a2_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a1_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_init.pt')\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a1_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a1_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a1_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.124298Z",
     "start_time": "2024-06-19T06:31:33.106467Z"
    }
   },
   "id": "2d60961b8d0a8bc7",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[ 0.0275,  0.0715,  0.1256, -0.1479, -0.1567],\n",
      "          [-0.0226,  0.0100, -0.1715, -0.1725, -0.1208],\n",
      "          [-0.0613,  0.0349,  0.0108,  0.1250, -0.1452],\n",
      "          [ 0.1867,  0.1224,  0.1494, -0.0769,  0.1487],\n",
      "          [ 0.1965,  0.1156, -0.1832,  0.0777, -0.0907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1958, -0.1091, -0.0394,  0.0192,  0.1636],\n",
      "          [ 0.0413,  0.0136, -0.0527,  0.0598, -0.0349],\n",
      "          [-0.0775,  0.1340,  0.0160,  0.0956, -0.0025],\n",
      "          [-0.0257,  0.1061,  0.1439, -0.0596,  0.0039],\n",
      "          [-0.0475, -0.0550, -0.1336,  0.0186, -0.1608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1140,  0.0929, -0.1603, -0.1158, -0.0963],\n",
      "          [ 0.0612,  0.1323, -0.0751, -0.0008,  0.1730],\n",
      "          [-0.0210,  0.0880,  0.1101,  0.0027, -0.0185],\n",
      "          [-0.1288,  0.1883,  0.1363, -0.1108, -0.1461],\n",
      "          [-0.1278,  0.1464, -0.0103,  0.1723,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0337, -0.1431, -0.0953, -0.1032, -0.0572],\n",
      "          [ 0.1921, -0.1182, -0.0249, -0.1109,  0.0914],\n",
      "          [-0.1189,  0.0667,  0.0767,  0.0571, -0.1452],\n",
      "          [ 0.0160, -0.1395, -0.0432,  0.0421,  0.0813],\n",
      "          [ 0.1571, -0.1986,  0.1900, -0.0524,  0.0951]]]])), ('conv1.bias', tensor([-0.0427,  0.0574,  0.0915, -0.0145])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n"
     ]
    }
   ],
   "source": [
    "net_a2_hf = NetA2(10)\n",
    "net_a2_ht = NetA2(10)\n",
    "net_a2_dt = NetA2(10)\n",
    "\n",
    "#set conv1 initialization of net_a2_hf\n",
    "net_a2_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a2_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a2_ht.load_state_dict(net_a2_hf.state_dict())\n",
    "for name, param in net_a2_hf.state_dict().items():\n",
    "    if \"conv1\" not in name:\n",
    "        net_a2_dt.state_dict()[name].copy_(param)\n",
    "\n",
    "#set conv1 initialization\n",
    "net_a2_dt.conv1.load_state_dict(net_a1_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a2_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_init.pt')\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_init.pt')\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a2_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a2_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a2_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.176823Z",
     "start_time": "2024-06-19T06:31:33.146304Z"
    }
   },
   "id": "8dc214f47bd76048",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Analysys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e527a7ee28f832"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1: \n",
      " \t|W_{conv_a1_hf} - W_{conv_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A2: \n",
      " \t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A1 Vs Net_A2: \n",
      " \t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| = tensor(0.) \n",
      " \t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n"
     ]
    }
   ],
   "source": [
    "print( \"Net_A1: \\n\",\n",
    "       \"\\t|W_{conv_a1_hf} - W_{conv_a1_ht}| =\", torch.norm(net_a1_hf.conv1.weight - net_a1_ht.conv1.weight),\"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_ht}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_ht.linear1.weight), \"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_dt}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_dt.linear1.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| =\", torch.norm(net_a2_hf.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| =\", torch.norm(net_a2_hf.conv2.weight - net_a2_ht.conv2.weight),\"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_ht.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_dt.linear1.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A1 Vs Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| =\", torch.norm(net_a1_hf.conv1.weight - net_a2_hf.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| =\", torch.norm(net_a1_ht.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| =\", torch.norm(net_a1_dt.conv1.weight - net_a2_dt.conv1.weight),\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T08:21:47.718460Z",
     "start_time": "2024-06-19T08:21:47.709694Z"
    }
   },
   "id": "eabba971baa9ea0d",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A2HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A1HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A1DT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2DT:\n",
      "\t True\n",
      "\t True\n"
     ]
    }
   ],
   "source": [
    "print(\"Net_A1HF:\")\n",
    "for param in net_a1_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HF:\")\n",
    "for param in net_a2_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1HT:\")\n",
    "for param in net_a1_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HT:\")\n",
    "for param in net_a2_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1DT:\")\n",
    "for param in net_a1_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2DT:\")\n",
    "for param in net_a2_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.250492Z",
     "start_time": "2024-06-19T06:31:33.244199Z"
    }
   },
   "id": "527076a732d29933",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net_a1_ht.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a1_ht.linear1.weight is net_a1_hf.linear1.weight)\n",
    "print(net_a1_hf.linear1.weight is net_a1_dt.linear1.weight)\n",
    "print(net_a2_hf.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a2_ht.conv1.weight is net_a1_ht.conv1.weight)\n",
    "print(net_a2_dt.conv1.weight is net_a1_dt.conv1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_ht.linear1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_dt.linear1.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.273822Z",
     "start_time": "2024-06-19T06:31:33.267498Z"
    }
   },
   "id": "7f3e08ca6623fd25",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9884bc78c276b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.327907Z",
     "start_time": "2024-06-19T06:31:33.286829Z"
    }
   },
   "id": "e8c282a53727943",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "image, label = train_data[sample_idx]\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.369360Z",
     "start_time": "2024-06-19T06:31:33.360919Z"
    }
   },
   "id": "84fa5b26a60379de",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader= DataLoader(train_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.391081Z",
     "start_time": "2024-06-19T06:31:33.384393Z"
    }
   },
   "id": "be585d7eb8bcae35",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training/Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238fefd45b206a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "      size = len(dataloader.dataset)\n",
    "      num_batches = len(dataloader)\n",
    "      test_loss, correct = 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          pred = model(X)\n",
    "          test_loss += loss_fn(pred, y).item()\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      test_loss /= num_batches\n",
    "      correct /= size\n",
    "      print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "      return 100*correct, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.444389Z",
     "start_time": "2024-06-19T06:31:33.436254Z"
    }
   },
   "id": "1b06878e4ef1f642",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79254d3d93db89e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 3.35e-4#4.1\n",
    "epochs = 60"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.469002Z",
     "start_time": "2024-06-19T06:31:33.464396Z"
    }
   },
   "id": "b437c40fa560695f",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_test(device, train_dataloader, test_dataloader, net, learning_rate, epochs):\n",
    "    net.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    times=[]\n",
    "    \n",
    "    time_s = time.time()\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, net, loss_fn, optimizer)\n",
    "        acc, loss = test_loop(device, test_dataloader, net, loss_fn)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "        times.append(time.time() - time_s)\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": [ i for i in range(epochs)],\n",
    "            \"times\": times,\n",
    "            \"loss\": losses,\n",
    "            \"accuracy\": accuracies\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:31:33.508656Z",
     "start_time": "2024-06-19T06:31:33.503008Z"
    }
   },
   "id": "a166ce4ccf0cb4a4",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1 -> HF Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcdec5977207972"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302696  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.267866 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.269222  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.2%, Avg loss: 2.267366 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.268754  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.4%, Avg loss: 2.267254 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.268582  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 2.267192 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.268500  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.267108 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.268586  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.267059 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.268597  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.267028 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.268594  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 2.267011 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.268577  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 2.267005 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.268567  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.267004 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.268576  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.267002 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.268590  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 2.266998 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.268598  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 2.266993 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.268594  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.266988 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.268583  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.266982 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.268567  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.266978 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.268551  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.266971 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.268534  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.266966 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.268518  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.266961 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.268501  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.266951 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.268486  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.266954 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.268454  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.266957 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.268448  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.266960 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.268446  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.266961 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.268412  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.266960 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.268402  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.266960 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.268374  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.266960 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.268324  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.266961 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.268278  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 2.266964 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.268226  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.266967 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.268212  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.266967 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.268212  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.266967 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.268210  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 2.266970 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.268200  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 2.266973 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.268192  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.266972 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.268183  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.266968 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.268175  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.266964 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.268172  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266960 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.268173  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266957 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.268174  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266956 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.268174  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266955 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.268171  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266953 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.268166  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 2.266950 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.268162  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 2.266946 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.268159  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 2.266942 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.268156  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 2.266938 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.268151  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 2.266934 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.268146  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.266928 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.268141  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.266923 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.268138  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.266916 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.268136  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.266908 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.268135  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.266900 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.268134  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.266892 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.268134  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.266884 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.268133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.266877 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.268133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.266872 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.268133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.266867 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.268134  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.266865 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.268133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.266864 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.268133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.266864 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[1., 0., 0., 0., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [0., 1., 0., 1., 0.],\n                        [1., 0., 0., 0., 1.]]],\n              \n              \n                      [[[0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.]]],\n              \n              \n                      [[[0., 1., 1., 1., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 0., 0., 0., 1.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 1., 1., 1., 0.]]],\n              \n              \n                      [[[1., 1., 0., 1., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 1., 0., 1., 1.]]]])),\n             ('conv1.bias', tensor([0., 0., 0., 0.])),\n             ('linear1.weight',\n              tensor([[-2.8387e-02,  3.7454e-02, -3.5231e-02,  ...,  1.8308e-02,\n                       -1.1517e-02, -4.0560e-02],\n                      [-8.6206e-02, -7.5071e-01,  1.4484e-02,  ..., -2.1944e-02,\n                       -1.3402e-01, -4.1939e-01],\n                      [-1.4999e-02, -2.5156e-02,  2.5912e-02,  ..., -1.4901e-02,\n                       -1.0626e-02,  1.1748e-02],\n                      ...,\n                      [ 2.0153e-04,  7.2838e-03,  1.8583e-02,  ..., -3.4488e-02,\n                        1.7248e-02, -3.8208e-02],\n                      [-1.3315e-01, -3.2253e-01, -6.3525e-01,  ..., -2.0466e-02,\n                       -3.8637e-02, -5.2685e-02],\n                      [ 9.4034e-02,  1.2844e-02,  8.2581e-02,  ...,  1.0315e-01,\n                        8.0759e-02, -4.9601e-02]])),\n             ('linear1.bias',\n              tensor([-0.0148,  5.9678, -0.0249, -0.0193, -0.2058,  1.9772,  0.0352,  0.0289,\n                       3.2958,  3.3247]))])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_hf.state_dict())\n",
    "df_net_a1_hf = train_test(device, train_dataloader, test_dataloader, net_a1_hf, learning_rate, epochs)\n",
    "df_net_a1_hf.to_csv('NetA1HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_trained.pt')\n",
    "net_a1_hf.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:35:14.243691Z",
     "start_time": "2024-06-19T06:31:33.515662Z"
    }
   },
   "id": "98b5cb92a0191d1e",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> HT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae1a1c117cd5c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302696  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.267788 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.269121  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.267337 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.268744  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 2.267247 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.268633  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 2.267187 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.268498  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 2.267073 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.268599  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.267023 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.268608  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.267010 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.268621  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 2.267010 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.268605  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.267011 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.268594  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 2.267007 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.268599  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: 2.266998 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.268598  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 2.267023 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.268594  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.266970 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.268581  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.266944 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.268556  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.266932 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.268491  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.266922 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.268277  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 2.266927 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.268211  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.266909 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.268185  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.266903 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.268202  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.266904 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.268181  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 2.266898 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.268167  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 2.266890 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.268189  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.266883 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.268191  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.266880 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.268195  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 2.266880 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.268204  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.266877 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.268206  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.266870 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.268200  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 2.266862 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.268193  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 2.266855 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.268185  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.266849 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.268176  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.266844 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.268167  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.266840 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.268158  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.266836 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.268150  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266833 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.268142  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266830 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.268136  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266829 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.268130  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 2.266830 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.268126  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 2.266833 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.268121  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266838 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.268119  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266842 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.268116  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266842 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.268114  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266835 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.268112  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.266823 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.268111  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.266809 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.268111  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266795 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.268109  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266783 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.268107  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.266772 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.268106  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.266759 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.268104  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.266745 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.268102  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.266730 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.268100  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266714 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.268097  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266698 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.268094  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266683 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.268091  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266669 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.268089  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266656 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.268086  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266643 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.268085  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.266632 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.268082  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.266621 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.268080  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.266611 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.268079  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.266596 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 1.2183e+00,  2.8325e-02, -1.2216e-01,  2.1136e-01,  1.4437e+00],\n                        [ 2.2626e-01,  1.2044e+00,  7.0670e-02,  1.4468e+00,  1.1033e-01],\n                        [ 1.5126e-01,  2.6986e-01,  1.0204e+00,  4.6302e-01, -5.9271e-02],\n                        [ 1.4950e-01,  1.0262e+00, -2.2274e-02,  1.0609e+00,  9.0414e-02],\n                        [ 1.1168e+00,  6.7376e-02, -1.4932e-01,  1.9719e-01,  1.2310e+00]]],\n              \n              \n                      [[[-1.4701e-03,  7.3060e-02,  1.8331e+00,  1.3713e-01, -1.5949e-01],\n                        [ 1.1946e+00,  1.2364e+00,  8.1732e-01,  1.3197e+00,  9.9091e-01],\n                        [ 2.4730e-01,  1.9423e-01,  1.5210e+00,  1.7718e-01, -3.1371e-01],\n                        [ 1.4889e+00,  9.1130e-01,  7.5617e-01,  1.0226e+00,  9.4003e-01],\n                        [-1.2306e-01, -2.3071e-01,  1.3387e+00, -1.7354e-01, -2.2742e-01]]],\n              \n              \n                      [[[ 2.0150e-01,  1.3659e+00,  1.4054e+00,  1.6894e+00, -2.6269e-01],\n                        [ 1.4636e+00,  1.4830e+00,  2.6453e-01,  1.6909e+00,  9.4855e-01],\n                        [ 1.4770e+00,  9.5100e-02, -3.6343e-01,  2.5698e-01,  9.0007e-01],\n                        [ 1.3366e+00,  9.4818e-01, -2.2300e-01,  8.9253e-01,  8.0750e-01],\n                        [-1.5016e-01,  1.1498e+00,  8.0977e-01,  1.2699e+00, -1.6113e-01]]],\n              \n              \n                      [[[ 1.3056e+00,  1.3542e+00, -4.5610e-02,  1.4560e+00,  1.1228e+00],\n                        [ 1.7679e-01,  1.1655e+00, -6.2411e-02,  1.2822e+00, -2.1903e-01],\n                        [-8.2346e-02, -1.6871e-01,  6.9979e-01,  1.2362e-01, -1.4171e-01],\n                        [ 1.1724e+00,  8.8907e-01, -1.5241e-01,  1.1136e+00,  1.2347e+00],\n                        [ 1.1569e+00,  1.1216e+00, -1.2067e-01,  1.2893e+00,  1.2880e+00]]]])),\n             ('conv1.bias', tensor([-0.9645,  0.0987,  0.0845,  1.6536])),\n             ('linear1.weight',\n              tensor([[-2.8388e-02,  3.7455e-02, -3.5230e-02,  ...,  1.8283e-02,\n                       -1.1531e-02, -4.0574e-02],\n                      [-4.4762e-02, -1.8487e-01,  5.7369e-02,  ..., -6.3760e-02,\n                        1.0282e-02, -9.7137e-02],\n                      [-1.4999e-02, -2.5155e-02,  2.5913e-02,  ..., -1.5036e-02,\n                       -1.0765e-02,  1.1599e-02],\n                      ...,\n                      [ 2.0153e-04,  7.2838e-03,  1.8583e-02,  ..., -3.4488e-02,\n                        1.7248e-02, -3.8208e-02],\n                      [-4.0914e-01, -1.9895e-01, -4.8805e-01,  ...,  1.1019e-02,\n                       -2.0648e-02, -2.8833e-02],\n                      [-3.5773e-02, -7.8933e-02, -4.6017e-02,  ...,  1.1692e-01,\n                        6.9173e-02, -3.6746e-02]])),\n             ('linear1.bias',\n              tensor([-0.0145,  1.3757, -0.0260, -0.0193, -0.1054,  0.8880,  0.0354,  0.0289,\n                       2.0673,  1.9605]))])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_ht.state_dict())\n",
    "df_net_a1_ht = train_test(device, train_dataloader, test_dataloader, net_a1_ht, learning_rate, epochs)\n",
    "df_net_a1_ht.to_csv('NetA1HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_trained.pt')\n",
    "net_a1_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:40:21.826357Z",
     "start_time": "2024-06-19T06:35:14.244709Z"
    }
   },
   "id": "57d9aed7097ab0b0",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> DT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e7c98ba5a28137"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 0.0275,  0.0715,  0.1256, -0.1479, -0.1567],\n",
      "          [-0.0226,  0.0100, -0.1715, -0.1725, -0.1208],\n",
      "          [-0.0613,  0.0349,  0.0108,  0.1250, -0.1452],\n",
      "          [ 0.1867,  0.1224,  0.1494, -0.0769,  0.1487],\n",
      "          [ 0.1965,  0.1156, -0.1832,  0.0777, -0.0907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1958, -0.1091, -0.0394,  0.0192,  0.1636],\n",
      "          [ 0.0413,  0.0136, -0.0527,  0.0598, -0.0349],\n",
      "          [-0.0775,  0.1340,  0.0160,  0.0956, -0.0025],\n",
      "          [-0.0257,  0.1061,  0.1439, -0.0596,  0.0039],\n",
      "          [-0.0475, -0.0550, -0.1336,  0.0186, -0.1608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1140,  0.0929, -0.1603, -0.1158, -0.0963],\n",
      "          [ 0.0612,  0.1323, -0.0751, -0.0008,  0.1730],\n",
      "          [-0.0210,  0.0880,  0.1101,  0.0027, -0.0185],\n",
      "          [-0.1288,  0.1883,  0.1363, -0.1108, -0.1461],\n",
      "          [-0.1278,  0.1464, -0.0103,  0.1723,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0337, -0.1431, -0.0953, -0.1032, -0.0572],\n",
      "          [ 0.1921, -0.1182, -0.0249, -0.1109,  0.0914],\n",
      "          [-0.1189,  0.0667,  0.0767,  0.0571, -0.1452],\n",
      "          [ 0.0160, -0.1395, -0.0432,  0.0421,  0.0813],\n",
      "          [ 0.1571, -0.1986,  0.1900, -0.0524,  0.0951]]]])), ('conv1.bias', tensor([-0.0427,  0.0574,  0.0915, -0.0145])), ('linear1.weight', tensor([[-0.0308,  0.0350, -0.0375,  ...,  0.0210, -0.0087, -0.0375],\n",
      "        [-0.0086, -0.0217, -0.0280,  ..., -0.0164,  0.0151,  0.0401],\n",
      "        [-0.0174, -0.0276,  0.0234,  ..., -0.0107, -0.0063,  0.0163],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0093,  0.0206,  ..., -0.0345,  0.0172, -0.0382],\n",
      "        [-0.0126, -0.0030,  0.0216,  ...,  0.0142, -0.0080, -0.0096],\n",
      "        [-0.0269, -0.0064,  0.0409,  ...,  0.0398,  0.0018, -0.0405]])), ('linear1.bias', tensor([-0.0093, -0.0258, -0.0203, -0.0162, -0.0154,  0.0025,  0.0396,  0.0309,\n",
      "        -0.0048,  0.0264]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302629  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 2.268335 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.270597  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.267547 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.269154  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.267167 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.268807  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.266910 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.268620  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 2.266763 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.268484  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 2.266682 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.268373  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 2.266634 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.268299  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 2.266594 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.268246  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.266558 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.268193  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 2.266525 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.268149  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.266494 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.268116  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.266467 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.268093  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.266442 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.268075  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.266423 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.268060  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.266405 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.268048  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.266388 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.268037  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.266374 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.268028  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.266360 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.268018  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.266347 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.268010  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.266334 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.268001  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.266321 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.267992  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266308 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.267982  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266295 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.267971  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266284 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.267959  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266274 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.267945  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266265 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.267931  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266256 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.267918  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266247 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.267905  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.266240 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.267891  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.266233 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.267879  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266226 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.267868  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266219 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.267858  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266213 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.267849  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266206 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.267842  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266199 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.267836  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266192 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.267831  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266185 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.267827  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266178 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.267823  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266172 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.267820  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266166 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.267818  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266161 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.267816  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.266157 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.267814  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266154 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.267813  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266151 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.267812  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266149 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.267810  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266146 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.267810  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266145 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.267809  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266143 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.267808  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266142 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.267807  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266142 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.267805  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.266141 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.267805  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266141 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.267804  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266140 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.267803  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266140 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.267802  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266140 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.267801  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266139 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.267800  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266139 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.267799  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266138 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.267798  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266138 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.267797  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.266138 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 0.5147,  0.6134,  0.4386, -0.1886, -0.3646],\n                        [ 0.3639,  0.3088,  0.0538, -0.0511, -0.3185],\n                        [ 0.3375,  0.2911,  0.2094,  0.4347, -0.1082],\n                        [ 0.9686,  0.5961,  0.6051,  0.4283,  0.3701],\n                        [ 0.9524,  0.5788, -0.0121,  0.3675,  0.0136]]],\n              \n              \n                      [[[ 0.3201, -0.1353, -0.0719,  0.2254,  0.6204],\n                        [ 0.4320,  0.2959,  0.3680,  0.5170,  0.2406],\n                        [ 0.2979,  0.5338,  0.3017,  0.3671,  0.0988],\n                        [ 0.3818,  0.3478,  0.3470,  0.1499,  0.0558],\n                        [ 0.1031, -0.0164, -0.1185,  0.0867, -0.2251]]],\n              \n              \n                      [[[-0.1509,  0.2358, -0.3273, -0.3818,  0.0629],\n                        [ 0.3563,  0.6370,  0.3187,  0.2370,  0.6795],\n                        [-0.0414,  0.6113,  0.2466, -0.0273,  0.1904],\n                        [-0.2467,  0.7045,  0.2516, -0.0988,  0.0199],\n                        [-0.3159,  0.5795,  0.1115,  0.2920,  0.3976]]],\n              \n              \n                      [[[-0.3465, -0.8361, -0.6515, -0.1546,  0.1525],\n                        [ 0.0311, -0.7297, -0.4074,  0.0632,  0.5049],\n                        [-0.1357, -0.2425, -0.0506,  0.5095,  0.2981],\n                        [ 0.3249, -0.2393,  0.0496,  0.6312,  0.7622],\n                        [ 0.4300, -0.3312,  0.2361,  0.5131,  0.6977]]]])),\n             ('conv1.bias', tensor([0.0549, 0.1645, 3.3790, 0.3624])),\n             ('linear1.weight',\n              tensor([[-0.0322,  0.0334, -0.0387,  ...,  0.0172, -0.0064, -0.0407],\n                      [ 0.0355, -0.3905, -0.4614,  ...,  0.0077,  0.5456, -0.0223],\n                      [-0.0179, -0.0281,  0.0229,  ..., -0.0107, -0.0063,  0.0163],\n                      ...,\n                      [ 0.2184, -0.0352, -0.2782,  ..., -0.2060, -0.0044, -0.0076],\n                      [-0.2220, -0.3277, -0.1906,  ..., -0.0242, -0.0527, -0.2659],\n                      [ 0.1650, -0.0612, -0.1496,  ...,  0.0052, -0.1880, -0.2096]])),\n             ('linear1.bias',\n              tensor([-1.4042e-02,  3.5668e-01, -2.5426e-02, -1.9557e-02,  4.6679e-01,\n                      -1.3133e-03,  3.4897e-02,  3.3927e-01,  3.1198e+00,  4.8733e-01]))])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_dt.state_dict())\n",
    "df_net_a1_dt = train_test(device, train_dataloader, test_dataloader, net_a1_dt, learning_rate, epochs)\n",
    "df_net_a1_dt.to_csv('NetA1DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_trained.pt')\n",
    "net_a1_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:45:31.334177Z",
     "start_time": "2024-06-19T06:40:21.826357Z"
    }
   },
   "id": "5fc9595e6f55df0b",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HF Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aabaf6906661353"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458617  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.025754 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.974472  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.975356 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.902637  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.953268 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.877771  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.938160 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.861839  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.927176 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.851549  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.916291 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.843965  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.908686 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.838538  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.901027 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.832535  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.896949 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.830911  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.891461 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.828442  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.887246 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.825345  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.884116 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.822881  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.880886 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.821934  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.877013 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.818866  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.874190 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.817344  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.871676 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.815403  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.869332 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.812426  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.867083 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.812207  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.865155 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.811008  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.864148 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.810800  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.864176 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.810167  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.862808 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.809145  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.860815 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.807023  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.859470 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.806622  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.857605 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.804476  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.856159 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.803557  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.855750 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.802457  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.854529 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.801104  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.853622 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.799671  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.852443 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.798243  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.851934 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.796926  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.850813 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.795656  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.850185 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.792621  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.850034 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.792758  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.849886 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.791847  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.848167 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.789563  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.847806 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.787537  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.846892 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.786922  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.846470 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.786295  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.846182 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.787011  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.846109 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.786427  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.845540 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.786254  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.844902 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.785704  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.844329 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.784729  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.843869 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.783952  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.843350 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.783109  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.843177 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.782188  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.842347 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.780594  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.841326 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.778994  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.840864 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.778278  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.840799 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.777431  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.840241 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.776425  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.839598 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.775321  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.839371 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.774860  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.838944 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.773910  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.838413 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.772232  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.837769 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.771592  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.837603 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.771099  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.837588 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.770102  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.836798 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[1., 0., 0., 0., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [0., 1., 0., 1., 0.],\n                        [1., 0., 0., 0., 1.]]],\n              \n              \n                      [[[0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.]]],\n              \n              \n                      [[[0., 1., 1., 1., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 0., 0., 0., 1.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 1., 1., 1., 0.]]],\n              \n              \n                      [[[1., 1., 0., 1., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 1., 0., 1., 1.]]]])),\n             ('conv1.bias', tensor([0., 0., 0., 0.])),\n             ('conv2.weight',\n              tensor([[[[-0.3183, -0.1789,  0.0638],\n                        [-0.0590, -0.0336,  0.0288],\n                        [ 0.1037,  0.2040,  0.0191]],\n              \n                       [[-0.1921, -0.0923,  0.0977],\n                        [-0.0234,  0.1652,  0.1164],\n                        [ 0.0969,  0.1511, -0.1174]],\n              \n                       [[-0.2727, -0.2033, -0.1485],\n                        [-0.1451,  0.1885, -0.0978],\n                        [ 0.1208,  0.1532, -0.0817]],\n              \n                       [[-0.0789, -0.1363, -0.1126],\n                        [ 0.1457, -0.0357, -0.0724],\n                        [ 0.3360,  0.1855,  0.1753]]],\n              \n              \n                      [[[-0.1087,  0.0048, -0.0092],\n                        [ 0.0266,  0.1367,  0.1470],\n                        [-0.0949,  0.1618, -0.1222]],\n              \n                       [[-0.0505, -0.0304, -0.1019],\n                        [ 0.0221, -0.0025, -0.0200],\n                        [-0.2036,  0.1353,  0.1851]],\n              \n                       [[-0.1658,  0.1761, -0.0096],\n                        [ 0.1147,  0.1783,  0.1730],\n                        [-0.0744, -0.0488,  0.0626]],\n              \n                       [[-0.0770,  0.1247, -0.0401],\n                        [ 0.0668,  0.0895, -0.1192],\n                        [-0.1627,  0.0205,  0.1439]]],\n              \n              \n                      [[[-0.0230,  0.1484, -0.1621],\n                        [-0.2017,  0.0092,  0.1675],\n                        [-0.0092, -0.1105, -0.0256]],\n              \n                       [[ 0.1940,  0.0760,  0.3435],\n                        [-0.1931, -0.1261, -0.0255],\n                        [-0.0588, -0.1161,  0.0632]],\n              \n                       [[ 0.1273, -0.0026,  0.2728],\n                        [-0.0273,  0.1105,  0.0497],\n                        [ 0.0894, -0.2111, -0.1291]],\n              \n                       [[-0.1507,  0.1185, -0.1119],\n                        [ 0.0100,  0.1509,  0.1014],\n                        [ 0.2384, -0.0990, -0.1571]]],\n              \n              \n                      [[[-0.1978,  0.0065,  0.1350],\n                        [-0.0782,  0.0484,  0.1326],\n                        [-0.0216,  0.1320,  0.0483]],\n              \n                       [[-0.4958,  0.1090,  0.1813],\n                        [-0.3970, -0.0139,  0.1594],\n                        [ 0.0226, -0.0039,  0.2176]],\n              \n                       [[-0.6783, -0.2112,  0.0020],\n                        [-0.1522,  0.1606,  0.2480],\n                        [ 0.0985, -0.0791,  0.1274]],\n              \n                       [[-0.2548, -0.2783,  0.0182],\n                        [-0.1414, -0.0605,  0.0457],\n                        [ 0.0315,  0.2459,  0.1775]]],\n              \n              \n                      [[[ 0.1311,  0.0304, -0.0214],\n                        [ 0.1961, -0.0152, -0.1519],\n                        [ 0.1614,  0.0639, -0.2430]],\n              \n                       [[ 0.2147,  0.0640, -0.0361],\n                        [ 0.1280, -0.0549, -0.1668],\n                        [ 0.1396,  0.1129, -0.2591]],\n              \n                       [[ 0.2480, -0.0365, -0.2352],\n                        [ 0.2261,  0.1999, -0.2585],\n                        [ 0.1397, -0.0492, -0.1750]],\n              \n                       [[ 0.0408, -0.0581, -0.1518],\n                        [-0.0075,  0.0397, -0.0349],\n                        [ 0.1379,  0.0137, -0.1733]]],\n              \n              \n                      [[[-0.1304, -0.2121,  0.3444],\n                        [ 0.0669,  0.1211,  0.0149],\n                        [ 0.0729, -0.1469,  0.0365]],\n              \n                       [[ 0.1061, -0.1025,  0.0500],\n                        [ 0.0013,  0.2506,  0.0446],\n                        [ 0.1015,  0.1854, -0.3854]],\n              \n                       [[ 0.1875, -0.2643, -0.1110],\n                        [-0.0282, -0.0278,  0.1482],\n                        [ 0.1299,  0.1117, -0.2182]],\n              \n                       [[ 0.1033, -0.2094,  0.2758],\n                        [-0.0835,  0.0756,  0.0424],\n                        [-0.0543, -0.1594,  0.0303]]],\n              \n              \n                      [[[ 0.0413, -0.0742,  0.2251],\n                        [-0.2599,  0.0327,  0.1115],\n                        [-0.1680, -0.0084,  0.0419]],\n              \n                       [[-0.0147, -0.2183,  0.1757],\n                        [ 0.1019, -0.1605,  0.1986],\n                        [-0.2296, -0.1528,  0.0706]],\n              \n                       [[-0.2236, -0.0244,  0.0835],\n                        [-0.1663, -0.0668,  0.1613],\n                        [-0.2908,  0.1214,  0.0867]],\n              \n                       [[-0.1766, -0.0500,  0.2971],\n                        [-0.0650,  0.0742,  0.3073],\n                        [ 0.0351, -0.1270,  0.1748]]],\n              \n              \n                      [[[-0.0147,  0.0549, -0.1130],\n                        [-0.1358,  0.2107,  0.1349],\n                        [ 0.0378,  0.1574,  0.0796]],\n              \n                       [[ 0.1312,  0.0805, -0.2067],\n                        [ 0.0696, -0.1855,  0.0758],\n                        [-0.1090,  0.1454,  0.0067]],\n              \n                       [[ 0.0572,  0.2185, -0.1753],\n                        [-0.1006, -0.0088, -0.0635],\n                        [ 0.0468,  0.1616, -0.0275]],\n              \n                       [[-0.1156,  0.1935, -0.0795],\n                        [ 0.0413,  0.0913, -0.0562],\n                        [-0.0648,  0.1660,  0.0243]]],\n              \n              \n                      [[[ 0.1704, -0.1057,  0.3284],\n                        [ 0.1268, -0.2649, -0.0500],\n                        [ 0.0121, -0.0017,  0.2231]],\n              \n                       [[-0.0215,  0.2326,  0.2194],\n                        [-0.1105, -0.0738, -0.0573],\n                        [-0.0885, -0.2586,  0.0289]],\n              \n                       [[-0.1213,  0.1641,  0.2226],\n                        [-0.1090,  0.0072, -0.1848],\n                        [-0.1821, -0.0507,  0.1380]],\n              \n                       [[ 0.0058, -0.1828,  0.2799],\n                        [-0.0106,  0.1223,  0.1158],\n                        [-0.0696,  0.0266,  0.0094]]],\n              \n              \n                      [[[ 0.1281, -0.0604, -0.0108],\n                        [-0.0733, -0.1483,  0.1287],\n                        [ 0.0163,  0.0386,  0.0503]],\n              \n                       [[-0.1969, -0.2486,  0.1217],\n                        [ 0.0558,  0.1303,  0.2695],\n                        [-0.0014,  0.1241,  0.1997]],\n              \n                       [[-0.1740, -0.2363,  0.1551],\n                        [ 0.1600, -0.0797,  0.0736],\n                        [ 0.1788,  0.0547,  0.0758]],\n              \n                       [[ 0.0807, -0.2662, -0.0187],\n                        [ 0.1580, -0.1043,  0.0112],\n                        [ 0.0864, -0.0164, -0.1092]]],\n              \n              \n                      [[[-0.1040, -0.1146, -0.1767],\n                        [ 0.0133,  0.1376,  0.0687],\n                        [-0.0686, -0.0236,  0.3131]],\n              \n                       [[-0.0028, -0.3170, -0.1312],\n                        [ 0.0116,  0.1019,  0.0824],\n                        [ 0.1242, -0.1153,  0.1273]],\n              \n                       [[ 0.0376, -0.1469, -0.1955],\n                        [ 0.2647,  0.0502, -0.0744],\n                        [-0.0463,  0.0152,  0.1062]],\n              \n                       [[-0.1381,  0.0495, -0.1065],\n                        [ 0.1900,  0.1876, -0.0265],\n                        [-0.2043, -0.0600,  0.2548]]],\n              \n              \n                      [[[-0.3402, -0.0677,  0.0545],\n                        [-0.1996,  0.0160, -0.0552],\n                        [ 0.1036, -0.0020, -0.0158]],\n              \n                       [[-0.3229, -0.1206,  0.1454],\n                        [ 0.1312,  0.0274,  0.1553],\n                        [-0.2055,  0.2275, -0.0950]],\n              \n                       [[-0.3311,  0.0974, -0.1088],\n                        [ 0.0160,  0.1375, -0.0014],\n                        [-0.0986,  0.2072,  0.1855]],\n              \n                       [[-0.1652,  0.1268, -0.0887],\n                        [ 0.0578,  0.2055, -0.1670],\n                        [-0.0337,  0.2647,  0.2056]]]])),\n             ('conv2.bias',\n              tensor([ 0.5953, -0.4906,  0.2531,  0.3558,  1.0102,  0.1508,  0.3604,  0.0041,\n                       0.2438, -0.0071,  0.0150,  0.6987])),\n             ('linear1.weight',\n              tensor([[ 0.2305, -0.0655,  0.1855,  ..., -0.0276,  0.0109, -0.2909],\n                      [ 0.0717, -0.3552, -0.0975,  ..., -0.3794,  0.0772,  0.2651],\n                      [-0.0464,  0.0369, -0.0130,  ...,  0.0117,  0.0236, -0.0551],\n                      ...,\n                      [ 0.1844,  0.1182,  0.0251,  ..., -0.1913,  0.0205,  0.1662],\n                      [ 0.0592,  0.0461,  0.0435,  ..., -0.0132,  0.0753,  0.0823],\n                      [-0.0580, -0.0965,  0.1229,  ...,  0.0299,  0.0257, -0.2199]])),\n             ('linear1.bias',\n              tensor([ 0.3705,  0.1809,  0.0364, -0.0109, -0.0931,  0.2008,  0.4083,  0.1707,\n                       0.1619, -0.0417]))])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a2_hf.state_dict())\n",
    "df_net_a2_hf = train_test(device, train_dataloader, test_dataloader, net_a2_hf, learning_rate, epochs)\n",
    "df_net_a2_hf.to_csv('NetA2HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_trained.pt')\n",
    "net_a2_hf.state_dict() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:50:01.868270Z",
     "start_time": "2024-06-19T06:45:31.335184Z"
    }
   },
   "id": "f0485488dfe86d3f",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c29cd600b2295f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458617  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.020441 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.963781  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.968841 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.892119  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.947097 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.867829  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.931741 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.851451  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.918274 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.838315  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.906048 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.830431  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.898023 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.826579  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.893095 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.821735  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.886968 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.817109  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.883148 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.815687  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.879395 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.813328  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.876595 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.811374  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.872066 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.809263  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.869548 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.808293  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.868064 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.809559  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.864508 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.802707  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.862129 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.800419  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.859995 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.798164  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.858193 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.796697  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.855779 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.794012  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.854028 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.792519  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.852855 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.791535  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.851376 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.789946  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.849812 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.788663  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.848501 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.788655  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.847694 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.786473  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.846111 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.783110  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.843969 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.780974  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.842713 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.779323  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.841052 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.777281  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.840072 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.775620  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.838635 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.773754  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.838753 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.772607  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.838001 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.770987  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.837869 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.769085  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.835958 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.767299  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.835374 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.765348  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.834523 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.764045  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.833610 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.763172  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.832850 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.761613  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.831944 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.759202  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.830903 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.758789  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.830286 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.756614  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.829251 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.754238  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.829032 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.752084  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.829701 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.751620  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.828912 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.750637  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.828119 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.748477  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.827005 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.747377  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.826527 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.746513  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.826247 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.744877  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.825802 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.744269  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.824872 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.743011  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.824488 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.742434  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.824256 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.741633  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.823176 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.742105  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.822952 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.742115  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.822515 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.741345  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.822098 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.740963  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.821510 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 1.1262,  0.1075, -0.0870, -0.0846,  1.8457],\n                        [-0.2603,  1.1281,  0.7913,  1.4658,  0.2752],\n                        [-0.0234,  0.3841,  1.5265,  0.7119,  0.4967],\n                        [-0.0956,  0.9792,  0.3302,  1.0249,  0.0470],\n                        [ 0.9983, -0.0494,  0.2457,  0.0681,  1.0438]]],\n              \n              \n                      [[[-0.3275,  0.2842,  1.5510,  0.5833, -0.3892],\n                        [ 1.2335,  1.5805,  1.3342,  2.0544,  1.1795],\n                        [-0.0717,  0.2067,  1.9478,  0.6418,  0.1125],\n                        [-0.1573,  0.6388,  0.5613,  1.0572,  0.9055],\n                        [-0.8954, -0.0262,  1.4276,  0.3624, -0.0451]]],\n              \n              \n                      [[[ 0.2069,  1.3344,  1.7686,  1.3148, -0.1821],\n                        [ 0.7095,  1.3760,  1.0920,  1.3641,  1.2512],\n                        [ 1.0053,  0.1303,  0.4392,  0.3734,  1.3532],\n                        [ 0.2705,  0.8894,  0.5534,  1.3019,  1.2768],\n                        [-0.4375,  0.9012,  1.3675,  1.3559,  0.3207]]],\n              \n              \n                      [[[ 1.4088,  1.3638, -0.0096,  0.8396,  0.8669],\n                        [ 0.0926,  1.4016,  0.5849,  1.0873,  0.0707],\n                        [ 0.3334,  0.2264,  1.0808,  0.3117,  0.5026],\n                        [ 0.6558,  1.0971,  0.1849,  1.0948,  1.1088],\n                        [ 0.9234,  1.0057,  0.1930,  1.3741,  1.3306]]]])),\n             ('conv1.bias', tensor([0.0007, 0.3522, 0.0011, 0.2565])),\n             ('conv2.weight',\n              tensor([[[[-3.2260e-01, -1.5813e-01,  7.4386e-02],\n                        [-6.9030e-02, -1.2396e-02,  2.9542e-02],\n                        [ 7.6054e-02,  1.6782e-01,  2.0611e-02]],\n              \n                       [[-1.3076e-01, -6.5161e-02,  1.0149e-01],\n                        [ 2.8419e-02,  1.6850e-01,  1.2278e-01],\n                        [ 1.0943e-01,  7.7867e-02, -1.1183e-01]],\n              \n                       [[-2.6316e-01, -1.7801e-01, -1.6289e-01],\n                        [-1.4586e-01,  1.8487e-01, -9.3334e-02],\n                        [ 9.2065e-02,  1.0530e-01, -8.6475e-02]],\n              \n                       [[-7.5047e-02, -8.7248e-02, -1.1666e-01],\n                        [ 1.3858e-01, -1.7658e-02, -6.7206e-02],\n                        [ 2.8986e-01,  1.4643e-01,  1.7305e-01]]],\n              \n              \n                      [[[-1.0543e-01,  1.5490e-02, -7.3248e-03],\n                        [ 5.5112e-02,  1.0037e-01,  1.4917e-01],\n                        [-1.0501e-01,  1.5731e-01, -1.1050e-01]],\n              \n                       [[-5.2350e-02, -5.2100e-02, -1.3810e-01],\n                        [ 4.2315e-02, -6.2311e-03, -5.9433e-02],\n                        [-1.7537e-01,  1.2007e-01,  1.4047e-01]],\n              \n                       [[-1.3154e-01,  1.4883e-01, -9.1735e-03],\n                        [ 1.0647e-01,  1.6844e-01,  1.6022e-01],\n                        [-5.1919e-02, -6.5150e-02,  4.9631e-02]],\n              \n                       [[-6.2037e-02,  1.0566e-01, -5.4414e-02],\n                        [ 8.7252e-02,  5.9087e-02, -1.3158e-01],\n                        [-1.5525e-01,  1.3611e-02,  1.3903e-01]]],\n              \n              \n                      [[[-1.4236e-02,  9.5644e-02, -1.5291e-01],\n                        [-1.7338e-01,  3.7696e-02,  1.4903e-01],\n                        [-6.5267e-03, -1.4715e-01, -1.5699e-02]],\n              \n                       [[ 2.6771e-01,  2.4732e-02,  2.3615e-01],\n                        [-1.3086e-01, -7.6829e-02,  3.6061e-02],\n                        [-7.7416e-02, -1.2144e-01,  8.8880e-02]],\n              \n                       [[ 1.5798e-01, -5.0153e-02,  1.5280e-01],\n                        [-7.0924e-03,  8.2377e-02,  1.2023e-01],\n                        [ 7.1342e-02, -2.4454e-01, -7.2404e-02]],\n              \n                       [[-1.0292e-01,  1.2599e-01, -7.2074e-02],\n                        [-1.2641e-02,  1.3143e-01,  8.1764e-02],\n                        [ 2.2356e-01, -1.1737e-01, -1.3873e-01]]],\n              \n              \n                      [[[-3.7716e-01, -7.7984e-02,  1.5121e-01],\n                        [-1.1501e-01, -3.3415e-02,  9.4324e-02],\n                        [-1.6392e-01,  1.5099e-01,  3.3911e-02]],\n              \n                       [[-4.9476e-01,  1.2475e-01,  2.1804e-01],\n                        [-4.9516e-01, -2.3877e-02,  9.2103e-02],\n                        [-7.5853e-02,  7.2758e-02,  2.3511e-01]],\n              \n                       [[-7.8650e-01, -2.7102e-01, -2.8336e-03],\n                        [-2.9000e-01,  1.0745e-01,  1.8307e-01],\n                        [-5.8624e-02, -6.2720e-02,  1.1199e-01]],\n              \n                       [[-2.5597e-01, -2.8658e-01,  1.9893e-02],\n                        [-6.7886e-02, -8.5936e-02,  5.7662e-03],\n                        [-7.2110e-02,  2.9500e-01,  1.7207e-01]]],\n              \n              \n                      [[[ 5.4387e-02, -1.1504e-04,  6.2973e-03],\n                        [ 1.8047e-01, -1.7127e-02, -1.5656e-01],\n                        [ 1.3080e-01,  1.4884e-02, -2.4775e-01]],\n              \n                       [[ 1.8646e-01,  1.3837e-02,  6.3499e-03],\n                        [ 1.3588e-01, -5.4078e-02, -1.1411e-01],\n                        [ 1.1366e-01,  9.0313e-02, -1.4284e-01]],\n              \n                       [[ 2.2630e-01, -7.6095e-02, -2.4063e-01],\n                        [ 1.8753e-01,  1.7368e-01, -2.4983e-01],\n                        [ 1.1459e-01, -6.5119e-02, -1.5711e-01]],\n              \n                       [[ 5.4785e-02, -3.0652e-02, -1.1576e-01],\n                        [-2.6500e-02,  5.4559e-02, -1.2204e-04],\n                        [ 1.3166e-01,  4.9222e-03, -1.3664e-01]]],\n              \n              \n                      [[[-1.5443e-01, -2.2275e-01,  3.5119e-01],\n                        [ 4.6641e-02,  8.9317e-02,  3.5428e-03],\n                        [ 6.4900e-02, -1.3867e-01,  5.2757e-02]],\n              \n                       [[ 4.2225e-02, -3.5848e-02, -1.0975e-03],\n                        [-6.6529e-02,  1.9665e-01,  2.0758e-02],\n                        [ 5.1016e-02,  2.5471e-01, -3.2669e-01]],\n              \n                       [[ 1.6818e-01, -1.6741e-01, -1.1470e-01],\n                        [-6.6393e-02, -1.1304e-01,  1.0831e-01],\n                        [ 1.6486e-01,  1.3746e-01, -1.7396e-01]],\n              \n                       [[ 1.3855e-01, -1.5829e-01,  3.0470e-01],\n                        [-4.3353e-02, -1.4452e-03,  8.9393e-02],\n                        [-2.7540e-02, -2.0210e-01,  7.6551e-02]]],\n              \n              \n                      [[[ 7.3221e-02,  1.2741e-02,  1.3336e-01],\n                        [-2.6188e-01,  3.5088e-02,  8.8025e-02],\n                        [-1.7821e-01, -3.9967e-03,  2.1034e-02]],\n              \n                       [[-9.1323e-02, -1.3445e-01,  1.5495e-01],\n                        [ 1.0212e-01, -1.4296e-01,  2.2010e-01],\n                        [-2.1131e-01, -1.4370e-01,  6.7704e-02]],\n              \n                       [[-2.3032e-01,  3.8386e-02,  3.4654e-02],\n                        [-1.7561e-01, -8.4301e-02,  1.3630e-01],\n                        [-2.4960e-01,  1.2967e-01,  7.3283e-02]],\n              \n                       [[-1.6564e-01, -3.6849e-02,  2.3472e-01],\n                        [ 3.0367e-03,  7.2793e-02,  2.9234e-01],\n                        [ 5.5721e-02, -1.4025e-01,  1.4815e-01]]],\n              \n              \n                      [[[-1.3080e-02,  2.2152e-02, -1.5051e-01],\n                        [-1.6143e-01,  1.2095e-01,  1.5577e-01],\n                        [ 2.2056e-02,  1.9036e-01,  9.3946e-02]],\n              \n                       [[ 2.0291e-01,  1.3388e-02, -1.8444e-01],\n                        [ 1.1435e-01, -2.2957e-01,  7.7433e-02],\n                        [-7.5281e-02,  1.5503e-01, -5.9033e-02]],\n              \n                       [[ 9.0236e-02,  1.1932e-01, -1.6017e-01],\n                        [-9.0710e-02, -4.9567e-02, -5.0230e-02],\n                        [ 7.2955e-02,  1.5643e-01, -5.6688e-02]],\n              \n                       [[-1.2721e-01,  1.7510e-01, -8.5188e-02],\n                        [ 1.3514e-02,  5.7470e-02, -5.9525e-02],\n                        [-2.9895e-02,  1.8645e-01,  2.6608e-02]]],\n              \n              \n                      [[[ 1.7156e-01, -1.1988e-01,  2.6209e-01],\n                        [ 1.2768e-01, -2.2568e-01, -6.0745e-02],\n                        [-1.9903e-03, -1.0888e-02,  2.1208e-01]],\n              \n                       [[ 2.6313e-02,  1.8367e-01,  2.5065e-01],\n                        [-1.2970e-01, -1.0464e-02, -2.2295e-03],\n                        [-9.5316e-02, -2.1631e-01, -1.2461e-03]],\n              \n                       [[-7.8766e-02,  1.5535e-01,  1.1024e-01],\n                        [-1.4304e-01,  2.7672e-02, -1.3908e-01],\n                        [-1.7671e-01, -7.7526e-02,  1.2249e-01]],\n              \n                       [[ 4.4616e-02, -1.4116e-01,  2.1422e-01],\n                        [-4.4228e-02,  1.2944e-01,  7.6996e-02],\n                        [-4.5681e-02,  2.9810e-02,  6.6810e-04]]],\n              \n              \n                      [[[ 1.2686e-01, -2.3499e-02, -7.8699e-03],\n                        [-1.0679e-01, -1.0410e-01,  1.2327e-01],\n                        [-9.1482e-03,  6.2964e-02,  4.7834e-02]],\n              \n                       [[-1.9296e-01, -1.6935e-01,  6.2846e-02],\n                        [ 5.0995e-02,  1.4529e-01,  1.8044e-01],\n                        [-1.1326e-03,  1.6558e-01,  1.2131e-01]],\n              \n                       [[-1.8241e-01, -1.6681e-01,  1.2331e-01],\n                        [ 1.1807e-01, -8.6488e-02,  5.0872e-02],\n                        [ 1.5381e-01,  8.7871e-02,  4.7718e-02]],\n              \n                       [[ 8.9321e-02, -2.6626e-01, -4.7824e-02],\n                        [ 1.5115e-01, -8.7815e-02,  2.3177e-02],\n                        [ 5.1493e-02, -1.3772e-02, -1.3423e-01]]],\n              \n              \n                      [[[-8.2984e-02, -1.3335e-01, -1.2872e-01],\n                        [ 2.4032e-02,  1.1683e-01,  4.5359e-02],\n                        [-1.1252e-02,  3.0579e-02,  3.0059e-01]],\n              \n                       [[-9.4187e-02, -2.8759e-01, -1.5364e-01],\n                        [-4.6724e-02,  8.7153e-02,  5.7466e-02],\n                        [ 1.0332e-01, -2.5663e-02,  1.4486e-01]],\n              \n                       [[ 9.4893e-03, -1.1543e-01, -2.0245e-01],\n                        [ 2.2970e-01,  2.6330e-02, -9.5293e-02],\n                        [-3.5288e-02,  6.8704e-02,  1.0464e-01]],\n              \n                       [[-1.2004e-01,  2.2032e-02, -9.1910e-02],\n                        [ 1.7224e-01,  1.6047e-01, -4.5145e-02],\n                        [-1.9063e-01, -2.5659e-02,  2.3900e-01]]],\n              \n              \n                      [[[-3.1235e-01,  2.1458e-02,  2.7954e-02],\n                        [-1.9477e-01, -1.6732e-02, -4.6430e-02],\n                        [ 4.4826e-02,  1.9304e-02, -6.3956e-02]],\n              \n                       [[-3.2299e-01, -3.5694e-02,  7.9196e-02],\n                        [ 1.8057e-01,  5.8041e-02,  1.1287e-01],\n                        [-1.4925e-01,  2.0587e-01, -1.6879e-01]],\n              \n                       [[-2.8586e-01,  1.2716e-01, -1.3373e-01],\n                        [-4.3031e-02,  1.7841e-01,  2.2978e-04],\n                        [-9.5499e-02,  1.6703e-01,  1.1796e-01]],\n              \n                       [[-1.5639e-01,  1.7383e-01, -1.1265e-01],\n                        [ 8.3460e-02,  2.1489e-01, -1.1981e-01],\n                        [-7.9215e-02,  2.7059e-01,  1.9124e-01]]]])),\n             ('conv2.bias',\n              tensor([ 0.7253, -0.3417,  0.1195,  0.8487,  0.9569,  0.1528,  0.3848, -0.0305,\n                       0.2065, -0.0682,  0.0452,  0.8256])),\n             ('linear1.weight',\n              tensor([[ 0.2283, -0.0428,  0.1046,  ..., -0.0219,  0.0210, -0.2497],\n                      [ 0.1445, -0.2301, -0.0967,  ..., -0.3824,  0.2776,  0.1478],\n                      [-0.0557,  0.0275, -0.0231,  ...,  0.0086,  0.0311, -0.0551],\n                      ...,\n                      [ 0.1826,  0.1380,  0.0609,  ..., -0.1239, -0.0323, -0.0552],\n                      [ 0.0588,  0.0885,  0.0235,  ...,  0.0468,  0.0529,  0.0726],\n                      [-0.0640, -0.1001,  0.1467,  ...,  0.0071,  0.0360, -0.1620]])),\n             ('linear1.bias',\n              tensor([ 0.3322,  0.1791,  0.0281, -0.0109, -0.1383,  0.1551,  0.3784,  0.1561,\n                       0.2089, -0.0401]))])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a2_ht.state_dict())\n",
    "df_net_a2_ht = train_test(device, train_dataloader, test_dataloader, net_a2_ht, learning_rate, epochs)\n",
    "df_net_a2_ht.to_csv('NetA2HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_trained.pt')\n",
    "net_a2_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T06:55:59.074326Z",
     "start_time": "2024-06-19T06:50:01.869277Z"
    }
   },
   "id": "bc170d6165469498",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> DT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ae93a49f80a7f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 0.0275,  0.0715,  0.1256, -0.1479, -0.1567],\n",
      "          [-0.0226,  0.0100, -0.1715, -0.1725, -0.1208],\n",
      "          [-0.0613,  0.0349,  0.0108,  0.1250, -0.1452],\n",
      "          [ 0.1867,  0.1224,  0.1494, -0.0769,  0.1487],\n",
      "          [ 0.1965,  0.1156, -0.1832,  0.0777, -0.0907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1958, -0.1091, -0.0394,  0.0192,  0.1636],\n",
      "          [ 0.0413,  0.0136, -0.0527,  0.0598, -0.0349],\n",
      "          [-0.0775,  0.1340,  0.0160,  0.0956, -0.0025],\n",
      "          [-0.0257,  0.1061,  0.1439, -0.0596,  0.0039],\n",
      "          [-0.0475, -0.0550, -0.1336,  0.0186, -0.1608]]],\n",
      "\n",
      "\n",
      "        [[[-0.1140,  0.0929, -0.1603, -0.1158, -0.0963],\n",
      "          [ 0.0612,  0.1323, -0.0751, -0.0008,  0.1730],\n",
      "          [-0.0210,  0.0880,  0.1101,  0.0027, -0.0185],\n",
      "          [-0.1288,  0.1883,  0.1363, -0.1108, -0.1461],\n",
      "          [-0.1278,  0.1464, -0.0103,  0.1723,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.0337, -0.1431, -0.0953, -0.1032, -0.0572],\n",
      "          [ 0.1921, -0.1182, -0.0249, -0.1109,  0.0914],\n",
      "          [-0.1189,  0.0667,  0.0767,  0.0571, -0.1452],\n",
      "          [ 0.0160, -0.1395, -0.0432,  0.0421,  0.0813],\n",
      "          [ 0.1571, -0.1986,  0.1900, -0.0524,  0.0951]]]])), ('conv1.bias', tensor([-0.0427,  0.0574,  0.0915, -0.0145])), ('conv2.weight', tensor([[[[-1.6052e-01, -6.2000e-02,  1.0837e-01],\n",
      "          [-5.0771e-02, -8.0880e-02,  2.3348e-02],\n",
      "          [-6.8565e-02, -1.7490e-03,  3.6397e-03]],\n",
      "\n",
      "         [[-3.7457e-03,  4.4198e-02,  1.4329e-01],\n",
      "          [ 1.4342e-03,  9.4503e-02,  9.8810e-02],\n",
      "          [-4.9744e-02, -9.5421e-02, -1.4808e-01]],\n",
      "\n",
      "         [[-1.2722e-01, -7.1580e-02, -1.2394e-01],\n",
      "          [-1.5096e-01,  1.3032e-01, -1.0246e-01],\n",
      "          [-4.7858e-02, -4.7792e-02, -1.0565e-01]],\n",
      "\n",
      "         [[ 3.2689e-02, -2.9720e-02, -9.5437e-02],\n",
      "          [ 1.3592e-01, -9.7682e-02, -7.7052e-02],\n",
      "          [ 1.4754e-01, -1.4298e-02,  1.6085e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1413e-01, -2.4897e-02, -4.0433e-02],\n",
      "          [ 5.5097e-02,  2.4970e-02,  1.5878e-01],\n",
      "          [-6.2286e-02,  1.4072e-01, -1.0783e-01]],\n",
      "\n",
      "         [[ 7.7002e-03, -1.0852e-01, -1.4950e-01],\n",
      "          [ 2.2118e-02, -8.1380e-02, -8.7123e-02],\n",
      "          [-1.1405e-01,  1.1754e-01,  1.2353e-01]],\n",
      "\n",
      "         [[-9.7695e-02,  8.0854e-02,  1.2575e-03],\n",
      "          [ 9.1815e-02,  1.2782e-01,  1.5159e-01],\n",
      "          [-6.4834e-03, -7.8857e-02,  4.9748e-02]],\n",
      "\n",
      "         [[-8.3979e-02,  6.8250e-02, -3.6014e-02],\n",
      "          [ 9.8771e-02,  1.9050e-02, -1.0809e-01],\n",
      "          [-1.2503e-01, -9.9689e-06,  1.6075e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.3051e-02, -4.8846e-02, -4.8163e-02],\n",
      "          [-1.5666e-01,  2.9813e-02,  1.1130e-01],\n",
      "          [-4.8904e-02,  6.4008e-03,  2.4026e-02]],\n",
      "\n",
      "         [[ 1.3931e-01, -8.5983e-02,  1.3366e-01],\n",
      "          [-7.2061e-02, -4.2794e-02, -8.9988e-02],\n",
      "          [-1.0316e-01,  6.0355e-02,  3.6064e-02]],\n",
      "\n",
      "         [[ 9.4286e-02, -8.8436e-02,  9.4266e-02],\n",
      "          [ 2.8189e-03,  4.6797e-02,  6.1327e-02],\n",
      "          [-1.6074e-03, -1.4566e-01, -3.3301e-02]],\n",
      "\n",
      "         [[-1.0158e-01,  4.8387e-02, -1.1762e-02],\n",
      "          [-1.0284e-02,  5.1555e-02,  1.0268e-01],\n",
      "          [ 1.3171e-01, -2.9054e-02, -8.4396e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.9052e-03,  1.5875e-01, -1.6294e-02],\n",
      "          [ 7.5736e-02, -7.5512e-03, -3.3559e-02],\n",
      "          [-1.3409e-01,  2.4476e-03, -4.6340e-02]],\n",
      "\n",
      "         [[ 1.3732e-01,  1.5026e-01,  2.0499e-03],\n",
      "          [-1.5791e-01, -1.0088e-02, -3.3751e-02],\n",
      "          [-1.5063e-02, -9.9950e-02,  3.6298e-02]],\n",
      "\n",
      "         [[-1.1751e-01, -9.4486e-02, -1.5214e-01],\n",
      "          [ 8.7828e-03,  1.3726e-01,  7.8886e-02],\n",
      "          [-3.5003e-02, -1.6253e-01, -1.3392e-02]],\n",
      "\n",
      "         [[ 2.4896e-02, -7.8726e-02, -9.9595e-02],\n",
      "          [-2.2285e-02, -1.3525e-01, -1.5152e-01],\n",
      "          [-1.3592e-01,  1.5998e-01,  7.9233e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6911e-02, -2.0040e-02,  1.5132e-01],\n",
      "          [ 1.4087e-01,  2.7485e-03, -6.2440e-02],\n",
      "          [ 4.7075e-02, -1.6722e-02, -1.0128e-01]],\n",
      "\n",
      "         [[ 1.2311e-02,  4.2930e-02,  1.0816e-01],\n",
      "          [ 9.6878e-03, -1.3296e-01, -5.7780e-02],\n",
      "          [ 6.1654e-03,  3.8568e-02, -3.9195e-02]],\n",
      "\n",
      "         [[ 5.6548e-02, -2.2183e-02, -1.5870e-01],\n",
      "          [ 1.2138e-01,  1.5898e-01, -1.0449e-01],\n",
      "          [ 3.6741e-02, -1.1299e-01,  2.4198e-03]],\n",
      "\n",
      "         [[-1.0836e-01, -1.1110e-01, -4.2810e-02],\n",
      "          [-7.8578e-02,  6.9719e-02,  1.2894e-01],\n",
      "          [ 1.1558e-02, -7.4511e-02, -2.3770e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9978e-02, -1.0808e-01,  1.5110e-01],\n",
      "          [ 1.2936e-01,  6.6495e-02,  4.7024e-04],\n",
      "          [-4.7242e-03, -1.0013e-01,  6.0522e-02]],\n",
      "\n",
      "         [[ 1.2435e-02,  6.6344e-02, -3.5441e-02],\n",
      "          [-4.8220e-03,  1.6401e-01,  1.6741e-02],\n",
      "          [ 5.1192e-02,  1.1158e-01, -1.4874e-01]],\n",
      "\n",
      "         [[ 8.6227e-02, -1.3202e-01, -1.3351e-01],\n",
      "          [ 5.5776e-02, -4.2022e-03,  3.0210e-02],\n",
      "          [ 1.3627e-01,  7.6975e-02, -1.0973e-01]],\n",
      "\n",
      "         [[ 6.4697e-02, -1.2984e-01,  9.8713e-02],\n",
      "          [ 8.6322e-02,  1.3497e-01,  4.8326e-02],\n",
      "          [-1.2537e-01, -1.2923e-01,  3.1029e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4071e-01, -4.6887e-02,  5.2225e-02],\n",
      "          [-1.1682e-01, -1.6443e-02, -1.0144e-02],\n",
      "          [-8.0450e-02,  4.2975e-02, -1.1903e-01]],\n",
      "\n",
      "         [[ 5.2139e-02, -1.2735e-01,  2.1783e-02],\n",
      "          [ 1.1092e-01, -1.5457e-01,  1.0519e-01],\n",
      "          [-6.5824e-02, -9.1418e-02,  2.8844e-03]],\n",
      "\n",
      "         [[-6.9633e-02,  1.0009e-02, -8.3939e-02],\n",
      "          [-1.5814e-01, -1.0780e-01, -1.6528e-02],\n",
      "          [-1.4562e-01,  1.6051e-01, -1.2680e-02]],\n",
      "\n",
      "         [[-1.3928e-01, -4.5479e-02,  1.0237e-01],\n",
      "          [ 1.0528e-01,  1.1859e-02,  1.6138e-01],\n",
      "          [ 1.0626e-01, -1.2315e-01,  1.4872e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9703e-02,  4.9307e-02, -9.1250e-02],\n",
      "          [-1.0825e-02,  1.4034e-01,  1.6200e-01],\n",
      "          [ 1.1225e-01,  1.2649e-01,  1.3142e-01]],\n",
      "\n",
      "         [[ 1.1230e-01,  5.6115e-02, -1.3389e-01],\n",
      "          [ 1.6437e-01, -1.4614e-01,  1.1210e-01],\n",
      "          [ 2.0901e-02,  1.0919e-01,  3.6635e-02]],\n",
      "\n",
      "         [[ 8.9583e-02,  1.0003e-01, -7.3951e-02],\n",
      "          [ 3.1458e-03,  1.4987e-02, -3.0221e-02],\n",
      "          [ 1.2416e-01,  1.2514e-01,  1.1811e-03]],\n",
      "\n",
      "         [[-9.9454e-02,  1.5090e-01, -9.6278e-03],\n",
      "          [ 1.2716e-01,  8.5661e-02, -5.3998e-02],\n",
      "          [-2.3826e-02,  1.4641e-01,  6.6900e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5163e-01, -1.2792e-01,  5.8412e-02],\n",
      "          [ 1.6361e-01, -1.5502e-01, -1.0283e-01],\n",
      "          [ 3.5596e-02,  4.2119e-02,  1.3741e-01]],\n",
      "\n",
      "         [[ 8.8273e-02, -2.0656e-02,  1.5287e-01],\n",
      "          [-4.9307e-02,  2.5434e-02, -5.6089e-03],\n",
      "          [-1.0297e-05, -1.0339e-01, -8.5841e-02]],\n",
      "\n",
      "         [[-4.9138e-03,  4.8292e-02,  2.3774e-02],\n",
      "          [-8.7204e-02,  9.5143e-03, -1.5686e-01],\n",
      "          [-1.2573e-01, -2.5819e-03,  3.6667e-02]],\n",
      "\n",
      "         [[ 7.8625e-02, -1.4556e-01,  8.4233e-02],\n",
      "          [-1.9085e-03,  1.1752e-01,  2.4402e-02],\n",
      "          [-4.2966e-02,  7.0911e-02, -7.6456e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7555e-02,  7.6840e-02, -3.4220e-02],\n",
      "          [-1.4314e-01, -1.2172e-01,  9.2902e-02],\n",
      "          [-7.8366e-02, -5.5247e-03, -2.0488e-02]],\n",
      "\n",
      "         [[-1.3432e-01, -1.1425e-01, -2.0767e-02],\n",
      "          [ 6.1636e-02,  1.3234e-01,  9.8070e-02],\n",
      "          [-8.7682e-03,  9.6868e-02,  8.2483e-02]],\n",
      "\n",
      "         [[-1.5633e-01, -5.5776e-02,  7.8871e-02],\n",
      "          [ 9.2738e-02, -9.1327e-02, -2.5464e-02],\n",
      "          [ 9.9202e-02,  3.2142e-02,  5.1596e-02]],\n",
      "\n",
      "         [[ 5.0102e-02, -1.2274e-01, -6.9407e-02],\n",
      "          [ 9.7974e-02, -8.7524e-02,  1.0907e-02],\n",
      "          [-4.9150e-02, -7.0065e-02, -1.3316e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n",
      "          [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n",
      "          [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n",
      "\n",
      "         [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n",
      "          [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n",
      "          [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n",
      "\n",
      "         [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n",
      "          [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n",
      "          [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n",
      "\n",
      "         [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n",
      "          [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n",
      "          [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5063e-01, -5.9558e-02,  6.4863e-02],\n",
      "          [-1.4339e-01, -1.4985e-01,  2.6524e-02],\n",
      "          [ 1.0794e-02, -9.9042e-02, -8.4866e-03]],\n",
      "\n",
      "         [[-1.5534e-01, -8.6753e-02,  4.9951e-02],\n",
      "          [ 1.3370e-01, -5.7628e-02,  6.5397e-02],\n",
      "          [-9.0536e-02,  1.1109e-01, -8.5114e-02]],\n",
      "\n",
      "         [[-1.3785e-01,  9.5587e-02, -1.0950e-01],\n",
      "          [-7.5067e-02,  1.1620e-01,  1.9314e-02],\n",
      "          [-5.5802e-02,  7.6196e-02,  1.3980e-01]],\n",
      "\n",
      "         [[-5.2662e-02,  1.4150e-01, -1.0820e-01],\n",
      "          [ 9.3876e-02,  1.2421e-01, -7.9089e-02],\n",
      "          [-1.3670e-01,  1.5867e-01,  1.5091e-01]]]])), ('conv2.bias', tensor([ 0.1667, -0.1424,  0.0724,  0.0755,  0.1017,  0.1216,  0.1630, -0.1584,\n",
      "         0.1059, -0.0842, -0.0969,  0.1154])), ('linear1.weight', tensor([[-1.4098e-02,  3.0289e-02,  3.0587e-02,  ...,  3.9514e-02,\n",
      "          2.7737e-02,  5.5783e-02],\n",
      "        [-4.8542e-02, -8.4717e-03, -3.3017e-03,  ...,  7.3422e-05,\n",
      "          4.8121e-02,  4.6231e-02],\n",
      "        [-3.7414e-02,  4.5861e-02, -6.5247e-03,  ...,  2.6695e-02,\n",
      "          3.7199e-02, -4.9246e-02],\n",
      "        ...,\n",
      "        [ 4.3782e-02,  3.3919e-02, -2.2004e-02,  ..., -5.2779e-02,\n",
      "          3.5255e-02, -9.2468e-03],\n",
      "        [ 8.0068e-03,  8.6617e-04,  2.7932e-02,  ...,  2.6480e-02,\n",
      "          4.9556e-02,  2.0348e-02],\n",
      "        [-3.9863e-02, -2.9890e-02,  4.7548e-02,  ...,  6.2217e-03,\n",
      "         -4.1600e-02,  5.5578e-02]])), ('linear1.bias', tensor([ 2.8120e-02,  4.4694e-02,  4.4684e-02, -2.9737e-03,  4.9443e-02,\n",
      "        -2.8751e-02,  4.1877e-02, -5.5969e-05,  8.2151e-03, -2.9368e-02]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.315302  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.897614 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.841245  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.844014 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.757914  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.814876 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.721906  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.791946 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.695034  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.771850 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.674632  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.756978 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.654719  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.744918 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.639744  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.734921 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.626450  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.725954 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.616734  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.718295 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.607073  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.711595 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.600420  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.705858 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.594097  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.700428 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.588380  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.695869 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.582832  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.691350 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.578256  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.687219 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.574542  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.683291 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.570157  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.680120 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.566776  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.676168 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.562895  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.673057 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.559875  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.670386 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.556139  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.667475 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.553471  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.664759 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.549542  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.662418 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.547164  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.660051 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.544221  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.657872 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.540998  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.655623 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.538393  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.653685 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.536252  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.651778 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.535904  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.649873 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.533338  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.648209 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.530855  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.646513 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.528975  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.644843 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.527378  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.643121 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.525062  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.641562 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.523387  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.639680 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.520941  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.638343 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.519747  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.636986 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.518105  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.635596 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.517453  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.634334 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.516072  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.633118 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.515342  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.631953 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.513789  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.630558 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.513333  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.629386 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.512854  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.627699 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.512729  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.626483 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.510897  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.625327 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.509729  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.624056 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.509823  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.622492 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.509303  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.621463 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.509162  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.620589 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.509299  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.619656 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.508771  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.619356 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.508628  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.618261 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.508085  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.617352 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.507331  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.616922 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.506896  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.616128 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.506810  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.615404 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.506584  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.614726 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.506329  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.614091 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 0.4309,  0.7635,  0.4031, -0.1265, -0.1777],\n                        [ 0.2771,  0.5113,  0.4370, -0.1511, -0.3245],\n                        [ 0.2340,  0.2092,  0.3924,  0.2479, -0.0751],\n                        [ 0.4789,  0.5039,  0.4232, -0.1991,  0.0963],\n                        [ 0.6423,  0.5881,  0.2937,  0.1533, -0.1147]]],\n              \n              \n                      [[[ 0.7746,  0.6257,  0.5472,  0.6948,  0.3142],\n                        [ 0.5473,  0.7029,  0.8824,  0.9372,  0.2170],\n                        [-0.0467,  0.0691,  0.2509,  0.3820,  0.1022],\n                        [-0.4274,  0.0692,  0.3995,  0.0965, -0.1380],\n                        [-0.3820, -0.0621,  0.0978,  0.1411, -0.2133]]],\n              \n              \n                      [[[ 0.1741,  0.2850,  0.1097, -0.0692, -0.2193],\n                        [ 0.1892,  0.3428,  0.3135,  0.3275,  0.3277],\n                        [ 0.0565,  0.2706,  0.4631,  0.1718,  0.0866],\n                        [-0.2073,  0.4428,  0.5828,  0.1748,  0.1196],\n                        [-0.2231,  0.5780,  0.5525,  0.3545,  0.3566]]],\n              \n              \n                      [[[-0.2817, -0.5277, -0.3571, -0.0114,  0.0576],\n                        [ 0.0841, -0.3449, -0.6153,  0.3376,  1.0039],\n                        [-0.1700,  0.3203,  0.1607,  0.6807,  0.4083],\n                        [-0.1081, -0.1599, -0.0269,  0.9351,  0.9711],\n                        [ 0.2070,  0.0417,  0.1902,  0.7096,  0.8556]]]])),\n             ('conv1.bias', tensor([0.0565, 0.5858, 0.0147, 0.3793])),\n             ('conv2.weight',\n              tensor([[[[-2.8973e-01, -1.9191e-01,  2.5472e-01],\n                        [-1.4496e-01, -7.3766e-02,  8.7439e-02],\n                        [-2.1811e-01,  8.2486e-02,  3.2213e-02]],\n              \n                       [[-4.5587e-02,  1.1083e-01,  2.5729e-01],\n                        [ 6.4917e-02,  1.8636e-01,  1.5562e-01],\n                        [-3.4807e-03,  2.7113e-03, -9.1973e-02]],\n              \n                       [[-1.6008e-01, -2.6417e-02, -9.8313e-02],\n                        [-1.7445e-01,  2.1991e-01, -2.1399e-01],\n                        [-6.4316e-02,  1.0308e-01, -2.0769e-01]],\n              \n                       [[ 1.2134e-01,  3.0895e-02, -2.4475e-01],\n                        [ 2.8466e-01, -7.1204e-02, -6.3307e-02],\n                        [ 4.5453e-01, -6.2077e-02,  2.3842e-01]]],\n              \n              \n                      [[[-1.3380e-01,  7.7585e-03, -3.2908e-02],\n                        [ 8.6875e-02,  5.1443e-02,  1.6961e-01],\n                        [-3.2243e-02,  1.7783e-01, -3.6175e-02]],\n              \n                       [[ 3.3498e-02, -1.1662e-01, -1.0995e-01],\n                        [ 3.4579e-02, -8.1701e-02, -9.9834e-02],\n                        [-7.8247e-02,  1.8302e-01,  2.1793e-01]],\n              \n                       [[-7.4647e-02,  1.2281e-01,  4.9907e-02],\n                        [ 1.1562e-01,  1.8384e-01,  2.0592e-01],\n                        [ 3.6861e-02, -2.0585e-02,  1.4420e-01]],\n              \n                       [[-7.5390e-02,  9.4819e-02,  2.5187e-02],\n                        [ 1.2978e-01,  1.1367e-01,  2.0675e-02],\n                        [-9.9195e-02,  1.0105e-01,  2.6519e-01]]],\n              \n              \n                      [[[-1.8253e-01, -4.5895e-02,  2.0367e-02],\n                        [-2.5443e-01, -6.0765e-02,  1.6004e-01],\n                        [-2.6438e-01, -9.1947e-02,  1.8870e-02]],\n              \n                       [[ 4.2792e-01,  1.0041e-01,  3.9682e-01],\n                        [-2.5919e-02, -4.7380e-02,  5.9521e-02],\n                        [-1.0859e-01,  9.5461e-02,  1.1163e-01]],\n              \n                       [[ 1.6613e-01, -5.2394e-02,  2.1807e-01],\n                        [-6.0257e-02,  2.2692e-02,  1.6560e-01],\n                        [-2.8930e-02, -1.6775e-01,  1.5152e-02]],\n              \n                       [[-2.1523e-01,  9.7621e-02,  6.9690e-02],\n                        [-1.6247e-01,  2.1705e-01,  1.7962e-01],\n                        [-2.2848e-02,  7.3487e-02, -1.5696e-05]]],\n              \n              \n                      [[[ 9.4707e-02,  4.7579e-01,  4.7911e-02],\n                        [ 1.3220e-01,  2.7828e-02, -1.5377e-01],\n                        [-2.5695e-01, -1.9320e-01, -3.7351e-01]],\n              \n                       [[ 4.7191e-01,  4.9496e-01,  2.5038e-01],\n                        [ 8.5161e-03,  7.0475e-02,  7.1557e-03],\n                        [-3.5815e-02, -2.5981e-01,  1.0229e-01]],\n              \n                       [[ 1.3349e-01,  1.0684e-02, -1.2347e-01],\n                        [ 2.3022e-02,  3.8251e-02,  2.9341e-02],\n                        [-2.0745e-01, -4.8371e-01, -3.2476e-02]],\n              \n                       [[ 2.0275e-01, -5.0196e-02, -3.0928e-01],\n                        [-1.4643e-01, -2.5717e-01, -1.7866e-01],\n                        [-5.0775e-01,  2.1970e-02,  3.7860e-01]]],\n              \n              \n                      [[[ 2.4646e-01, -1.4197e-01,  7.7013e-02],\n                        [ 4.2247e-01, -8.1834e-03, -3.7788e-01],\n                        [ 3.8123e-01,  5.6568e-02, -2.4518e-01]],\n              \n                       [[ 2.4723e-01,  2.2044e-01,  3.4628e-01],\n                        [ 1.5505e-01, -2.9574e-01, -3.0306e-01],\n                        [ 2.4024e-01,  6.1626e-03,  1.2360e-02]],\n              \n                       [[ 1.3386e-01, -1.5714e-01, -2.4621e-01],\n                        [ 2.6004e-01, -4.3409e-02, -3.8503e-01],\n                        [ 2.6103e-01, -1.2369e-01,  9.2268e-03]],\n              \n                       [[-2.5518e-01, -2.5871e-01, -5.7167e-01],\n                        [-1.9502e-01,  6.6982e-02,  3.3110e-02],\n                        [ 1.4078e-01,  8.4287e-02,  1.3357e-01]]],\n              \n              \n                      [[[ 8.2321e-02, -3.2517e-02,  8.6965e-02],\n                        [ 1.6040e-01,  5.9320e-02, -1.8619e-01],\n                        [ 5.2904e-02,  2.0985e-01,  1.3011e-01]],\n              \n                       [[ 1.4681e-01,  7.4784e-02, -2.6543e-01],\n                        [ 9.2006e-02,  1.7836e-01, -7.2439e-02],\n                        [ 1.5387e-01,  1.5861e-01, -4.6167e-01]],\n              \n                       [[ 1.7663e-01, -2.0452e-01, -2.3684e-01],\n                        [ 2.8534e-01, -5.1887e-02, -1.9880e-01],\n                        [ 4.1349e-01,  2.6422e-01, -3.8006e-01]],\n              \n                       [[ 6.1604e-02, -2.4317e-01,  1.1153e-01],\n                        [ 2.0448e-01,  1.0766e-01, -2.4157e-01],\n                        [ 3.3217e-01, -1.2014e-01, -2.4565e-01]]],\n              \n              \n                      [[[ 3.9864e-01,  1.6399e-01, -1.9483e-01],\n                        [-7.1534e-02,  2.5130e-02, -9.5722e-02],\n                        [-7.7160e-02,  2.5700e-01, -2.2025e-01]],\n              \n                       [[ 2.8191e-01, -2.1301e-01, -1.8868e-01],\n                        [ 1.6978e-01, -1.6489e-01,  1.8527e-01],\n                        [ 3.3163e-02, -9.9524e-02,  3.9235e-02]],\n              \n                       [[ 1.1478e-01, -1.2490e-01, -2.4872e-01],\n                        [-6.5096e-02, -1.5078e-01, -1.9312e-03],\n                        [-5.2592e-02,  1.6797e-01, -3.9188e-02]],\n              \n                       [[-1.9865e-01, -3.6848e-01,  4.0550e-01],\n                        [ 1.3634e-01, -1.3860e-01,  2.8025e-01],\n                        [ 1.3391e-01, -2.2423e-01,  1.9758e-01]]],\n              \n              \n                      [[[ 2.5150e-02, -2.2010e-02, -1.1195e-01],\n                        [-4.5238e-02,  2.9629e-02,  9.6498e-02],\n                        [ 5.3504e-02,  1.3692e-01,  2.7709e-01]],\n              \n                       [[ 1.6211e-01,  2.8789e-03, -2.2422e-01],\n                        [ 1.4691e-01, -2.1098e-01,  1.2861e-01],\n                        [-2.4239e-02,  1.0701e-01,  5.1154e-02]],\n              \n                       [[ 8.1539e-02,  2.3689e-03, -1.2689e-01],\n                        [-2.6674e-02,  4.4012e-02, -9.8883e-02],\n                        [ 1.6372e-01,  2.6135e-01,  4.4438e-02]],\n              \n                       [[-1.8739e-01, -5.5380e-02, -2.3443e-03],\n                        [ 1.2331e-01,  5.9478e-02, -2.7793e-02],\n                        [ 1.8089e-01,  2.5118e-01,  9.1009e-02]]],\n              \n              \n                      [[[ 3.6581e-02, -2.9030e-01,  1.3777e-01],\n                        [ 8.9694e-02, -4.3149e-01, -7.5106e-02],\n                        [ 7.7525e-02, -2.4775e-01,  1.7994e-01]],\n              \n                       [[ 2.0623e-01,  8.3999e-02,  2.8345e-01],\n                        [-1.4858e-01, -3.6538e-02,  1.4627e-01],\n                        [-1.0542e-01, -1.4562e-01,  4.1886e-02]],\n              \n                       [[-9.5478e-03,  5.7495e-02,  1.9483e-01],\n                        [-2.5378e-01, -9.7013e-02,  7.3616e-03],\n                        [-2.4056e-01, -4.9712e-02,  1.9532e-01]],\n              \n                       [[-9.4090e-02,  8.2104e-02,  3.4345e-02],\n                        [-2.6490e-01,  4.0461e-01, -6.9858e-04],\n                        [-2.2173e-01,  3.9761e-01, -5.6778e-02]]],\n              \n              \n                      [[[ 1.7480e-01,  1.9795e-01, -5.7388e-03],\n                        [-1.2558e-01, -1.6676e-01,  2.5515e-02],\n                        [-1.0221e-01, -2.4138e-02, -6.5456e-02]],\n              \n                       [[-5.8313e-02, -9.4567e-03,  6.7806e-02],\n                        [ 1.9851e-01,  1.9739e-01,  1.9596e-01],\n                        [-8.3882e-02,  3.5721e-02, -1.9237e-02]],\n              \n                       [[-1.6930e-02,  1.9251e-02,  1.8720e-01],\n                        [ 1.6523e-01, -1.0697e-01, -1.8241e-02],\n                        [ 1.3424e-01,  2.1462e-02, -5.3437e-02]],\n              \n                       [[ 2.1618e-01, -1.5478e-01,  2.6410e-01],\n                        [ 1.6668e-02, -2.0487e-01,  1.0109e-01],\n                        [ 2.0563e-02, -1.1608e-01, -4.7705e-02]]],\n              \n              \n                      [[[-8.2231e-02, -1.8768e-02, -4.4741e-02],\n                        [-2.2228e-02, -3.2686e-03, -2.2865e-02],\n                        [-6.7609e-02, -6.4106e-02,  1.3547e-01]],\n              \n                       [[-1.2501e-01, -1.5709e-01, -5.4788e-02],\n                        [-1.5385e-01, -6.4451e-03, -1.4145e-02],\n                        [ 7.7811e-02, -1.5477e-01, -3.1027e-02]],\n              \n                       [[ 2.5587e-02, -4.9829e-03, -1.1092e-01],\n                        [ 1.3913e-01, -1.2155e-02, -1.6493e-01],\n                        [-1.4344e-02, -5.8041e-02, -5.9348e-02]],\n              \n                       [[-1.1038e-01,  1.2795e-01, -1.6821e-02],\n                        [ 1.5903e-01,  9.5597e-02, -1.0510e-01],\n                        [-1.6090e-01, -1.2572e-01,  9.6878e-02]]],\n              \n              \n                      [[[-2.8267e-01, -3.0029e-01,  3.0673e-02],\n                        [-1.5829e-01, -1.9213e-01,  5.0117e-02],\n                        [-8.5868e-03, -3.7516e-02,  2.2585e-01]],\n              \n                       [[-4.3420e-01, -3.4318e-01,  5.8877e-02],\n                        [ 1.1376e-01, -1.1679e-01,  7.5850e-02],\n                        [-6.4442e-02,  2.3878e-01,  7.8654e-02]],\n              \n                       [[-2.9267e-01, -3.8967e-02, -1.3392e-01],\n                        [-6.5002e-02,  1.7423e-01,  1.0796e-01],\n                        [-8.0662e-02,  2.2211e-01,  4.4286e-01]],\n              \n                       [[-2.1416e-01,  1.5721e-01, -2.1226e-02],\n                        [ 1.2492e-01,  2.7496e-01,  1.1926e-01],\n                        [-1.2057e-01,  4.9085e-01,  4.6319e-01]]]])),\n             ('conv2.bias',\n              tensor([ 0.6118, -0.2413,  0.3966,  0.3992,  0.7223,  0.4575,  0.1253, -0.2043,\n                       0.5085, -0.1979, -0.0969,  0.3054])),\n             ('linear1.weight',\n              tensor([[ 0.0705, -0.1155,  0.0186,  ..., -0.1284, -0.0349, -0.1706],\n                      [-0.0638, -0.0366, -0.1015,  ..., -0.2960,  0.2634,  0.0373],\n                      [ 0.1682,  0.0181,  0.0420,  ..., -0.0653, -0.0076, -0.0146],\n                      ...,\n                      [ 0.2195,  0.1533, -0.0356,  ..., -0.4443, -0.2881, -0.0448],\n                      [ 0.1363, -0.2639,  0.1975,  ...,  0.1849,  0.1405, -0.0687],\n                      [-0.0258, -0.0060,  0.1240,  ...,  0.2279,  0.1679,  0.1211]])),\n             ('linear1.bias',\n              tensor([ 0.0741, -0.2659,  0.1965, -0.0082, -0.1621,  0.0142,  0.2818,  0.1693,\n                       0.1963, -0.0177]))])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a2_dt.state_dict())\n",
    "df_net_a2_dt = train_test(device, train_dataloader, test_dataloader, net_a2_dt, learning_rate, epochs)\n",
    "df_net_a2_dt.to_csv('NetA2DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2DT_trained.pt')\n",
    "net_a2_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T07:02:02.957710Z",
     "start_time": "2024-06-19T06:55:59.075344Z"
    }
   },
   "id": "d2e02ef68d3a9f05",
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
