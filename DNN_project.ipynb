{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.models.optical_flow.raft import ResidualBlock\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.409669Z",
     "start_time": "2024-06-16T19:12:25.193655Z"
    }
   },
   "id": "685462a85f2ae1e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.415073Z",
     "start_time": "2024-06-16T19:12:28.410674Z"
    }
   },
   "id": "8fb89e7978c49f98",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bdd2924b375a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA1(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(676, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "            \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.423733Z",
     "start_time": "2024-06-16T19:12:28.416078Z"
    }
   },
   "id": "8e067a677d103ee6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA2(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=6, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(216, 260)\n",
    "        self.linear2 = nn.Linear(260, 160)\n",
    "        self.linear3 = nn.Linear(160, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.433016Z",
     "start_time": "2024-06-16T19:12:28.426738Z"
    }
   },
   "id": "4f398a6ac2c01f29",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99360d229c0dbc75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1, 3, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization_weights = torch.tensor([\n",
    "    [[[1, 0, 1], [0, 1, 0], [1, 0, 1]]],\n",
    "    [[[1, 1, 0], [0, 0, 1], [1, 1, 0]]],\n",
    "    [[[0, 1, 1], [1, 0, 0], [0, 1, 1]]],\n",
    "    [[[0, 1, 0], [1, 1, 0], [0, 1, 0]]]\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "initialization_biases = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "initialization_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.443996Z",
     "start_time": "2024-06-16T19:12:28.435022Z"
    }
   },
   "id": "3e5ae64e8c459822",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1285,  0.1293, -0.1389],\n",
      "          [-0.0664,  0.0077, -0.1630],\n",
      "          [-0.0279,  0.0455,  0.1343]],\n",
      "\n",
      "         [[ 0.1068, -0.1197,  0.0090],\n",
      "          [-0.0064, -0.0420, -0.0895],\n",
      "          [-0.1413,  0.0500,  0.0897]],\n",
      "\n",
      "         [[-0.0711, -0.1296, -0.1002],\n",
      "          [-0.0239,  0.0477,  0.1364],\n",
      "          [-0.0958, -0.1656, -0.1276]],\n",
      "\n",
      "         [[-0.1230, -0.0910,  0.0032],\n",
      "          [-0.0958, -0.1549, -0.1324],\n",
      "          [ 0.1377, -0.1392,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510,  0.0343,  0.1636],\n",
      "          [ 0.0559,  0.0953, -0.1076],\n",
      "          [ 0.0381,  0.0672, -0.1269]],\n",
      "\n",
      "         [[ 0.0494, -0.1396, -0.0025],\n",
      "          [-0.1630, -0.1530,  0.0134],\n",
      "          [ 0.0688, -0.1538, -0.0249]],\n",
      "\n",
      "         [[-0.1279,  0.0739,  0.0044],\n",
      "          [-0.0093,  0.0977, -0.0246],\n",
      "          [-0.1637, -0.0414, -0.0718]],\n",
      "\n",
      "         [[-0.1459, -0.1286, -0.1408],\n",
      "          [-0.0520,  0.1448,  0.1576],\n",
      "          [ 0.0211,  0.1597,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154,  0.1175, -0.1635],\n",
      "          [-0.0776, -0.0315,  0.1326],\n",
      "          [ 0.0815,  0.0204, -0.0603]],\n",
      "\n",
      "         [[ 0.0975, -0.1333,  0.0701],\n",
      "          [ 0.0633, -0.0995, -0.1419],\n",
      "          [-0.1589, -0.0483,  0.0435]],\n",
      "\n",
      "         [[-0.0155, -0.0939, -0.0870],\n",
      "          [-0.0359, -0.0107, -0.0814],\n",
      "          [ 0.0747, -0.0659, -0.0912]],\n",
      "\n",
      "         [[ 0.0312, -0.0822,  0.0852],\n",
      "          [-0.1027,  0.0658,  0.1524],\n",
      "          [-0.0611,  0.0508, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0886, -0.0288, -0.0476],\n",
      "          [ 0.0636, -0.1219, -0.1228],\n",
      "          [-0.1626,  0.1552, -0.0104]],\n",
      "\n",
      "         [[-0.0444, -0.0287, -0.0863],\n",
      "          [-0.1455,  0.0695,  0.1488],\n",
      "          [ 0.0660,  0.0844,  0.1053]],\n",
      "\n",
      "         [[-0.0399, -0.0200, -0.1058],\n",
      "          [ 0.0172,  0.1119,  0.1368],\n",
      "          [ 0.0402, -0.0858, -0.0749]],\n",
      "\n",
      "         [[-0.0745,  0.1459, -0.1552],\n",
      "          [-0.1600,  0.0574, -0.1118],\n",
      "          [ 0.0488, -0.1259,  0.1628]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872, -0.1264, -0.0872],\n",
      "          [-0.0671,  0.1279, -0.0624],\n",
      "          [-0.0455,  0.0237, -0.1169]],\n",
      "\n",
      "         [[ 0.1078, -0.0462,  0.1611],\n",
      "          [ 0.0378,  0.0042,  0.0240],\n",
      "          [-0.0082, -0.0213, -0.0640]],\n",
      "\n",
      "         [[-0.0785, -0.1561,  0.1334],\n",
      "          [ 0.0412,  0.0912,  0.0069],\n",
      "          [ 0.0332, -0.0704, -0.1196]],\n",
      "\n",
      "         [[-0.0315, -0.1624, -0.1553],\n",
      "          [-0.0109, -0.1157, -0.1260],\n",
      "          [-0.1114,  0.1594,  0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290, -0.0769, -0.0430],\n",
      "          [ 0.0620, -0.0631, -0.0678],\n",
      "          [-0.1418,  0.0943, -0.1289]],\n",
      "\n",
      "         [[-0.1345, -0.0887,  0.0157],\n",
      "          [ 0.0949, -0.0884,  0.1076],\n",
      "          [ 0.0698, -0.0051, -0.1075]],\n",
      "\n",
      "         [[-0.0982, -0.1582,  0.0671],\n",
      "          [ 0.0299, -0.0899, -0.0835],\n",
      "          [-0.0357, -0.1117, -0.0830]],\n",
      "\n",
      "         [[-0.1652, -0.0075,  0.0882],\n",
      "          [ 0.1611, -0.1534,  0.1237],\n",
      "          [ 0.1374, -0.1394, -0.1483]]]])), ('conv2.bias', tensor([ 0.1339,  0.0760, -0.1164,  0.0165, -0.0154,  0.0566])), ('linear1.weight', tensor([[-0.0620, -0.0663,  0.0086,  ..., -0.0562,  0.0230, -0.0066],\n",
      "        [-0.0331,  0.0079, -0.0449,  ...,  0.0446,  0.0275, -0.0118],\n",
      "        [ 0.0647,  0.0101,  0.0594,  ..., -0.0497,  0.0610,  0.0487],\n",
      "        ...,\n",
      "        [-0.0569,  0.0372, -0.0361,  ..., -0.0561, -0.0232,  0.0188],\n",
      "        [-0.0152, -0.0105, -0.0606,  ..., -0.0606,  0.0094, -0.0161],\n",
      "        [ 0.0504, -0.0006, -0.0249,  ..., -0.0138,  0.0210, -0.0435]])), ('linear1.bias', tensor([-6.0873e-02,  6.5560e-02, -4.3044e-02, -6.3863e-03, -4.4803e-02,\n",
      "        -3.3020e-02, -1.3932e-02,  7.4465e-03,  2.3414e-02, -5.0514e-02,\n",
      "        -2.0304e-02,  3.5026e-02, -2.9176e-02, -6.7145e-02,  1.9335e-02,\n",
      "         1.5226e-02, -3.8411e-02,  5.4850e-02, -2.9086e-04,  7.3328e-03,\n",
      "         2.0166e-02, -5.9463e-02,  1.2512e-02,  3.8834e-02, -1.2995e-02,\n",
      "        -2.0696e-03, -5.7886e-02, -6.6434e-02,  2.0881e-02,  6.2669e-02,\n",
      "        -6.7346e-03,  5.1529e-02, -4.3780e-02, -4.4061e-02,  3.4837e-02,\n",
      "         6.1586e-03,  3.7851e-02,  6.2219e-02,  1.2197e-02,  4.9986e-02,\n",
      "         1.9788e-02, -6.0168e-02,  4.9329e-02,  2.1731e-02, -4.9728e-02,\n",
      "         5.4118e-02, -8.4684e-04, -1.0249e-02,  2.7321e-02, -5.3289e-02,\n",
      "        -6.0395e-02,  3.1339e-02, -1.4949e-02,  4.6386e-03,  6.4307e-02,\n",
      "         8.9219e-03, -3.7237e-02,  5.2332e-02,  2.8919e-02, -2.7775e-02,\n",
      "         9.4815e-03,  5.6762e-02,  4.4966e-02,  1.0521e-02, -4.0578e-02,\n",
      "         1.3906e-02, -2.5521e-02, -5.4316e-02,  1.2339e-02,  4.2722e-02,\n",
      "         1.9184e-02, -6.1930e-02,  1.9331e-02, -2.1756e-02,  3.7002e-02,\n",
      "         4.5658e-02,  4.0470e-02,  2.1073e-02,  5.3643e-02, -5.4933e-02,\n",
      "        -1.8724e-02, -4.5842e-02, -5.3424e-02, -2.8775e-02, -1.7706e-02,\n",
      "         6.6402e-02, -1.3323e-02,  5.3333e-02,  4.5054e-02,  4.1214e-02,\n",
      "        -7.3912e-03, -6.4565e-02, -1.3194e-02,  6.6996e-02, -3.9084e-02,\n",
      "         6.4009e-02,  3.1221e-02, -6.5513e-02,  1.0571e-03, -6.5073e-02,\n",
      "        -3.4405e-02,  5.2692e-02,  5.1586e-02, -6.6245e-02,  3.3782e-02,\n",
      "        -2.6436e-02,  2.4386e-02, -3.2056e-02, -1.3929e-02,  2.4294e-02,\n",
      "        -3.9106e-02,  6.4875e-02, -1.0712e-03,  1.7288e-02,  6.6765e-05,\n",
      "        -8.5600e-04,  4.1666e-02,  1.6196e-02,  3.9983e-02,  4.9545e-02,\n",
      "        -2.1091e-02, -3.4429e-02, -4.9302e-02,  1.8530e-02, -6.6794e-02,\n",
      "        -4.1074e-02, -5.8870e-02, -5.8249e-02, -6.1702e-02, -2.1042e-02,\n",
      "         5.7717e-02,  4.6078e-02, -3.1355e-02, -1.0239e-02, -3.8538e-02,\n",
      "         8.5815e-04,  6.6288e-02,  6.7854e-02, -1.2928e-02, -2.1495e-02,\n",
      "         4.4455e-02,  4.5449e-02, -2.8614e-02, -4.0425e-03,  5.6584e-02,\n",
      "        -5.5950e-02, -1.3736e-02, -4.0708e-02, -4.3129e-02,  2.3393e-02,\n",
      "         5.2120e-02,  4.6945e-02,  2.3704e-02, -4.4895e-03,  2.3276e-03,\n",
      "        -4.6276e-02, -4.6073e-02,  2.6573e-02,  2.5198e-02, -5.5994e-02,\n",
      "        -2.7689e-03,  6.2543e-02, -5.4211e-02,  2.7657e-02, -3.5267e-03,\n",
      "         2.1026e-02,  3.9520e-02, -8.1219e-03,  2.0721e-02,  3.3508e-02,\n",
      "        -3.0952e-02, -6.1246e-02,  4.0363e-02,  5.5333e-02, -6.5602e-02,\n",
      "        -5.5229e-02,  1.2202e-02,  3.0445e-02,  1.1241e-02, -5.0941e-02,\n",
      "         6.4340e-02, -4.8585e-02,  2.7508e-02, -2.9939e-02, -6.3172e-03,\n",
      "        -2.8367e-02, -3.0125e-02,  3.2778e-02, -2.9546e-02,  1.1376e-02,\n",
      "         8.5921e-03,  3.2469e-02,  7.9027e-03,  6.4648e-02,  2.7774e-02,\n",
      "        -3.8146e-02, -3.4692e-02,  3.2897e-02, -3.9575e-02, -5.5060e-02,\n",
      "         4.6400e-02,  1.4507e-02,  4.9920e-02, -2.8626e-02,  2.5824e-02,\n",
      "        -1.6718e-02, -1.0379e-02, -3.0170e-02, -8.2306e-04, -4.6724e-02,\n",
      "         5.7998e-02,  5.8384e-03, -3.1310e-02,  1.2629e-02, -5.9381e-02,\n",
      "        -7.4855e-04,  1.6694e-02,  6.4214e-02,  4.7362e-02,  2.8770e-03,\n",
      "        -1.1494e-02, -1.2972e-02, -6.3430e-02,  2.6580e-02, -3.4418e-02,\n",
      "         4.5590e-02, -4.7691e-02, -6.0233e-03,  2.5052e-02,  2.0533e-02,\n",
      "         1.4752e-02,  3.2718e-02, -2.9200e-02, -2.8462e-02, -6.6703e-03,\n",
      "         1.9703e-02,  4.1442e-02,  3.8995e-02, -4.9531e-02, -5.6046e-02,\n",
      "         6.1156e-03, -2.4786e-02,  1.7079e-02,  6.0489e-02,  6.6523e-02,\n",
      "         3.9427e-02, -5.0210e-02,  3.5371e-02, -5.4468e-02,  6.2431e-02,\n",
      "        -5.1263e-02,  4.9748e-03, -2.6898e-02, -5.1688e-02, -5.4482e-02,\n",
      "         8.7758e-03, -1.9238e-02, -1.7703e-02, -3.4707e-02,  6.4090e-02])), ('linear2.weight', tensor([[-0.0210,  0.0475,  0.0102,  ...,  0.0603, -0.0074, -0.0214],\n",
      "        [-0.0399,  0.0602, -0.0484,  ...,  0.0567, -0.0230,  0.0023],\n",
      "        [-0.0595, -0.0417,  0.0493,  ...,  0.0065,  0.0484, -0.0585],\n",
      "        ...,\n",
      "        [-0.0352, -0.0373,  0.0312,  ...,  0.0390, -0.0278,  0.0614],\n",
      "        [ 0.0556,  0.0090,  0.0202,  ..., -0.0431, -0.0560, -0.0315],\n",
      "        [ 0.0026,  0.0469, -0.0173,  ...,  0.0148, -0.0476,  0.0277]])), ('linear2.bias', tensor([ 0.0136,  0.0081, -0.0116, -0.0029,  0.0313,  0.0108,  0.0341,  0.0212,\n",
      "         0.0105, -0.0476, -0.0378,  0.0560,  0.0025,  0.0132,  0.0357, -0.0018,\n",
      "        -0.0355, -0.0344, -0.0257,  0.0028,  0.0090, -0.0471, -0.0198, -0.0289,\n",
      "         0.0051, -0.0557, -0.0136,  0.0269, -0.0292, -0.0207,  0.0372,  0.0088,\n",
      "        -0.0127,  0.0411,  0.0063, -0.0238, -0.0405, -0.0142, -0.0022,  0.0213,\n",
      "         0.0314, -0.0392, -0.0189,  0.0003,  0.0357, -0.0584,  0.0223, -0.0426,\n",
      "        -0.0377, -0.0049, -0.0061,  0.0546, -0.0086, -0.0218, -0.0397, -0.0435,\n",
      "         0.0359, -0.0413, -0.0123, -0.0248, -0.0498, -0.0368,  0.0447, -0.0378,\n",
      "         0.0005,  0.0588,  0.0047,  0.0245,  0.0417,  0.0144,  0.0370,  0.0593,\n",
      "        -0.0293,  0.0033,  0.0364, -0.0143, -0.0439, -0.0120, -0.0217, -0.0485,\n",
      "         0.0284,  0.0413,  0.0237,  0.0592,  0.0569, -0.0462, -0.0358,  0.0131,\n",
      "         0.0430,  0.0185, -0.0053,  0.0260,  0.0407, -0.0548,  0.0053, -0.0197,\n",
      "         0.0464,  0.0148, -0.0230,  0.0460, -0.0395, -0.0185,  0.0158, -0.0328,\n",
      "         0.0517,  0.0025, -0.0156, -0.0140, -0.0208,  0.0033,  0.0187,  0.0321,\n",
      "        -0.0274, -0.0005, -0.0181, -0.0473,  0.0144,  0.0450,  0.0007,  0.0032,\n",
      "        -0.0279, -0.0084, -0.0177, -0.0591, -0.0235, -0.0027, -0.0555,  0.0550,\n",
      "        -0.0299, -0.0509, -0.0160,  0.0568,  0.0051,  0.0228,  0.0115,  0.0042,\n",
      "         0.0519, -0.0075, -0.0288,  0.0447,  0.0378, -0.0242,  0.0426, -0.0228,\n",
      "         0.0458, -0.0516,  0.0303, -0.0322, -0.0551,  0.0027, -0.0255,  0.0572,\n",
      "        -0.0596,  0.0400,  0.0454,  0.0466, -0.0382,  0.0161, -0.0478, -0.0207])), ('linear3.weight', tensor([[-0.0706,  0.0681, -0.0080,  ..., -0.0186,  0.0047,  0.0409],\n",
      "        [ 0.0089,  0.0618, -0.0299,  ...,  0.0540,  0.0040, -0.0399],\n",
      "        [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0518, -0.0753,  ..., -0.0176,  0.0297, -0.0049],\n",
      "        [-0.0109, -0.0724,  0.0292,  ..., -0.0603,  0.0476,  0.0132],\n",
      "        [-0.0658,  0.0038, -0.0390,  ...,  0.0289, -0.0218, -0.0459]])), ('linear3.bias', tensor([ 0.0145,  0.0002, -0.0470, -0.0006,  0.0558, -0.0591,  0.0525,  0.0646,\n",
      "         0.0103,  0.0019]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1285,  0.1293, -0.1389],\n",
      "          [-0.0664,  0.0077, -0.1630],\n",
      "          [-0.0279,  0.0455,  0.1343]],\n",
      "\n",
      "         [[ 0.1068, -0.1197,  0.0090],\n",
      "          [-0.0064, -0.0420, -0.0895],\n",
      "          [-0.1413,  0.0500,  0.0897]],\n",
      "\n",
      "         [[-0.0711, -0.1296, -0.1002],\n",
      "          [-0.0239,  0.0477,  0.1364],\n",
      "          [-0.0958, -0.1656, -0.1276]],\n",
      "\n",
      "         [[-0.1230, -0.0910,  0.0032],\n",
      "          [-0.0958, -0.1549, -0.1324],\n",
      "          [ 0.1377, -0.1392,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510,  0.0343,  0.1636],\n",
      "          [ 0.0559,  0.0953, -0.1076],\n",
      "          [ 0.0381,  0.0672, -0.1269]],\n",
      "\n",
      "         [[ 0.0494, -0.1396, -0.0025],\n",
      "          [-0.1630, -0.1530,  0.0134],\n",
      "          [ 0.0688, -0.1538, -0.0249]],\n",
      "\n",
      "         [[-0.1279,  0.0739,  0.0044],\n",
      "          [-0.0093,  0.0977, -0.0246],\n",
      "          [-0.1637, -0.0414, -0.0718]],\n",
      "\n",
      "         [[-0.1459, -0.1286, -0.1408],\n",
      "          [-0.0520,  0.1448,  0.1576],\n",
      "          [ 0.0211,  0.1597,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154,  0.1175, -0.1635],\n",
      "          [-0.0776, -0.0315,  0.1326],\n",
      "          [ 0.0815,  0.0204, -0.0603]],\n",
      "\n",
      "         [[ 0.0975, -0.1333,  0.0701],\n",
      "          [ 0.0633, -0.0995, -0.1419],\n",
      "          [-0.1589, -0.0483,  0.0435]],\n",
      "\n",
      "         [[-0.0155, -0.0939, -0.0870],\n",
      "          [-0.0359, -0.0107, -0.0814],\n",
      "          [ 0.0747, -0.0659, -0.0912]],\n",
      "\n",
      "         [[ 0.0312, -0.0822,  0.0852],\n",
      "          [-0.1027,  0.0658,  0.1524],\n",
      "          [-0.0611,  0.0508, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0886, -0.0288, -0.0476],\n",
      "          [ 0.0636, -0.1219, -0.1228],\n",
      "          [-0.1626,  0.1552, -0.0104]],\n",
      "\n",
      "         [[-0.0444, -0.0287, -0.0863],\n",
      "          [-0.1455,  0.0695,  0.1488],\n",
      "          [ 0.0660,  0.0844,  0.1053]],\n",
      "\n",
      "         [[-0.0399, -0.0200, -0.1058],\n",
      "          [ 0.0172,  0.1119,  0.1368],\n",
      "          [ 0.0402, -0.0858, -0.0749]],\n",
      "\n",
      "         [[-0.0745,  0.1459, -0.1552],\n",
      "          [-0.1600,  0.0574, -0.1118],\n",
      "          [ 0.0488, -0.1259,  0.1628]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872, -0.1264, -0.0872],\n",
      "          [-0.0671,  0.1279, -0.0624],\n",
      "          [-0.0455,  0.0237, -0.1169]],\n",
      "\n",
      "         [[ 0.1078, -0.0462,  0.1611],\n",
      "          [ 0.0378,  0.0042,  0.0240],\n",
      "          [-0.0082, -0.0213, -0.0640]],\n",
      "\n",
      "         [[-0.0785, -0.1561,  0.1334],\n",
      "          [ 0.0412,  0.0912,  0.0069],\n",
      "          [ 0.0332, -0.0704, -0.1196]],\n",
      "\n",
      "         [[-0.0315, -0.1624, -0.1553],\n",
      "          [-0.0109, -0.1157, -0.1260],\n",
      "          [-0.1114,  0.1594,  0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290, -0.0769, -0.0430],\n",
      "          [ 0.0620, -0.0631, -0.0678],\n",
      "          [-0.1418,  0.0943, -0.1289]],\n",
      "\n",
      "         [[-0.1345, -0.0887,  0.0157],\n",
      "          [ 0.0949, -0.0884,  0.1076],\n",
      "          [ 0.0698, -0.0051, -0.1075]],\n",
      "\n",
      "         [[-0.0982, -0.1582,  0.0671],\n",
      "          [ 0.0299, -0.0899, -0.0835],\n",
      "          [-0.0357, -0.1117, -0.0830]],\n",
      "\n",
      "         [[-0.1652, -0.0075,  0.0882],\n",
      "          [ 0.1611, -0.1534,  0.1237],\n",
      "          [ 0.1374, -0.1394, -0.1483]]]])), ('conv2.bias', tensor([ 0.1339,  0.0760, -0.1164,  0.0165, -0.0154,  0.0566])), ('linear1.weight', tensor([[-0.0620, -0.0663,  0.0086,  ..., -0.0562,  0.0230, -0.0066],\n",
      "        [-0.0331,  0.0079, -0.0449,  ...,  0.0446,  0.0275, -0.0118],\n",
      "        [ 0.0647,  0.0101,  0.0594,  ..., -0.0497,  0.0610,  0.0487],\n",
      "        ...,\n",
      "        [-0.0569,  0.0372, -0.0361,  ..., -0.0561, -0.0232,  0.0188],\n",
      "        [-0.0152, -0.0105, -0.0606,  ..., -0.0606,  0.0094, -0.0161],\n",
      "        [ 0.0504, -0.0006, -0.0249,  ..., -0.0138,  0.0210, -0.0435]])), ('linear1.bias', tensor([-6.0873e-02,  6.5560e-02, -4.3044e-02, -6.3863e-03, -4.4803e-02,\n",
      "        -3.3020e-02, -1.3932e-02,  7.4465e-03,  2.3414e-02, -5.0514e-02,\n",
      "        -2.0304e-02,  3.5026e-02, -2.9176e-02, -6.7145e-02,  1.9335e-02,\n",
      "         1.5226e-02, -3.8411e-02,  5.4850e-02, -2.9086e-04,  7.3328e-03,\n",
      "         2.0166e-02, -5.9463e-02,  1.2512e-02,  3.8834e-02, -1.2995e-02,\n",
      "        -2.0696e-03, -5.7886e-02, -6.6434e-02,  2.0881e-02,  6.2669e-02,\n",
      "        -6.7346e-03,  5.1529e-02, -4.3780e-02, -4.4061e-02,  3.4837e-02,\n",
      "         6.1586e-03,  3.7851e-02,  6.2219e-02,  1.2197e-02,  4.9986e-02,\n",
      "         1.9788e-02, -6.0168e-02,  4.9329e-02,  2.1731e-02, -4.9728e-02,\n",
      "         5.4118e-02, -8.4684e-04, -1.0249e-02,  2.7321e-02, -5.3289e-02,\n",
      "        -6.0395e-02,  3.1339e-02, -1.4949e-02,  4.6386e-03,  6.4307e-02,\n",
      "         8.9219e-03, -3.7237e-02,  5.2332e-02,  2.8919e-02, -2.7775e-02,\n",
      "         9.4815e-03,  5.6762e-02,  4.4966e-02,  1.0521e-02, -4.0578e-02,\n",
      "         1.3906e-02, -2.5521e-02, -5.4316e-02,  1.2339e-02,  4.2722e-02,\n",
      "         1.9184e-02, -6.1930e-02,  1.9331e-02, -2.1756e-02,  3.7002e-02,\n",
      "         4.5658e-02,  4.0470e-02,  2.1073e-02,  5.3643e-02, -5.4933e-02,\n",
      "        -1.8724e-02, -4.5842e-02, -5.3424e-02, -2.8775e-02, -1.7706e-02,\n",
      "         6.6402e-02, -1.3323e-02,  5.3333e-02,  4.5054e-02,  4.1214e-02,\n",
      "        -7.3912e-03, -6.4565e-02, -1.3194e-02,  6.6996e-02, -3.9084e-02,\n",
      "         6.4009e-02,  3.1221e-02, -6.5513e-02,  1.0571e-03, -6.5073e-02,\n",
      "        -3.4405e-02,  5.2692e-02,  5.1586e-02, -6.6245e-02,  3.3782e-02,\n",
      "        -2.6436e-02,  2.4386e-02, -3.2056e-02, -1.3929e-02,  2.4294e-02,\n",
      "        -3.9106e-02,  6.4875e-02, -1.0712e-03,  1.7288e-02,  6.6765e-05,\n",
      "        -8.5600e-04,  4.1666e-02,  1.6196e-02,  3.9983e-02,  4.9545e-02,\n",
      "        -2.1091e-02, -3.4429e-02, -4.9302e-02,  1.8530e-02, -6.6794e-02,\n",
      "        -4.1074e-02, -5.8870e-02, -5.8249e-02, -6.1702e-02, -2.1042e-02,\n",
      "         5.7717e-02,  4.6078e-02, -3.1355e-02, -1.0239e-02, -3.8538e-02,\n",
      "         8.5815e-04,  6.6288e-02,  6.7854e-02, -1.2928e-02, -2.1495e-02,\n",
      "         4.4455e-02,  4.5449e-02, -2.8614e-02, -4.0425e-03,  5.6584e-02,\n",
      "        -5.5950e-02, -1.3736e-02, -4.0708e-02, -4.3129e-02,  2.3393e-02,\n",
      "         5.2120e-02,  4.6945e-02,  2.3704e-02, -4.4895e-03,  2.3276e-03,\n",
      "        -4.6276e-02, -4.6073e-02,  2.6573e-02,  2.5198e-02, -5.5994e-02,\n",
      "        -2.7689e-03,  6.2543e-02, -5.4211e-02,  2.7657e-02, -3.5267e-03,\n",
      "         2.1026e-02,  3.9520e-02, -8.1219e-03,  2.0721e-02,  3.3508e-02,\n",
      "        -3.0952e-02, -6.1246e-02,  4.0363e-02,  5.5333e-02, -6.5602e-02,\n",
      "        -5.5229e-02,  1.2202e-02,  3.0445e-02,  1.1241e-02, -5.0941e-02,\n",
      "         6.4340e-02, -4.8585e-02,  2.7508e-02, -2.9939e-02, -6.3172e-03,\n",
      "        -2.8367e-02, -3.0125e-02,  3.2778e-02, -2.9546e-02,  1.1376e-02,\n",
      "         8.5921e-03,  3.2469e-02,  7.9027e-03,  6.4648e-02,  2.7774e-02,\n",
      "        -3.8146e-02, -3.4692e-02,  3.2897e-02, -3.9575e-02, -5.5060e-02,\n",
      "         4.6400e-02,  1.4507e-02,  4.9920e-02, -2.8626e-02,  2.5824e-02,\n",
      "        -1.6718e-02, -1.0379e-02, -3.0170e-02, -8.2306e-04, -4.6724e-02,\n",
      "         5.7998e-02,  5.8384e-03, -3.1310e-02,  1.2629e-02, -5.9381e-02,\n",
      "        -7.4855e-04,  1.6694e-02,  6.4214e-02,  4.7362e-02,  2.8770e-03,\n",
      "        -1.1494e-02, -1.2972e-02, -6.3430e-02,  2.6580e-02, -3.4418e-02,\n",
      "         4.5590e-02, -4.7691e-02, -6.0233e-03,  2.5052e-02,  2.0533e-02,\n",
      "         1.4752e-02,  3.2718e-02, -2.9200e-02, -2.8462e-02, -6.6703e-03,\n",
      "         1.9703e-02,  4.1442e-02,  3.8995e-02, -4.9531e-02, -5.6046e-02,\n",
      "         6.1156e-03, -2.4786e-02,  1.7079e-02,  6.0489e-02,  6.6523e-02,\n",
      "         3.9427e-02, -5.0210e-02,  3.5371e-02, -5.4468e-02,  6.2431e-02,\n",
      "        -5.1263e-02,  4.9748e-03, -2.6898e-02, -5.1688e-02, -5.4482e-02,\n",
      "         8.7758e-03, -1.9238e-02, -1.7703e-02, -3.4707e-02,  6.4090e-02])), ('linear2.weight', tensor([[-0.0210,  0.0475,  0.0102,  ...,  0.0603, -0.0074, -0.0214],\n",
      "        [-0.0399,  0.0602, -0.0484,  ...,  0.0567, -0.0230,  0.0023],\n",
      "        [-0.0595, -0.0417,  0.0493,  ...,  0.0065,  0.0484, -0.0585],\n",
      "        ...,\n",
      "        [-0.0352, -0.0373,  0.0312,  ...,  0.0390, -0.0278,  0.0614],\n",
      "        [ 0.0556,  0.0090,  0.0202,  ..., -0.0431, -0.0560, -0.0315],\n",
      "        [ 0.0026,  0.0469, -0.0173,  ...,  0.0148, -0.0476,  0.0277]])), ('linear2.bias', tensor([ 0.0136,  0.0081, -0.0116, -0.0029,  0.0313,  0.0108,  0.0341,  0.0212,\n",
      "         0.0105, -0.0476, -0.0378,  0.0560,  0.0025,  0.0132,  0.0357, -0.0018,\n",
      "        -0.0355, -0.0344, -0.0257,  0.0028,  0.0090, -0.0471, -0.0198, -0.0289,\n",
      "         0.0051, -0.0557, -0.0136,  0.0269, -0.0292, -0.0207,  0.0372,  0.0088,\n",
      "        -0.0127,  0.0411,  0.0063, -0.0238, -0.0405, -0.0142, -0.0022,  0.0213,\n",
      "         0.0314, -0.0392, -0.0189,  0.0003,  0.0357, -0.0584,  0.0223, -0.0426,\n",
      "        -0.0377, -0.0049, -0.0061,  0.0546, -0.0086, -0.0218, -0.0397, -0.0435,\n",
      "         0.0359, -0.0413, -0.0123, -0.0248, -0.0498, -0.0368,  0.0447, -0.0378,\n",
      "         0.0005,  0.0588,  0.0047,  0.0245,  0.0417,  0.0144,  0.0370,  0.0593,\n",
      "        -0.0293,  0.0033,  0.0364, -0.0143, -0.0439, -0.0120, -0.0217, -0.0485,\n",
      "         0.0284,  0.0413,  0.0237,  0.0592,  0.0569, -0.0462, -0.0358,  0.0131,\n",
      "         0.0430,  0.0185, -0.0053,  0.0260,  0.0407, -0.0548,  0.0053, -0.0197,\n",
      "         0.0464,  0.0148, -0.0230,  0.0460, -0.0395, -0.0185,  0.0158, -0.0328,\n",
      "         0.0517,  0.0025, -0.0156, -0.0140, -0.0208,  0.0033,  0.0187,  0.0321,\n",
      "        -0.0274, -0.0005, -0.0181, -0.0473,  0.0144,  0.0450,  0.0007,  0.0032,\n",
      "        -0.0279, -0.0084, -0.0177, -0.0591, -0.0235, -0.0027, -0.0555,  0.0550,\n",
      "        -0.0299, -0.0509, -0.0160,  0.0568,  0.0051,  0.0228,  0.0115,  0.0042,\n",
      "         0.0519, -0.0075, -0.0288,  0.0447,  0.0378, -0.0242,  0.0426, -0.0228,\n",
      "         0.0458, -0.0516,  0.0303, -0.0322, -0.0551,  0.0027, -0.0255,  0.0572,\n",
      "        -0.0596,  0.0400,  0.0454,  0.0466, -0.0382,  0.0161, -0.0478, -0.0207])), ('linear3.weight', tensor([[-0.0706,  0.0681, -0.0080,  ..., -0.0186,  0.0047,  0.0409],\n",
      "        [ 0.0089,  0.0618, -0.0299,  ...,  0.0540,  0.0040, -0.0399],\n",
      "        [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0518, -0.0753,  ..., -0.0176,  0.0297, -0.0049],\n",
      "        [-0.0109, -0.0724,  0.0292,  ..., -0.0603,  0.0476,  0.0132],\n",
      "        [-0.0658,  0.0038, -0.0390,  ...,  0.0289, -0.0218, -0.0459]])), ('linear3.bias', tensor([ 0.0145,  0.0002, -0.0470, -0.0006,  0.0558, -0.0591,  0.0525,  0.0646,\n",
      "         0.0103,  0.0019]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.2328, -0.1575,  0.1404],\n",
      "          [-0.3092, -0.2124, -0.0475],\n",
      "          [-0.0328,  0.1321, -0.1510]]],\n",
      "\n",
      "\n",
      "        [[[-0.2308, -0.2374,  0.0224],\n",
      "          [-0.1765,  0.0793,  0.1701],\n",
      "          [ 0.3186, -0.2032, -0.2326]]],\n",
      "\n",
      "\n",
      "        [[[-0.2162,  0.2119,  0.2384],\n",
      "          [ 0.2922,  0.2603, -0.1914],\n",
      "          [ 0.0441, -0.1089, -0.0154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1941, -0.2157, -0.1246],\n",
      "          [-0.2838,  0.3148, -0.3312],\n",
      "          [ 0.2337,  0.1579,  0.2093]]]])), ('conv1.bias', tensor([-0.2088,  0.1795,  0.3233, -0.0254])), ('conv2.weight', tensor([[[[ 0.1285,  0.1293, -0.1389],\n",
      "          [-0.0664,  0.0077, -0.1630],\n",
      "          [-0.0279,  0.0455,  0.1343]],\n",
      "\n",
      "         [[ 0.1068, -0.1197,  0.0090],\n",
      "          [-0.0064, -0.0420, -0.0895],\n",
      "          [-0.1413,  0.0500,  0.0897]],\n",
      "\n",
      "         [[-0.0711, -0.1296, -0.1002],\n",
      "          [-0.0239,  0.0477,  0.1364],\n",
      "          [-0.0958, -0.1656, -0.1276]],\n",
      "\n",
      "         [[-0.1230, -0.0910,  0.0032],\n",
      "          [-0.0958, -0.1549, -0.1324],\n",
      "          [ 0.1377, -0.1392,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510,  0.0343,  0.1636],\n",
      "          [ 0.0559,  0.0953, -0.1076],\n",
      "          [ 0.0381,  0.0672, -0.1269]],\n",
      "\n",
      "         [[ 0.0494, -0.1396, -0.0025],\n",
      "          [-0.1630, -0.1530,  0.0134],\n",
      "          [ 0.0688, -0.1538, -0.0249]],\n",
      "\n",
      "         [[-0.1279,  0.0739,  0.0044],\n",
      "          [-0.0093,  0.0977, -0.0246],\n",
      "          [-0.1637, -0.0414, -0.0718]],\n",
      "\n",
      "         [[-0.1459, -0.1286, -0.1408],\n",
      "          [-0.0520,  0.1448,  0.1576],\n",
      "          [ 0.0211,  0.1597,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154,  0.1175, -0.1635],\n",
      "          [-0.0776, -0.0315,  0.1326],\n",
      "          [ 0.0815,  0.0204, -0.0603]],\n",
      "\n",
      "         [[ 0.0975, -0.1333,  0.0701],\n",
      "          [ 0.0633, -0.0995, -0.1419],\n",
      "          [-0.1589, -0.0483,  0.0435]],\n",
      "\n",
      "         [[-0.0155, -0.0939, -0.0870],\n",
      "          [-0.0359, -0.0107, -0.0814],\n",
      "          [ 0.0747, -0.0659, -0.0912]],\n",
      "\n",
      "         [[ 0.0312, -0.0822,  0.0852],\n",
      "          [-0.1027,  0.0658,  0.1524],\n",
      "          [-0.0611,  0.0508, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0886, -0.0288, -0.0476],\n",
      "          [ 0.0636, -0.1219, -0.1228],\n",
      "          [-0.1626,  0.1552, -0.0104]],\n",
      "\n",
      "         [[-0.0444, -0.0287, -0.0863],\n",
      "          [-0.1455,  0.0695,  0.1488],\n",
      "          [ 0.0660,  0.0844,  0.1053]],\n",
      "\n",
      "         [[-0.0399, -0.0200, -0.1058],\n",
      "          [ 0.0172,  0.1119,  0.1368],\n",
      "          [ 0.0402, -0.0858, -0.0749]],\n",
      "\n",
      "         [[-0.0745,  0.1459, -0.1552],\n",
      "          [-0.1600,  0.0574, -0.1118],\n",
      "          [ 0.0488, -0.1259,  0.1628]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872, -0.1264, -0.0872],\n",
      "          [-0.0671,  0.1279, -0.0624],\n",
      "          [-0.0455,  0.0237, -0.1169]],\n",
      "\n",
      "         [[ 0.1078, -0.0462,  0.1611],\n",
      "          [ 0.0378,  0.0042,  0.0240],\n",
      "          [-0.0082, -0.0213, -0.0640]],\n",
      "\n",
      "         [[-0.0785, -0.1561,  0.1334],\n",
      "          [ 0.0412,  0.0912,  0.0069],\n",
      "          [ 0.0332, -0.0704, -0.1196]],\n",
      "\n",
      "         [[-0.0315, -0.1624, -0.1553],\n",
      "          [-0.0109, -0.1157, -0.1260],\n",
      "          [-0.1114,  0.1594,  0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290, -0.0769, -0.0430],\n",
      "          [ 0.0620, -0.0631, -0.0678],\n",
      "          [-0.1418,  0.0943, -0.1289]],\n",
      "\n",
      "         [[-0.1345, -0.0887,  0.0157],\n",
      "          [ 0.0949, -0.0884,  0.1076],\n",
      "          [ 0.0698, -0.0051, -0.1075]],\n",
      "\n",
      "         [[-0.0982, -0.1582,  0.0671],\n",
      "          [ 0.0299, -0.0899, -0.0835],\n",
      "          [-0.0357, -0.1117, -0.0830]],\n",
      "\n",
      "         [[-0.1652, -0.0075,  0.0882],\n",
      "          [ 0.1611, -0.1534,  0.1237],\n",
      "          [ 0.1374, -0.1394, -0.1483]]]])), ('conv2.bias', tensor([ 0.1339,  0.0760, -0.1164,  0.0165, -0.0154,  0.0566])), ('linear1.weight', tensor([[-0.0620, -0.0663,  0.0086,  ..., -0.0562,  0.0230, -0.0066],\n",
      "        [-0.0331,  0.0079, -0.0449,  ...,  0.0446,  0.0275, -0.0118],\n",
      "        [ 0.0647,  0.0101,  0.0594,  ..., -0.0497,  0.0610,  0.0487],\n",
      "        ...,\n",
      "        [-0.0569,  0.0372, -0.0361,  ..., -0.0561, -0.0232,  0.0188],\n",
      "        [-0.0152, -0.0105, -0.0606,  ..., -0.0606,  0.0094, -0.0161],\n",
      "        [ 0.0504, -0.0006, -0.0249,  ..., -0.0138,  0.0210, -0.0435]])), ('linear1.bias', tensor([-6.0873e-02,  6.5560e-02, -4.3044e-02, -6.3863e-03, -4.4803e-02,\n",
      "        -3.3020e-02, -1.3932e-02,  7.4465e-03,  2.3414e-02, -5.0514e-02,\n",
      "        -2.0304e-02,  3.5026e-02, -2.9176e-02, -6.7145e-02,  1.9335e-02,\n",
      "         1.5226e-02, -3.8411e-02,  5.4850e-02, -2.9086e-04,  7.3328e-03,\n",
      "         2.0166e-02, -5.9463e-02,  1.2512e-02,  3.8834e-02, -1.2995e-02,\n",
      "        -2.0696e-03, -5.7886e-02, -6.6434e-02,  2.0881e-02,  6.2669e-02,\n",
      "        -6.7346e-03,  5.1529e-02, -4.3780e-02, -4.4061e-02,  3.4837e-02,\n",
      "         6.1586e-03,  3.7851e-02,  6.2219e-02,  1.2197e-02,  4.9986e-02,\n",
      "         1.9788e-02, -6.0168e-02,  4.9329e-02,  2.1731e-02, -4.9728e-02,\n",
      "         5.4118e-02, -8.4684e-04, -1.0249e-02,  2.7321e-02, -5.3289e-02,\n",
      "        -6.0395e-02,  3.1339e-02, -1.4949e-02,  4.6386e-03,  6.4307e-02,\n",
      "         8.9219e-03, -3.7237e-02,  5.2332e-02,  2.8919e-02, -2.7775e-02,\n",
      "         9.4815e-03,  5.6762e-02,  4.4966e-02,  1.0521e-02, -4.0578e-02,\n",
      "         1.3906e-02, -2.5521e-02, -5.4316e-02,  1.2339e-02,  4.2722e-02,\n",
      "         1.9184e-02, -6.1930e-02,  1.9331e-02, -2.1756e-02,  3.7002e-02,\n",
      "         4.5658e-02,  4.0470e-02,  2.1073e-02,  5.3643e-02, -5.4933e-02,\n",
      "        -1.8724e-02, -4.5842e-02, -5.3424e-02, -2.8775e-02, -1.7706e-02,\n",
      "         6.6402e-02, -1.3323e-02,  5.3333e-02,  4.5054e-02,  4.1214e-02,\n",
      "        -7.3912e-03, -6.4565e-02, -1.3194e-02,  6.6996e-02, -3.9084e-02,\n",
      "         6.4009e-02,  3.1221e-02, -6.5513e-02,  1.0571e-03, -6.5073e-02,\n",
      "        -3.4405e-02,  5.2692e-02,  5.1586e-02, -6.6245e-02,  3.3782e-02,\n",
      "        -2.6436e-02,  2.4386e-02, -3.2056e-02, -1.3929e-02,  2.4294e-02,\n",
      "        -3.9106e-02,  6.4875e-02, -1.0712e-03,  1.7288e-02,  6.6765e-05,\n",
      "        -8.5600e-04,  4.1666e-02,  1.6196e-02,  3.9983e-02,  4.9545e-02,\n",
      "        -2.1091e-02, -3.4429e-02, -4.9302e-02,  1.8530e-02, -6.6794e-02,\n",
      "        -4.1074e-02, -5.8870e-02, -5.8249e-02, -6.1702e-02, -2.1042e-02,\n",
      "         5.7717e-02,  4.6078e-02, -3.1355e-02, -1.0239e-02, -3.8538e-02,\n",
      "         8.5815e-04,  6.6288e-02,  6.7854e-02, -1.2928e-02, -2.1495e-02,\n",
      "         4.4455e-02,  4.5449e-02, -2.8614e-02, -4.0425e-03,  5.6584e-02,\n",
      "        -5.5950e-02, -1.3736e-02, -4.0708e-02, -4.3129e-02,  2.3393e-02,\n",
      "         5.2120e-02,  4.6945e-02,  2.3704e-02, -4.4895e-03,  2.3276e-03,\n",
      "        -4.6276e-02, -4.6073e-02,  2.6573e-02,  2.5198e-02, -5.5994e-02,\n",
      "        -2.7689e-03,  6.2543e-02, -5.4211e-02,  2.7657e-02, -3.5267e-03,\n",
      "         2.1026e-02,  3.9520e-02, -8.1219e-03,  2.0721e-02,  3.3508e-02,\n",
      "        -3.0952e-02, -6.1246e-02,  4.0363e-02,  5.5333e-02, -6.5602e-02,\n",
      "        -5.5229e-02,  1.2202e-02,  3.0445e-02,  1.1241e-02, -5.0941e-02,\n",
      "         6.4340e-02, -4.8585e-02,  2.7508e-02, -2.9939e-02, -6.3172e-03,\n",
      "        -2.8367e-02, -3.0125e-02,  3.2778e-02, -2.9546e-02,  1.1376e-02,\n",
      "         8.5921e-03,  3.2469e-02,  7.9027e-03,  6.4648e-02,  2.7774e-02,\n",
      "        -3.8146e-02, -3.4692e-02,  3.2897e-02, -3.9575e-02, -5.5060e-02,\n",
      "         4.6400e-02,  1.4507e-02,  4.9920e-02, -2.8626e-02,  2.5824e-02,\n",
      "        -1.6718e-02, -1.0379e-02, -3.0170e-02, -8.2306e-04, -4.6724e-02,\n",
      "         5.7998e-02,  5.8384e-03, -3.1310e-02,  1.2629e-02, -5.9381e-02,\n",
      "        -7.4855e-04,  1.6694e-02,  6.4214e-02,  4.7362e-02,  2.8770e-03,\n",
      "        -1.1494e-02, -1.2972e-02, -6.3430e-02,  2.6580e-02, -3.4418e-02,\n",
      "         4.5590e-02, -4.7691e-02, -6.0233e-03,  2.5052e-02,  2.0533e-02,\n",
      "         1.4752e-02,  3.2718e-02, -2.9200e-02, -2.8462e-02, -6.6703e-03,\n",
      "         1.9703e-02,  4.1442e-02,  3.8995e-02, -4.9531e-02, -5.6046e-02,\n",
      "         6.1156e-03, -2.4786e-02,  1.7079e-02,  6.0489e-02,  6.6523e-02,\n",
      "         3.9427e-02, -5.0210e-02,  3.5371e-02, -5.4468e-02,  6.2431e-02,\n",
      "        -5.1263e-02,  4.9748e-03, -2.6898e-02, -5.1688e-02, -5.4482e-02,\n",
      "         8.7758e-03, -1.9238e-02, -1.7703e-02, -3.4707e-02,  6.4090e-02])), ('linear2.weight', tensor([[-0.0210,  0.0475,  0.0102,  ...,  0.0603, -0.0074, -0.0214],\n",
      "        [-0.0399,  0.0602, -0.0484,  ...,  0.0567, -0.0230,  0.0023],\n",
      "        [-0.0595, -0.0417,  0.0493,  ...,  0.0065,  0.0484, -0.0585],\n",
      "        ...,\n",
      "        [-0.0352, -0.0373,  0.0312,  ...,  0.0390, -0.0278,  0.0614],\n",
      "        [ 0.0556,  0.0090,  0.0202,  ..., -0.0431, -0.0560, -0.0315],\n",
      "        [ 0.0026,  0.0469, -0.0173,  ...,  0.0148, -0.0476,  0.0277]])), ('linear2.bias', tensor([ 0.0136,  0.0081, -0.0116, -0.0029,  0.0313,  0.0108,  0.0341,  0.0212,\n",
      "         0.0105, -0.0476, -0.0378,  0.0560,  0.0025,  0.0132,  0.0357, -0.0018,\n",
      "        -0.0355, -0.0344, -0.0257,  0.0028,  0.0090, -0.0471, -0.0198, -0.0289,\n",
      "         0.0051, -0.0557, -0.0136,  0.0269, -0.0292, -0.0207,  0.0372,  0.0088,\n",
      "        -0.0127,  0.0411,  0.0063, -0.0238, -0.0405, -0.0142, -0.0022,  0.0213,\n",
      "         0.0314, -0.0392, -0.0189,  0.0003,  0.0357, -0.0584,  0.0223, -0.0426,\n",
      "        -0.0377, -0.0049, -0.0061,  0.0546, -0.0086, -0.0218, -0.0397, -0.0435,\n",
      "         0.0359, -0.0413, -0.0123, -0.0248, -0.0498, -0.0368,  0.0447, -0.0378,\n",
      "         0.0005,  0.0588,  0.0047,  0.0245,  0.0417,  0.0144,  0.0370,  0.0593,\n",
      "        -0.0293,  0.0033,  0.0364, -0.0143, -0.0439, -0.0120, -0.0217, -0.0485,\n",
      "         0.0284,  0.0413,  0.0237,  0.0592,  0.0569, -0.0462, -0.0358,  0.0131,\n",
      "         0.0430,  0.0185, -0.0053,  0.0260,  0.0407, -0.0548,  0.0053, -0.0197,\n",
      "         0.0464,  0.0148, -0.0230,  0.0460, -0.0395, -0.0185,  0.0158, -0.0328,\n",
      "         0.0517,  0.0025, -0.0156, -0.0140, -0.0208,  0.0033,  0.0187,  0.0321,\n",
      "        -0.0274, -0.0005, -0.0181, -0.0473,  0.0144,  0.0450,  0.0007,  0.0032,\n",
      "        -0.0279, -0.0084, -0.0177, -0.0591, -0.0235, -0.0027, -0.0555,  0.0550,\n",
      "        -0.0299, -0.0509, -0.0160,  0.0568,  0.0051,  0.0228,  0.0115,  0.0042,\n",
      "         0.0519, -0.0075, -0.0288,  0.0447,  0.0378, -0.0242,  0.0426, -0.0228,\n",
      "         0.0458, -0.0516,  0.0303, -0.0322, -0.0551,  0.0027, -0.0255,  0.0572,\n",
      "        -0.0596,  0.0400,  0.0454,  0.0466, -0.0382,  0.0161, -0.0478, -0.0207])), ('linear3.weight', tensor([[-0.0706,  0.0681, -0.0080,  ..., -0.0186,  0.0047,  0.0409],\n",
      "        [ 0.0089,  0.0618, -0.0299,  ...,  0.0540,  0.0040, -0.0399],\n",
      "        [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0518, -0.0753,  ..., -0.0176,  0.0297, -0.0049],\n",
      "        [-0.0109, -0.0724,  0.0292,  ..., -0.0603,  0.0476,  0.0132],\n",
      "        [-0.0658,  0.0038, -0.0390,  ...,  0.0289, -0.0218, -0.0459]])), ('linear3.bias', tensor([ 0.0145,  0.0002, -0.0470, -0.0006,  0.0558, -0.0591,  0.0525,  0.0646,\n",
      "         0.0103,  0.0019]))])\n"
     ]
    }
   ],
   "source": [
    "net_a2_hf = NetA2(10)\n",
    "net_a2_ht = NetA2(10)\n",
    "net_a2_dt = NetA2(10)\n",
    "\n",
    "#set conv1 initialization of net_a2_hf\n",
    "net_a2_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a2_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a2_ht.load_state_dict(net_a2_hf.state_dict())\n",
    "for name, param in net_a2_hf.state_dict().items():\n",
    "    if \"conv1\" not in name:\n",
    "        net_a2_dt.state_dict()[name].copy_(param)\n",
    "\n",
    "#set conv1 initialization\n",
    "#net_a2_dt.conv1.load_state_dict(net_a1_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a2_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_init.pt')\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_init.pt')\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a2_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a2_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a2_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.483715Z",
     "start_time": "2024-06-16T19:12:28.446001Z"
    }
   },
   "id": "214b25adc652c12c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[ 0.0178,  0.0251, -0.0195,  ...,  0.0242,  0.0078,  0.0143],\n",
      "        [ 0.0378, -0.0285, -0.0059,  ...,  0.0324, -0.0183, -0.0011],\n",
      "        [ 0.0119,  0.0369,  0.0041,  ...,  0.0214,  0.0243, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0059, -0.0222,  ...,  0.0063, -0.0187, -0.0121],\n",
      "        [-0.0203, -0.0247,  0.0333,  ...,  0.0254, -0.0090,  0.0344],\n",
      "        [-0.0116, -0.0273, -0.0160,  ..., -0.0193,  0.0077, -0.0220]])), ('linear1.bias', tensor([-0.0202,  0.0237,  0.0262,  0.0373, -0.0302, -0.0359, -0.0348, -0.0348,\n",
      "        -0.0305, -0.0113]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[ 0.0178,  0.0251, -0.0195,  ...,  0.0242,  0.0078,  0.0143],\n",
      "        [ 0.0378, -0.0285, -0.0059,  ...,  0.0324, -0.0183, -0.0011],\n",
      "        [ 0.0119,  0.0369,  0.0041,  ...,  0.0214,  0.0243, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0059, -0.0222,  ...,  0.0063, -0.0187, -0.0121],\n",
      "        [-0.0203, -0.0247,  0.0333,  ...,  0.0254, -0.0090,  0.0344],\n",
      "        [-0.0116, -0.0273, -0.0160,  ..., -0.0193,  0.0077, -0.0220]])), ('linear1.bias', tensor([-0.0202,  0.0237,  0.0262,  0.0373, -0.0302, -0.0359, -0.0348, -0.0348,\n",
      "        -0.0305, -0.0113]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.2328, -0.1575,  0.1404],\n",
      "          [-0.3092, -0.2124, -0.0475],\n",
      "          [-0.0328,  0.1321, -0.1510]]],\n",
      "\n",
      "\n",
      "        [[[-0.2308, -0.2374,  0.0224],\n",
      "          [-0.1765,  0.0793,  0.1701],\n",
      "          [ 0.3186, -0.2032, -0.2326]]],\n",
      "\n",
      "\n",
      "        [[[-0.2162,  0.2119,  0.2384],\n",
      "          [ 0.2922,  0.2603, -0.1914],\n",
      "          [ 0.0441, -0.1089, -0.0154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1941, -0.2157, -0.1246],\n",
      "          [-0.2838,  0.3148, -0.3312],\n",
      "          [ 0.2337,  0.1579,  0.2093]]]])), ('conv1.bias', tensor([-0.2088,  0.1795,  0.3233, -0.0254])), ('linear1.weight', tensor([[ 0.0178,  0.0251, -0.0195,  ...,  0.0242,  0.0078,  0.0143],\n",
      "        [ 0.0378, -0.0285, -0.0059,  ...,  0.0324, -0.0183, -0.0011],\n",
      "        [ 0.0119,  0.0369,  0.0041,  ...,  0.0214,  0.0243, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0059, -0.0222,  ...,  0.0063, -0.0187, -0.0121],\n",
      "        [-0.0203, -0.0247,  0.0333,  ...,  0.0254, -0.0090,  0.0344],\n",
      "        [-0.0116, -0.0273, -0.0160,  ..., -0.0193,  0.0077, -0.0220]])), ('linear1.bias', tensor([-0.0202,  0.0237,  0.0262,  0.0373, -0.0302, -0.0359, -0.0348, -0.0348,\n",
      "        -0.0305, -0.0113]))])\n"
     ]
    }
   ],
   "source": [
    "net_a1_hf = NetA1(10)\n",
    "net_a1_ht = NetA1(10)\n",
    "net_a1_dt = NetA1(10)\n",
    "\n",
    "#set conv1 initialization of net_a1_hf\n",
    "net_a1_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a1_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a1_ht.load_state_dict(net_a1_hf.state_dict())\n",
    "net_a1_dt.load_state_dict(net_a1_hf.state_dict())\n",
    "\n",
    "#set conv1 initialization\n",
    "net_a1_dt.conv1.load_state_dict(net_a2_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a1_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_init.pt')\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a1_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a1_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a1_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.500919Z",
     "start_time": "2024-06-16T19:12:28.485721Z"
    }
   },
   "id": "29ff67df34fd0a6b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Analysys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e527a7ee28f832"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1: \n",
      " \t|W_{conv_a1_hf} - W_{conv_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A2: \n",
      " \t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A1 Vs Net_A2: \n",
      " \t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| = tensor(0.) \n",
      " \t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n"
     ]
    }
   ],
   "source": [
    "print( \"Net_A1: \\n\",\n",
    "       \"\\t|W_{conv_a1_hf} - W_{conv_a1_ht}| =\", torch.norm(net_a1_hf.conv1.weight - net_a1_ht.conv1.weight),\"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_ht}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_ht.linear1.weight), \"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_dt}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_dt.linear1.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| =\", torch.norm(net_a2_hf.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| =\", torch.norm(net_a2_hf.conv2.weight - net_a2_ht.conv2.weight),\"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_ht.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_dt.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_ht.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_dt.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_ht.linear3.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_dt.linear3.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A1 Vs Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| =\", torch.norm(net_a1_hf.conv1.weight - net_a2_hf.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| =\", torch.norm(net_a1_ht.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| =\", torch.norm(net_a1_dt.conv1.weight - net_a2_dt.conv1.weight),\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.514566Z",
     "start_time": "2024-06-16T19:12:28.502925Z"
    }
   },
   "id": "eabba971baa9ea0d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A2HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A1HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A1DT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2DT:\n",
      "\t True\n",
      "\t True\n"
     ]
    }
   ],
   "source": [
    "print(\"Net_A1HF:\")\n",
    "for param in net_a1_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HF:\")\n",
    "for param in net_a2_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1HT:\")\n",
    "for param in net_a1_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HT:\")\n",
    "for param in net_a2_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1DT:\")\n",
    "for param in net_a1_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2DT:\")\n",
    "for param in net_a2_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.523504Z",
     "start_time": "2024-06-16T19:12:28.515571Z"
    }
   },
   "id": "527076a732d29933",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net_a1_ht.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a1_ht.linear1.weight is net_a1_hf.linear1.weight)\n",
    "print(net_a1_hf.linear1.weight is net_a1_dt.linear1.weight)\n",
    "print(net_a2_hf.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a2_ht.conv1.weight is net_a1_ht.conv1.weight)\n",
    "print(net_a2_dt.conv1.weight is net_a1_dt.conv1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_ht.linear1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_dt.linear1.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.531062Z",
     "start_time": "2024-06-16T19:12:28.525511Z"
    }
   },
   "id": "7f3e08ca6623fd25",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9884bc78c276b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.579444Z",
     "start_time": "2024-06-16T19:12:28.533068Z"
    }
   },
   "id": "e8c282a53727943",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "image, label = train_data[sample_idx]\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.588833Z",
     "start_time": "2024-06-16T19:12:28.581454Z"
    }
   },
   "id": "84fa5b26a60379de",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 160\n",
    "\n",
    "train_dataloader= DataLoader(train_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.595126Z",
     "start_time": "2024-06-16T19:12:28.590838Z"
    }
   },
   "id": "be585d7eb8bcae35",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training/Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238fefd45b206a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "      size = len(dataloader.dataset)\n",
    "      num_batches = len(dataloader)\n",
    "      test_loss, correct = 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          pred = model(X)\n",
    "          test_loss += loss_fn(pred, y).item()\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      test_loss /= num_batches\n",
    "      correct /= size\n",
    "      print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "      return 100*correct, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.606468Z",
     "start_time": "2024-06-16T19:12:28.597133Z"
    }
   },
   "id": "1b06878e4ef1f642",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79254d3d93db89e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 3e-6#4-7\n",
    "epochs = 30"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.612948Z",
     "start_time": "2024-06-16T19:12:28.607474Z"
    }
   },
   "id": "b437c40fa560695f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_test(device, train_dataloader, test_dataloader, net, learning_rate, epochs):\n",
    "    net.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    times=[]\n",
    "    \n",
    "    time_s = time.time()\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, net, loss_fn, optimizer)\n",
    "        acc, loss = test_loop(device, test_dataloader, net, loss_fn)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "        times.append(time.time() - time_s)\n",
    "        time_s = time.time()\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": [ i for i in range(epochs)],\n",
    "            \"times\": times,\n",
    "            \"loss\": losses,\n",
    "            \"accuracy\": accuracies\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:12:28.621818Z",
     "start_time": "2024-06-16T19:12:28.613954Z"
    }
   },
   "id": "a166ce4ccf0cb4a4",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1 -> HF Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcdec5977207972"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a1_hf.state_dict())\n",
    "df_net_a1_hf = train_test(device, train_dataloader, test_dataloader, net_a1_hf, learning_rate, epochs)\n",
    "df_net_a1_hf.to_csv('NetA1HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_trained.pt')\n",
    "net_a1_hf.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b5cb92a0191d1e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> HT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae1a1c117cd5c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a1_ht.state_dict())\n",
    "df_net_a1_ht = train_test(device, train_dataloader, test_dataloader, net_a1_ht, learning_rate, epochs)\n",
    "df_net_a1_ht.to_csv('NetA1HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_trained.pt')\n",
    "net_a1_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d9aed7097ab0b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> DT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e7c98ba5a28137"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a1_dt.state_dict())\n",
    "df_net_a1_dt = train_test(device, train_dataloader, test_dataloader, net_a1_dt, learning_rate, epochs)\n",
    "df_net_a1_dt.to_csv('NetA1DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_trained.pt')\n",
    "net_a1_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fc9595e6f55df0b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HF Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aabaf6906661353"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_hf.state_dict())\n",
    "df_net_a2_hf = train_test(device, train_dataloader, test_dataloader, net_a2_hf, learning_rate, epochs)\n",
    "df_net_a2_hf.to_csv('NetA2HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_trained.pt')\n",
    "net_a2_hf.state_dict() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0485488dfe86d3f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c29cd600b2295f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1285,  0.1293, -0.1389],\n",
      "          [-0.0664,  0.0077, -0.1630],\n",
      "          [-0.0279,  0.0455,  0.1343]],\n",
      "\n",
      "         [[ 0.1068, -0.1197,  0.0090],\n",
      "          [-0.0064, -0.0420, -0.0895],\n",
      "          [-0.1413,  0.0500,  0.0897]],\n",
      "\n",
      "         [[-0.0711, -0.1296, -0.1002],\n",
      "          [-0.0239,  0.0477,  0.1364],\n",
      "          [-0.0958, -0.1656, -0.1276]],\n",
      "\n",
      "         [[-0.1230, -0.0910,  0.0032],\n",
      "          [-0.0958, -0.1549, -0.1324],\n",
      "          [ 0.1377, -0.1392,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510,  0.0343,  0.1636],\n",
      "          [ 0.0559,  0.0953, -0.1076],\n",
      "          [ 0.0381,  0.0672, -0.1269]],\n",
      "\n",
      "         [[ 0.0494, -0.1396, -0.0025],\n",
      "          [-0.1630, -0.1530,  0.0134],\n",
      "          [ 0.0688, -0.1538, -0.0249]],\n",
      "\n",
      "         [[-0.1279,  0.0739,  0.0044],\n",
      "          [-0.0093,  0.0977, -0.0246],\n",
      "          [-0.1637, -0.0414, -0.0718]],\n",
      "\n",
      "         [[-0.1459, -0.1286, -0.1408],\n",
      "          [-0.0520,  0.1448,  0.1576],\n",
      "          [ 0.0211,  0.1597,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154,  0.1175, -0.1635],\n",
      "          [-0.0776, -0.0315,  0.1326],\n",
      "          [ 0.0815,  0.0204, -0.0603]],\n",
      "\n",
      "         [[ 0.0975, -0.1333,  0.0701],\n",
      "          [ 0.0633, -0.0995, -0.1419],\n",
      "          [-0.1589, -0.0483,  0.0435]],\n",
      "\n",
      "         [[-0.0155, -0.0939, -0.0870],\n",
      "          [-0.0359, -0.0107, -0.0814],\n",
      "          [ 0.0747, -0.0659, -0.0912]],\n",
      "\n",
      "         [[ 0.0312, -0.0822,  0.0852],\n",
      "          [-0.1027,  0.0658,  0.1524],\n",
      "          [-0.0611,  0.0508, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0886, -0.0288, -0.0476],\n",
      "          [ 0.0636, -0.1219, -0.1228],\n",
      "          [-0.1626,  0.1552, -0.0104]],\n",
      "\n",
      "         [[-0.0444, -0.0287, -0.0863],\n",
      "          [-0.1455,  0.0695,  0.1488],\n",
      "          [ 0.0660,  0.0844,  0.1053]],\n",
      "\n",
      "         [[-0.0399, -0.0200, -0.1058],\n",
      "          [ 0.0172,  0.1119,  0.1368],\n",
      "          [ 0.0402, -0.0858, -0.0749]],\n",
      "\n",
      "         [[-0.0745,  0.1459, -0.1552],\n",
      "          [-0.1600,  0.0574, -0.1118],\n",
      "          [ 0.0488, -0.1259,  0.1628]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872, -0.1264, -0.0872],\n",
      "          [-0.0671,  0.1279, -0.0624],\n",
      "          [-0.0455,  0.0237, -0.1169]],\n",
      "\n",
      "         [[ 0.1078, -0.0462,  0.1611],\n",
      "          [ 0.0378,  0.0042,  0.0240],\n",
      "          [-0.0082, -0.0213, -0.0640]],\n",
      "\n",
      "         [[-0.0785, -0.1561,  0.1334],\n",
      "          [ 0.0412,  0.0912,  0.0069],\n",
      "          [ 0.0332, -0.0704, -0.1196]],\n",
      "\n",
      "         [[-0.0315, -0.1624, -0.1553],\n",
      "          [-0.0109, -0.1157, -0.1260],\n",
      "          [-0.1114,  0.1594,  0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290, -0.0769, -0.0430],\n",
      "          [ 0.0620, -0.0631, -0.0678],\n",
      "          [-0.1418,  0.0943, -0.1289]],\n",
      "\n",
      "         [[-0.1345, -0.0887,  0.0157],\n",
      "          [ 0.0949, -0.0884,  0.1076],\n",
      "          [ 0.0698, -0.0051, -0.1075]],\n",
      "\n",
      "         [[-0.0982, -0.1582,  0.0671],\n",
      "          [ 0.0299, -0.0899, -0.0835],\n",
      "          [-0.0357, -0.1117, -0.0830]],\n",
      "\n",
      "         [[-0.1652, -0.0075,  0.0882],\n",
      "          [ 0.1611, -0.1534,  0.1237],\n",
      "          [ 0.1374, -0.1394, -0.1483]]]])), ('conv2.bias', tensor([ 0.1339,  0.0760, -0.1164,  0.0165, -0.0154,  0.0566])), ('linear1.weight', tensor([[-0.0620, -0.0663,  0.0086,  ..., -0.0562,  0.0230, -0.0066],\n",
      "        [-0.0331,  0.0079, -0.0449,  ...,  0.0446,  0.0275, -0.0118],\n",
      "        [ 0.0647,  0.0101,  0.0594,  ..., -0.0497,  0.0610,  0.0487],\n",
      "        ...,\n",
      "        [-0.0569,  0.0372, -0.0361,  ..., -0.0561, -0.0232,  0.0188],\n",
      "        [-0.0152, -0.0105, -0.0606,  ..., -0.0606,  0.0094, -0.0161],\n",
      "        [ 0.0504, -0.0006, -0.0249,  ..., -0.0138,  0.0210, -0.0435]])), ('linear1.bias', tensor([-6.0873e-02,  6.5560e-02, -4.3044e-02, -6.3863e-03, -4.4803e-02,\n",
      "        -3.3020e-02, -1.3932e-02,  7.4465e-03,  2.3414e-02, -5.0514e-02,\n",
      "        -2.0304e-02,  3.5026e-02, -2.9176e-02, -6.7145e-02,  1.9335e-02,\n",
      "         1.5226e-02, -3.8411e-02,  5.4850e-02, -2.9086e-04,  7.3328e-03,\n",
      "         2.0166e-02, -5.9463e-02,  1.2512e-02,  3.8834e-02, -1.2995e-02,\n",
      "        -2.0696e-03, -5.7886e-02, -6.6434e-02,  2.0881e-02,  6.2669e-02,\n",
      "        -6.7346e-03,  5.1529e-02, -4.3780e-02, -4.4061e-02,  3.4837e-02,\n",
      "         6.1586e-03,  3.7851e-02,  6.2219e-02,  1.2197e-02,  4.9986e-02,\n",
      "         1.9788e-02, -6.0168e-02,  4.9329e-02,  2.1731e-02, -4.9728e-02,\n",
      "         5.4118e-02, -8.4684e-04, -1.0249e-02,  2.7321e-02, -5.3289e-02,\n",
      "        -6.0395e-02,  3.1339e-02, -1.4949e-02,  4.6386e-03,  6.4307e-02,\n",
      "         8.9219e-03, -3.7237e-02,  5.2332e-02,  2.8919e-02, -2.7775e-02,\n",
      "         9.4815e-03,  5.6762e-02,  4.4966e-02,  1.0521e-02, -4.0578e-02,\n",
      "         1.3906e-02, -2.5521e-02, -5.4316e-02,  1.2339e-02,  4.2722e-02,\n",
      "         1.9184e-02, -6.1930e-02,  1.9331e-02, -2.1756e-02,  3.7002e-02,\n",
      "         4.5658e-02,  4.0470e-02,  2.1073e-02,  5.3643e-02, -5.4933e-02,\n",
      "        -1.8724e-02, -4.5842e-02, -5.3424e-02, -2.8775e-02, -1.7706e-02,\n",
      "         6.6402e-02, -1.3323e-02,  5.3333e-02,  4.5054e-02,  4.1214e-02,\n",
      "        -7.3912e-03, -6.4565e-02, -1.3194e-02,  6.6996e-02, -3.9084e-02,\n",
      "         6.4009e-02,  3.1221e-02, -6.5513e-02,  1.0571e-03, -6.5073e-02,\n",
      "        -3.4405e-02,  5.2692e-02,  5.1586e-02, -6.6245e-02,  3.3782e-02,\n",
      "        -2.6436e-02,  2.4386e-02, -3.2056e-02, -1.3929e-02,  2.4294e-02,\n",
      "        -3.9106e-02,  6.4875e-02, -1.0712e-03,  1.7288e-02,  6.6765e-05,\n",
      "        -8.5600e-04,  4.1666e-02,  1.6196e-02,  3.9983e-02,  4.9545e-02,\n",
      "        -2.1091e-02, -3.4429e-02, -4.9302e-02,  1.8530e-02, -6.6794e-02,\n",
      "        -4.1074e-02, -5.8870e-02, -5.8249e-02, -6.1702e-02, -2.1042e-02,\n",
      "         5.7717e-02,  4.6078e-02, -3.1355e-02, -1.0239e-02, -3.8538e-02,\n",
      "         8.5815e-04,  6.6288e-02,  6.7854e-02, -1.2928e-02, -2.1495e-02,\n",
      "         4.4455e-02,  4.5449e-02, -2.8614e-02, -4.0425e-03,  5.6584e-02,\n",
      "        -5.5950e-02, -1.3736e-02, -4.0708e-02, -4.3129e-02,  2.3393e-02,\n",
      "         5.2120e-02,  4.6945e-02,  2.3704e-02, -4.4895e-03,  2.3276e-03,\n",
      "        -4.6276e-02, -4.6073e-02,  2.6573e-02,  2.5198e-02, -5.5994e-02,\n",
      "        -2.7689e-03,  6.2543e-02, -5.4211e-02,  2.7657e-02, -3.5267e-03,\n",
      "         2.1026e-02,  3.9520e-02, -8.1219e-03,  2.0721e-02,  3.3508e-02,\n",
      "        -3.0952e-02, -6.1246e-02,  4.0363e-02,  5.5333e-02, -6.5602e-02,\n",
      "        -5.5229e-02,  1.2202e-02,  3.0445e-02,  1.1241e-02, -5.0941e-02,\n",
      "         6.4340e-02, -4.8585e-02,  2.7508e-02, -2.9939e-02, -6.3172e-03,\n",
      "        -2.8367e-02, -3.0125e-02,  3.2778e-02, -2.9546e-02,  1.1376e-02,\n",
      "         8.5921e-03,  3.2469e-02,  7.9027e-03,  6.4648e-02,  2.7774e-02,\n",
      "        -3.8146e-02, -3.4692e-02,  3.2897e-02, -3.9575e-02, -5.5060e-02,\n",
      "         4.6400e-02,  1.4507e-02,  4.9920e-02, -2.8626e-02,  2.5824e-02,\n",
      "        -1.6718e-02, -1.0379e-02, -3.0170e-02, -8.2306e-04, -4.6724e-02,\n",
      "         5.7998e-02,  5.8384e-03, -3.1310e-02,  1.2629e-02, -5.9381e-02,\n",
      "        -7.4855e-04,  1.6694e-02,  6.4214e-02,  4.7362e-02,  2.8770e-03,\n",
      "        -1.1494e-02, -1.2972e-02, -6.3430e-02,  2.6580e-02, -3.4418e-02,\n",
      "         4.5590e-02, -4.7691e-02, -6.0233e-03,  2.5052e-02,  2.0533e-02,\n",
      "         1.4752e-02,  3.2718e-02, -2.9200e-02, -2.8462e-02, -6.6703e-03,\n",
      "         1.9703e-02,  4.1442e-02,  3.8995e-02, -4.9531e-02, -5.6046e-02,\n",
      "         6.1156e-03, -2.4786e-02,  1.7079e-02,  6.0489e-02,  6.6523e-02,\n",
      "         3.9427e-02, -5.0210e-02,  3.5371e-02, -5.4468e-02,  6.2431e-02,\n",
      "        -5.1263e-02,  4.9748e-03, -2.6898e-02, -5.1688e-02, -5.4482e-02,\n",
      "         8.7758e-03, -1.9238e-02, -1.7703e-02, -3.4707e-02,  6.4090e-02])), ('linear2.weight', tensor([[-0.0210,  0.0475,  0.0102,  ...,  0.0603, -0.0074, -0.0214],\n",
      "        [-0.0399,  0.0602, -0.0484,  ...,  0.0567, -0.0230,  0.0023],\n",
      "        [-0.0595, -0.0417,  0.0493,  ...,  0.0065,  0.0484, -0.0585],\n",
      "        ...,\n",
      "        [-0.0352, -0.0373,  0.0312,  ...,  0.0390, -0.0278,  0.0614],\n",
      "        [ 0.0556,  0.0090,  0.0202,  ..., -0.0431, -0.0560, -0.0315],\n",
      "        [ 0.0026,  0.0469, -0.0173,  ...,  0.0148, -0.0476,  0.0277]])), ('linear2.bias', tensor([ 0.0136,  0.0081, -0.0116, -0.0029,  0.0313,  0.0108,  0.0341,  0.0212,\n",
      "         0.0105, -0.0476, -0.0378,  0.0560,  0.0025,  0.0132,  0.0357, -0.0018,\n",
      "        -0.0355, -0.0344, -0.0257,  0.0028,  0.0090, -0.0471, -0.0198, -0.0289,\n",
      "         0.0051, -0.0557, -0.0136,  0.0269, -0.0292, -0.0207,  0.0372,  0.0088,\n",
      "        -0.0127,  0.0411,  0.0063, -0.0238, -0.0405, -0.0142, -0.0022,  0.0213,\n",
      "         0.0314, -0.0392, -0.0189,  0.0003,  0.0357, -0.0584,  0.0223, -0.0426,\n",
      "        -0.0377, -0.0049, -0.0061,  0.0546, -0.0086, -0.0218, -0.0397, -0.0435,\n",
      "         0.0359, -0.0413, -0.0123, -0.0248, -0.0498, -0.0368,  0.0447, -0.0378,\n",
      "         0.0005,  0.0588,  0.0047,  0.0245,  0.0417,  0.0144,  0.0370,  0.0593,\n",
      "        -0.0293,  0.0033,  0.0364, -0.0143, -0.0439, -0.0120, -0.0217, -0.0485,\n",
      "         0.0284,  0.0413,  0.0237,  0.0592,  0.0569, -0.0462, -0.0358,  0.0131,\n",
      "         0.0430,  0.0185, -0.0053,  0.0260,  0.0407, -0.0548,  0.0053, -0.0197,\n",
      "         0.0464,  0.0148, -0.0230,  0.0460, -0.0395, -0.0185,  0.0158, -0.0328,\n",
      "         0.0517,  0.0025, -0.0156, -0.0140, -0.0208,  0.0033,  0.0187,  0.0321,\n",
      "        -0.0274, -0.0005, -0.0181, -0.0473,  0.0144,  0.0450,  0.0007,  0.0032,\n",
      "        -0.0279, -0.0084, -0.0177, -0.0591, -0.0235, -0.0027, -0.0555,  0.0550,\n",
      "        -0.0299, -0.0509, -0.0160,  0.0568,  0.0051,  0.0228,  0.0115,  0.0042,\n",
      "         0.0519, -0.0075, -0.0288,  0.0447,  0.0378, -0.0242,  0.0426, -0.0228,\n",
      "         0.0458, -0.0516,  0.0303, -0.0322, -0.0551,  0.0027, -0.0255,  0.0572,\n",
      "        -0.0596,  0.0400,  0.0454,  0.0466, -0.0382,  0.0161, -0.0478, -0.0207])), ('linear3.weight', tensor([[-0.0706,  0.0681, -0.0080,  ..., -0.0186,  0.0047,  0.0409],\n",
      "        [ 0.0089,  0.0618, -0.0299,  ...,  0.0540,  0.0040, -0.0399],\n",
      "        [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0518, -0.0753,  ..., -0.0176,  0.0297, -0.0049],\n",
      "        [-0.0109, -0.0724,  0.0292,  ..., -0.0603,  0.0476,  0.0132],\n",
      "        [-0.0658,  0.0038, -0.0390,  ...,  0.0289, -0.0218, -0.0459]])), ('linear3.bias', tensor([ 0.0145,  0.0002, -0.0470, -0.0006,  0.0558, -0.0591,  0.0525,  0.0646,\n",
      "         0.0103,  0.0019]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302585  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.302542 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.302542  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 2.302475 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.302473  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.302370 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.302363  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 2.302204 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.302188  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.301922 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.301886  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 2.301377 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.301305  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 2.300366 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.300235  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 2.298592 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.298351  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 2.295729 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.295326  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 2.291865 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.291339  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 2.287645 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.287122  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 2.283701 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.283175  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 2.280382 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.279830  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 2.277748 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.277192  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 2.275710 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.275178  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 2.274084 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.273571  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 2.272669 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.272181  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 2.271049 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.270467  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 2.269768 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.269049  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 2.268692 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.267809  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 2.267808 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.266752  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.267087 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.265866  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.266498 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.265117  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 2.266006 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.264452  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.6%, Avg loss: 2.265581 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.263856  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 2.265204 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.263314  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 2.264854 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.262803  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 2.264523 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.262316  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 2.264188 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.261831  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 2.263843 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.261373  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 2.263501 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.260952  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 2.263163 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.260577  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 2.262830 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.260237  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 2.262505 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.259939  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 2.262190 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 0.9899,  0.0101,  1.0153],\n                        [-0.0017,  1.0102,  0.0125],\n                        [ 1.0012,  0.0098,  1.0119]]],\n              \n              \n                      [[[ 1.0421,  1.0405,  0.0387],\n                        [ 0.0417,  0.0404,  1.0390],\n                        [ 1.0410,  1.0397,  0.0394]]],\n              \n              \n                      [[[ 0.0202,  1.0297,  1.0338],\n                        [ 1.0238,  0.0348,  0.0375],\n                        [ 0.0243,  1.0374,  1.0400]]],\n              \n              \n                      [[[ 0.0407,  1.0414,  0.0342],\n                        [ 1.0378,  1.0341,  0.0162],\n                        [ 0.0230,  1.0151,  0.0099]]]])),\n             ('conv1.bias', tensor([ 0.0121,  0.0182, -0.0047, -0.0044])),\n             ('conv2.weight',\n              tensor([[[[ 0.1527,  0.1468, -0.1090],\n                        [-0.0415,  0.0346, -0.1262],\n                        [ 0.0060,  0.0800,  0.1740]],\n              \n                       [[ 0.1315, -0.1020,  0.0339],\n                        [ 0.0193, -0.0198, -0.0531],\n                        [-0.1074,  0.0835,  0.1294]],\n              \n                       [[-0.0479, -0.1114, -0.0694],\n                        [ 0.0007,  0.0742,  0.1741],\n                        [-0.0631, -0.1315, -0.0873]],\n              \n                       [[-0.1001, -0.0751,  0.0197],\n                        [-0.0728, -0.1381, -0.0972],\n                        [ 0.1700, -0.1084,  0.1887]]],\n              \n              \n                      [[[ 0.0495,  0.0467,  0.1958],\n                        [ 0.0625,  0.1166, -0.0806],\n                        [ 0.0576,  0.0947, -0.1045]],\n              \n                       [[ 0.0478, -0.1313,  0.0288],\n                        [-0.1569, -0.1339,  0.0398],\n                        [ 0.0880, -0.1276, -0.0029]],\n              \n                       [[-0.1306,  0.0912,  0.0364],\n                        [-0.0050,  0.1214,  0.0012],\n                        [-0.1462, -0.0127, -0.0521]],\n              \n                       [[-0.1487, -0.1257, -0.1106],\n                        [-0.0500,  0.1626,  0.1822],\n                        [ 0.0393,  0.1851,  0.0834]]],\n              \n              \n                      [[[ 0.0299,  0.1281, -0.1657],\n                        [-0.0665, -0.0174,  0.1302],\n                        [ 0.0839,  0.0257, -0.0623]],\n              \n                       [[ 0.1127, -0.1225,  0.0682],\n                        [ 0.0750, -0.0850, -0.1426],\n                        [-0.1557, -0.0420,  0.0432]],\n              \n                       [[-0.0007, -0.0847, -0.0893],\n                        [-0.0224, -0.0010, -0.0840],\n                        [ 0.0791, -0.0617, -0.0937]],\n              \n                       [[ 0.0464, -0.0724,  0.0835],\n                        [-0.0897,  0.0743,  0.1504],\n                        [-0.0572,  0.0548, -0.1151]]],\n              \n              \n                      [[[ 0.1258,  0.0078, -0.0154],\n                        [ 0.1022, -0.0835, -0.0853],\n                        [-0.1242,  0.1935,  0.0275]],\n              \n                       [[-0.0083,  0.0085, -0.0546],\n                        [-0.1073,  0.1083,  0.1858],\n                        [ 0.1041,  0.1230,  0.1429]],\n              \n                       [[-0.0019,  0.0157, -0.0736],\n                        [ 0.0561,  0.1501,  0.1739],\n                        [ 0.0790, -0.0478, -0.0370]],\n              \n                       [[-0.0394,  0.1832, -0.1244],\n                        [-0.1221,  0.0962, -0.0756],\n                        [ 0.0869, -0.0875,  0.1998]]],\n              \n              \n                      [[[ 0.1286, -0.0872, -0.0834],\n                        [-0.0238,  0.1693, -0.0614],\n                        [-0.0031,  0.0657, -0.1144]],\n              \n                       [[ 0.1487, -0.0059,  0.1658],\n                        [ 0.0808,  0.0460,  0.0263],\n                        [ 0.0336,  0.0212, -0.0608]],\n              \n                       [[-0.0369, -0.1193,  0.1325],\n                        [ 0.0844,  0.1315,  0.0017],\n                        [ 0.0758, -0.0295, -0.1328]],\n              \n                       [[ 0.0084, -0.1237, -0.1524],\n                        [ 0.0315, -0.0737, -0.1289],\n                        [-0.0706,  0.2017,  0.0680]]],\n              \n              \n                      [[[-0.0994, -0.0422, -0.0037],\n                        [ 0.0993, -0.0255, -0.0311],\n                        [-0.1021,  0.1323, -0.0934]],\n              \n                       [[-0.1043, -0.0545,  0.0542],\n                        [ 0.1325, -0.0513,  0.1433],\n                        [ 0.1094,  0.0332, -0.0729]],\n              \n                       [[-0.0693, -0.1236,  0.1059],\n                        [ 0.0672, -0.0533, -0.0474],\n                        [ 0.0038, -0.0745, -0.0479]],\n              \n                       [[-0.1363,  0.0233,  0.1248],\n                        [ 0.1987, -0.1195,  0.1555],\n                        [ 0.1767, -0.1027, -0.1200]]]])),\n             ('conv2.bias',\n              tensor([ 0.1724,  0.1143, -0.0991,  0.0488,  0.0232,  0.0951])),\n             ('linear1.weight',\n              tensor([[-0.0724, -0.0675,  0.0068,  ..., -0.0650,  0.0287,  0.0115],\n                      [-0.0543,  0.0455, -0.0342,  ...,  0.0729,  0.0419, -0.0263],\n                      [ 0.0365,  0.0518,  0.0863,  ..., -0.0377,  0.0733,  0.0295],\n                      ...,\n                      [-0.0546,  0.0686, -0.0188,  ..., -0.0492, -0.0183,  0.0379],\n                      [ 0.0054, -0.0173, -0.0538,  ..., -0.0700,  0.0120,  0.0018],\n                      [ 0.0707, -0.0140, -0.0190,  ...,  0.0175,  0.0330, -0.0558]])),\n             ('linear1.bias',\n              tensor([-0.0414,  0.0897, -0.0255,  0.0156, -0.0344, -0.0067, -0.0016,  0.0249,\n                       0.0398, -0.0408,  0.0024,  0.0580, -0.0066, -0.0544,  0.0332,  0.0265,\n                      -0.0153,  0.0716,  0.0264,  0.0254,  0.0468, -0.0414,  0.0287,  0.0570,\n                      -0.0039,  0.0125, -0.0509, -0.0519,  0.0449,  0.0816,  0.0165,  0.0744,\n                      -0.0432, -0.0351,  0.0512,  0.0161,  0.0556,  0.0834,  0.0293,  0.0827,\n                       0.0342, -0.0371,  0.0705,  0.0464, -0.0309,  0.0771,  0.0084,  0.0151,\n                       0.0523, -0.0243, -0.0322,  0.0485,  0.0111,  0.0145,  0.0817,  0.0303,\n                      -0.0146,  0.0713,  0.0434, -0.0047,  0.0153,  0.0726,  0.0684,  0.0204,\n                      -0.0251,  0.0224, -0.0313, -0.0458,  0.0277,  0.0588,  0.0326, -0.0414,\n                       0.0389, -0.0012,  0.0675,  0.0766,  0.0638,  0.0407,  0.0563, -0.0375,\n                       0.0047, -0.0158, -0.0228, -0.0083, -0.0125,  0.0854, -0.0161,  0.0717,\n                       0.0700,  0.0610,  0.0048, -0.0450, -0.0074,  0.0777, -0.0288,  0.0849,\n                       0.0529, -0.0477,  0.0217, -0.0460, -0.0129,  0.0691,  0.0628, -0.0559,\n                       0.0634, -0.0015,  0.0490, -0.0224,  0.0041,  0.0474, -0.0351,  0.0866,\n                       0.0203,  0.0270,  0.0136,  0.0270,  0.0635,  0.0351,  0.0631,  0.0649,\n                      -0.0190, -0.0367, -0.0209,  0.0510, -0.0446, -0.0255, -0.0405, -0.0431,\n                      -0.0446,  0.0086,  0.0776,  0.0625, -0.0086,  0.0109, -0.0188,  0.0147,\n                       0.0785,  0.0787, -0.0050, -0.0069,  0.0688,  0.0624, -0.0259,  0.0176,\n                       0.0801, -0.0462, -0.0013, -0.0435, -0.0284,  0.0421,  0.0688,  0.0544,\n                       0.0198,  0.0246,  0.0197, -0.0309, -0.0281,  0.0445,  0.0496, -0.0327,\n                       0.0129,  0.0738, -0.0271,  0.0438,  0.0185,  0.0375,  0.0584,  0.0093,\n                       0.0349,  0.0629, -0.0193, -0.0338,  0.0661,  0.0725, -0.0403, -0.0411,\n                       0.0318,  0.0506,  0.0387, -0.0275,  0.0803, -0.0355,  0.0477, -0.0097,\n                       0.0201, -0.0082, -0.0083,  0.0341, -0.0129,  0.0248,  0.0320,  0.0518,\n                       0.0261,  0.0790,  0.0377, -0.0204, -0.0098,  0.0203, -0.0244, -0.0367,\n                       0.0703,  0.0326,  0.0636, -0.0053,  0.0482, -0.0139,  0.0176, -0.0110,\n                       0.0244, -0.0281,  0.0852,  0.0166, -0.0036,  0.0279, -0.0470,  0.0192,\n                       0.0422,  0.0657,  0.0751,  0.0057,  0.0005,  0.0169, -0.0420,  0.0414,\n                      -0.0197,  0.0590, -0.0253,  0.0196,  0.0470,  0.0307,  0.0266,  0.0519,\n                      -0.0050, -0.0110, -0.0002,  0.0275,  0.0607,  0.0398, -0.0253, -0.0393,\n                       0.0211, -0.0129,  0.0381,  0.0871,  0.0920,  0.0543, -0.0226,  0.0476,\n                      -0.0310,  0.0878, -0.0290,  0.0185, -0.0047, -0.0293, -0.0346,  0.0271,\n                      -0.0002,  0.0066, -0.0118,  0.0816])),\n             ('linear2.weight',\n              tensor([[-0.0338,  0.0839,  0.0458,  ...,  0.0960, -0.0244, -0.0345],\n                      [-0.0108,  0.0799, -0.0770,  ...,  0.0534,  0.0041,  0.0276],\n                      [-0.0903, -0.0716,  0.0552,  ..., -0.0180,  0.0482, -0.0536],\n                      ...,\n                      [-0.0125, -0.0258,  0.0008,  ...,  0.0043, -0.0241,  0.0915],\n                      [ 0.0525,  0.0062,  0.0191,  ..., -0.0443, -0.0583, -0.0340],\n                      [-0.0274,  0.0480,  0.0158,  ...,  0.0395, -0.0409,  0.0038]])),\n             ('linear2.bias',\n              tensor([ 2.2863e-02,  2.1599e-02,  1.9606e-03,  1.1143e-03,  3.6653e-02,\n                       3.7824e-02,  4.9704e-02,  3.2356e-02,  1.6516e-02, -4.8794e-02,\n                      -5.0682e-02,  7.6993e-02,  5.7843e-03,  2.4780e-02,  4.6779e-02,\n                       4.9790e-03, -2.6469e-02, -3.6322e-02, -3.2618e-02,  2.1908e-02,\n                       1.8096e-02, -1.8980e-02, -1.7624e-02, -2.9044e-02,  1.6447e-02,\n                      -5.3312e-02,  9.1230e-03,  3.9859e-02, -1.0696e-02, -3.9803e-03,\n                       5.3309e-02,  1.3674e-02,  1.6337e-02,  6.2053e-02,  1.7982e-02,\n                      -1.8726e-02, -1.0122e-02, -1.5558e-03, -1.0061e-02,  4.8138e-02,\n                       3.8933e-02, -1.8254e-02,  5.6381e-05,  1.6965e-02,  6.9202e-02,\n                      -5.5354e-02,  5.5063e-02, -2.9150e-02, -8.0938e-03, -1.6672e-02,\n                       6.9815e-03,  6.5793e-02, -2.6653e-02, -2.4376e-02, -1.0242e-02,\n                      -3.3274e-02,  3.3834e-02, -2.5154e-02, -7.6810e-03, -2.7759e-02,\n                      -3.1731e-02, -1.3603e-02,  7.2946e-02, -4.0314e-02, -4.2333e-03,\n                       7.6795e-02,  1.2549e-02,  1.6498e-02,  3.8429e-02,  2.0592e-02,\n                       6.1745e-02,  7.4545e-02, -2.5136e-02,  2.5161e-03,  4.5078e-02,\n                      -3.8308e-03, -4.6191e-02,  1.2652e-02,  5.8733e-04, -5.0202e-02,\n                       4.0058e-02,  5.5638e-02,  5.3794e-02,  7.6904e-02,  5.8341e-02,\n                      -4.1917e-02, -3.7623e-02,  3.2926e-02,  6.5968e-02,  3.6440e-03,\n                      -1.7597e-02,  4.3635e-02,  5.2517e-02, -4.8395e-02,  1.9098e-02,\n                       8.1722e-03,  5.0181e-02,  9.3757e-03, -2.2927e-02,  6.0479e-02,\n                      -2.9733e-02, -1.4804e-02,  2.6515e-02, -3.1260e-02,  6.9742e-02,\n                       1.6443e-02, -1.6608e-02, -1.2985e-02, -7.4267e-03, -1.8555e-02,\n                       3.7349e-02,  5.6162e-02, -2.2727e-02,  1.6490e-03, -2.2519e-02,\n                      -1.7673e-02,  3.8135e-02,  3.5255e-02, -1.8825e-03,  3.3251e-02,\n                      -1.9671e-02,  1.5415e-03, -9.9209e-03, -4.0686e-02,  5.7633e-03,\n                      -1.3430e-03, -5.6024e-02,  8.1951e-02, -2.0918e-02, -5.1163e-02,\n                       5.9364e-03,  7.3846e-02,  4.6179e-03,  3.1070e-02,  2.8042e-02,\n                       1.7900e-02,  6.8789e-02,  1.4155e-02, -6.0650e-03,  7.8197e-02,\n                       3.8127e-02, -2.2908e-02,  2.3964e-02,  8.6798e-04,  4.8855e-02,\n                      -4.6320e-02,  5.6711e-02, -2.5276e-02, -5.5142e-02,  9.4057e-03,\n                      -1.1817e-02,  4.4408e-02, -4.3903e-02,  3.9782e-02,  6.5929e-02,\n                       7.0332e-02, -5.9190e-03,  2.9591e-02, -5.0615e-02, -1.4253e-02])),\n             ('linear3.weight',\n              tensor([[-0.1022,  0.0942,  0.0170,  ..., -0.0368,  0.0034,  0.0487],\n                      [ 0.0076,  0.0867, -0.0626,  ...,  0.0911,  0.0058, -0.0753],\n                      [-0.0159,  0.0303,  0.0743,  ...,  0.0792, -0.0543, -0.0276],\n                      ...,\n                      [ 0.0508, -0.0851, -0.1073,  ..., -0.0192,  0.0255,  0.0174],\n                      [ 0.0180, -0.1038,  0.0387,  ..., -0.0788,  0.0457,  0.0379],\n                      [-0.0924, -0.0144, -0.0515,  ...,  0.0556, -0.0235, -0.0745]])),\n             ('linear3.bias',\n              tensor([ 0.0345,  0.0250, -0.0410,  0.0173,  0.0699, -0.0394,  0.0692,  0.0846,\n                       0.0216,  0.0151]))])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a2_ht.state_dict())\n",
    "df_net_a2_ht = train_test(device, train_dataloader, test_dataloader, net_a2_ht, learning_rate, epochs)\n",
    "df_net_a2_ht.to_csv('NetA2HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_trained.pt')\n",
    "net_a2_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:23:33.618813Z",
     "start_time": "2024-06-16T19:18:08.687738Z"
    }
   },
   "id": "bc170d6165469498",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> DT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ae93a49f80a7f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[-0.2328, -0.1575,  0.1404],\n",
      "          [-0.3092, -0.2124, -0.0475],\n",
      "          [-0.0328,  0.1321, -0.1510]]],\n",
      "\n",
      "\n",
      "        [[[-0.2308, -0.2374,  0.0224],\n",
      "          [-0.1765,  0.0793,  0.1701],\n",
      "          [ 0.3186, -0.2032, -0.2326]]],\n",
      "\n",
      "\n",
      "        [[[-0.2162,  0.2119,  0.2384],\n",
      "          [ 0.2922,  0.2603, -0.1914],\n",
      "          [ 0.0441, -0.1089, -0.0154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1941, -0.2157, -0.1246],\n",
      "          [-0.2838,  0.3148, -0.3312],\n",
      "          [ 0.2337,  0.1579,  0.2093]]]])), ('conv1.bias', tensor([-0.2088,  0.1795,  0.3233, -0.0254])), ('conv2.weight', tensor([[[[ 0.1285,  0.1293, -0.1389],\n",
      "          [-0.0664,  0.0077, -0.1630],\n",
      "          [-0.0279,  0.0455,  0.1343]],\n",
      "\n",
      "         [[ 0.1068, -0.1197,  0.0090],\n",
      "          [-0.0064, -0.0420, -0.0895],\n",
      "          [-0.1413,  0.0500,  0.0897]],\n",
      "\n",
      "         [[-0.0711, -0.1296, -0.1002],\n",
      "          [-0.0239,  0.0477,  0.1364],\n",
      "          [-0.0958, -0.1656, -0.1276]],\n",
      "\n",
      "         [[-0.1230, -0.0910,  0.0032],\n",
      "          [-0.0958, -0.1549, -0.1324],\n",
      "          [ 0.1377, -0.1392,  0.1484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0510,  0.0343,  0.1636],\n",
      "          [ 0.0559,  0.0953, -0.1076],\n",
      "          [ 0.0381,  0.0672, -0.1269]],\n",
      "\n",
      "         [[ 0.0494, -0.1396, -0.0025],\n",
      "          [-0.1630, -0.1530,  0.0134],\n",
      "          [ 0.0688, -0.1538, -0.0249]],\n",
      "\n",
      "         [[-0.1279,  0.0739,  0.0044],\n",
      "          [-0.0093,  0.0977, -0.0246],\n",
      "          [-0.1637, -0.0414, -0.0718]],\n",
      "\n",
      "         [[-0.1459, -0.1286, -0.1408],\n",
      "          [-0.0520,  0.1448,  0.1576],\n",
      "          [ 0.0211,  0.1597,  0.0636]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154,  0.1175, -0.1635],\n",
      "          [-0.0776, -0.0315,  0.1326],\n",
      "          [ 0.0815,  0.0204, -0.0603]],\n",
      "\n",
      "         [[ 0.0975, -0.1333,  0.0701],\n",
      "          [ 0.0633, -0.0995, -0.1419],\n",
      "          [-0.1589, -0.0483,  0.0435]],\n",
      "\n",
      "         [[-0.0155, -0.0939, -0.0870],\n",
      "          [-0.0359, -0.0107, -0.0814],\n",
      "          [ 0.0747, -0.0659, -0.0912]],\n",
      "\n",
      "         [[ 0.0312, -0.0822,  0.0852],\n",
      "          [-0.1027,  0.0658,  0.1524],\n",
      "          [-0.0611,  0.0508, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0886, -0.0288, -0.0476],\n",
      "          [ 0.0636, -0.1219, -0.1228],\n",
      "          [-0.1626,  0.1552, -0.0104]],\n",
      "\n",
      "         [[-0.0444, -0.0287, -0.0863],\n",
      "          [-0.1455,  0.0695,  0.1488],\n",
      "          [ 0.0660,  0.0844,  0.1053]],\n",
      "\n",
      "         [[-0.0399, -0.0200, -0.1058],\n",
      "          [ 0.0172,  0.1119,  0.1368],\n",
      "          [ 0.0402, -0.0858, -0.0749]],\n",
      "\n",
      "         [[-0.0745,  0.1459, -0.1552],\n",
      "          [-0.1600,  0.0574, -0.1118],\n",
      "          [ 0.0488, -0.1259,  0.1628]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0872, -0.1264, -0.0872],\n",
      "          [-0.0671,  0.1279, -0.0624],\n",
      "          [-0.0455,  0.0237, -0.1169]],\n",
      "\n",
      "         [[ 0.1078, -0.0462,  0.1611],\n",
      "          [ 0.0378,  0.0042,  0.0240],\n",
      "          [-0.0082, -0.0213, -0.0640]],\n",
      "\n",
      "         [[-0.0785, -0.1561,  0.1334],\n",
      "          [ 0.0412,  0.0912,  0.0069],\n",
      "          [ 0.0332, -0.0704, -0.1196]],\n",
      "\n",
      "         [[-0.0315, -0.1624, -0.1553],\n",
      "          [-0.0109, -0.1157, -0.1260],\n",
      "          [-0.1114,  0.1594,  0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290, -0.0769, -0.0430],\n",
      "          [ 0.0620, -0.0631, -0.0678],\n",
      "          [-0.1418,  0.0943, -0.1289]],\n",
      "\n",
      "         [[-0.1345, -0.0887,  0.0157],\n",
      "          [ 0.0949, -0.0884,  0.1076],\n",
      "          [ 0.0698, -0.0051, -0.1075]],\n",
      "\n",
      "         [[-0.0982, -0.1582,  0.0671],\n",
      "          [ 0.0299, -0.0899, -0.0835],\n",
      "          [-0.0357, -0.1117, -0.0830]],\n",
      "\n",
      "         [[-0.1652, -0.0075,  0.0882],\n",
      "          [ 0.1611, -0.1534,  0.1237],\n",
      "          [ 0.1374, -0.1394, -0.1483]]]])), ('conv2.bias', tensor([ 0.1339,  0.0760, -0.1164,  0.0165, -0.0154,  0.0566])), ('linear1.weight', tensor([[-0.0620, -0.0663,  0.0086,  ..., -0.0562,  0.0230, -0.0066],\n",
      "        [-0.0331,  0.0079, -0.0449,  ...,  0.0446,  0.0275, -0.0118],\n",
      "        [ 0.0647,  0.0101,  0.0594,  ..., -0.0497,  0.0610,  0.0487],\n",
      "        ...,\n",
      "        [-0.0569,  0.0372, -0.0361,  ..., -0.0561, -0.0232,  0.0188],\n",
      "        [-0.0152, -0.0105, -0.0606,  ..., -0.0606,  0.0094, -0.0161],\n",
      "        [ 0.0504, -0.0006, -0.0249,  ..., -0.0138,  0.0210, -0.0435]])), ('linear1.bias', tensor([-6.0873e-02,  6.5560e-02, -4.3044e-02, -6.3863e-03, -4.4803e-02,\n",
      "        -3.3020e-02, -1.3932e-02,  7.4465e-03,  2.3414e-02, -5.0514e-02,\n",
      "        -2.0304e-02,  3.5026e-02, -2.9176e-02, -6.7145e-02,  1.9335e-02,\n",
      "         1.5226e-02, -3.8411e-02,  5.4850e-02, -2.9086e-04,  7.3328e-03,\n",
      "         2.0166e-02, -5.9463e-02,  1.2512e-02,  3.8834e-02, -1.2995e-02,\n",
      "        -2.0696e-03, -5.7886e-02, -6.6434e-02,  2.0881e-02,  6.2669e-02,\n",
      "        -6.7346e-03,  5.1529e-02, -4.3780e-02, -4.4061e-02,  3.4837e-02,\n",
      "         6.1586e-03,  3.7851e-02,  6.2219e-02,  1.2197e-02,  4.9986e-02,\n",
      "         1.9788e-02, -6.0168e-02,  4.9329e-02,  2.1731e-02, -4.9728e-02,\n",
      "         5.4118e-02, -8.4684e-04, -1.0249e-02,  2.7321e-02, -5.3289e-02,\n",
      "        -6.0395e-02,  3.1339e-02, -1.4949e-02,  4.6386e-03,  6.4307e-02,\n",
      "         8.9219e-03, -3.7237e-02,  5.2332e-02,  2.8919e-02, -2.7775e-02,\n",
      "         9.4815e-03,  5.6762e-02,  4.4966e-02,  1.0521e-02, -4.0578e-02,\n",
      "         1.3906e-02, -2.5521e-02, -5.4316e-02,  1.2339e-02,  4.2722e-02,\n",
      "         1.9184e-02, -6.1930e-02,  1.9331e-02, -2.1756e-02,  3.7002e-02,\n",
      "         4.5658e-02,  4.0470e-02,  2.1073e-02,  5.3643e-02, -5.4933e-02,\n",
      "        -1.8724e-02, -4.5842e-02, -5.3424e-02, -2.8775e-02, -1.7706e-02,\n",
      "         6.6402e-02, -1.3323e-02,  5.3333e-02,  4.5054e-02,  4.1214e-02,\n",
      "        -7.3912e-03, -6.4565e-02, -1.3194e-02,  6.6996e-02, -3.9084e-02,\n",
      "         6.4009e-02,  3.1221e-02, -6.5513e-02,  1.0571e-03, -6.5073e-02,\n",
      "        -3.4405e-02,  5.2692e-02,  5.1586e-02, -6.6245e-02,  3.3782e-02,\n",
      "        -2.6436e-02,  2.4386e-02, -3.2056e-02, -1.3929e-02,  2.4294e-02,\n",
      "        -3.9106e-02,  6.4875e-02, -1.0712e-03,  1.7288e-02,  6.6765e-05,\n",
      "        -8.5600e-04,  4.1666e-02,  1.6196e-02,  3.9983e-02,  4.9545e-02,\n",
      "        -2.1091e-02, -3.4429e-02, -4.9302e-02,  1.8530e-02, -6.6794e-02,\n",
      "        -4.1074e-02, -5.8870e-02, -5.8249e-02, -6.1702e-02, -2.1042e-02,\n",
      "         5.7717e-02,  4.6078e-02, -3.1355e-02, -1.0239e-02, -3.8538e-02,\n",
      "         8.5815e-04,  6.6288e-02,  6.7854e-02, -1.2928e-02, -2.1495e-02,\n",
      "         4.4455e-02,  4.5449e-02, -2.8614e-02, -4.0425e-03,  5.6584e-02,\n",
      "        -5.5950e-02, -1.3736e-02, -4.0708e-02, -4.3129e-02,  2.3393e-02,\n",
      "         5.2120e-02,  4.6945e-02,  2.3704e-02, -4.4895e-03,  2.3276e-03,\n",
      "        -4.6276e-02, -4.6073e-02,  2.6573e-02,  2.5198e-02, -5.5994e-02,\n",
      "        -2.7689e-03,  6.2543e-02, -5.4211e-02,  2.7657e-02, -3.5267e-03,\n",
      "         2.1026e-02,  3.9520e-02, -8.1219e-03,  2.0721e-02,  3.3508e-02,\n",
      "        -3.0952e-02, -6.1246e-02,  4.0363e-02,  5.5333e-02, -6.5602e-02,\n",
      "        -5.5229e-02,  1.2202e-02,  3.0445e-02,  1.1241e-02, -5.0941e-02,\n",
      "         6.4340e-02, -4.8585e-02,  2.7508e-02, -2.9939e-02, -6.3172e-03,\n",
      "        -2.8367e-02, -3.0125e-02,  3.2778e-02, -2.9546e-02,  1.1376e-02,\n",
      "         8.5921e-03,  3.2469e-02,  7.9027e-03,  6.4648e-02,  2.7774e-02,\n",
      "        -3.8146e-02, -3.4692e-02,  3.2897e-02, -3.9575e-02, -5.5060e-02,\n",
      "         4.6400e-02,  1.4507e-02,  4.9920e-02, -2.8626e-02,  2.5824e-02,\n",
      "        -1.6718e-02, -1.0379e-02, -3.0170e-02, -8.2306e-04, -4.6724e-02,\n",
      "         5.7998e-02,  5.8384e-03, -3.1310e-02,  1.2629e-02, -5.9381e-02,\n",
      "        -7.4855e-04,  1.6694e-02,  6.4214e-02,  4.7362e-02,  2.8770e-03,\n",
      "        -1.1494e-02, -1.2972e-02, -6.3430e-02,  2.6580e-02, -3.4418e-02,\n",
      "         4.5590e-02, -4.7691e-02, -6.0233e-03,  2.5052e-02,  2.0533e-02,\n",
      "         1.4752e-02,  3.2718e-02, -2.9200e-02, -2.8462e-02, -6.6703e-03,\n",
      "         1.9703e-02,  4.1442e-02,  3.8995e-02, -4.9531e-02, -5.6046e-02,\n",
      "         6.1156e-03, -2.4786e-02,  1.7079e-02,  6.0489e-02,  6.6523e-02,\n",
      "         3.9427e-02, -5.0210e-02,  3.5371e-02, -5.4468e-02,  6.2431e-02,\n",
      "        -5.1263e-02,  4.9748e-03, -2.6898e-02, -5.1688e-02, -5.4482e-02,\n",
      "         8.7758e-03, -1.9238e-02, -1.7703e-02, -3.4707e-02,  6.4090e-02])), ('linear2.weight', tensor([[-0.0210,  0.0475,  0.0102,  ...,  0.0603, -0.0074, -0.0214],\n",
      "        [-0.0399,  0.0602, -0.0484,  ...,  0.0567, -0.0230,  0.0023],\n",
      "        [-0.0595, -0.0417,  0.0493,  ...,  0.0065,  0.0484, -0.0585],\n",
      "        ...,\n",
      "        [-0.0352, -0.0373,  0.0312,  ...,  0.0390, -0.0278,  0.0614],\n",
      "        [ 0.0556,  0.0090,  0.0202,  ..., -0.0431, -0.0560, -0.0315],\n",
      "        [ 0.0026,  0.0469, -0.0173,  ...,  0.0148, -0.0476,  0.0277]])), ('linear2.bias', tensor([ 0.0136,  0.0081, -0.0116, -0.0029,  0.0313,  0.0108,  0.0341,  0.0212,\n",
      "         0.0105, -0.0476, -0.0378,  0.0560,  0.0025,  0.0132,  0.0357, -0.0018,\n",
      "        -0.0355, -0.0344, -0.0257,  0.0028,  0.0090, -0.0471, -0.0198, -0.0289,\n",
      "         0.0051, -0.0557, -0.0136,  0.0269, -0.0292, -0.0207,  0.0372,  0.0088,\n",
      "        -0.0127,  0.0411,  0.0063, -0.0238, -0.0405, -0.0142, -0.0022,  0.0213,\n",
      "         0.0314, -0.0392, -0.0189,  0.0003,  0.0357, -0.0584,  0.0223, -0.0426,\n",
      "        -0.0377, -0.0049, -0.0061,  0.0546, -0.0086, -0.0218, -0.0397, -0.0435,\n",
      "         0.0359, -0.0413, -0.0123, -0.0248, -0.0498, -0.0368,  0.0447, -0.0378,\n",
      "         0.0005,  0.0588,  0.0047,  0.0245,  0.0417,  0.0144,  0.0370,  0.0593,\n",
      "        -0.0293,  0.0033,  0.0364, -0.0143, -0.0439, -0.0120, -0.0217, -0.0485,\n",
      "         0.0284,  0.0413,  0.0237,  0.0592,  0.0569, -0.0462, -0.0358,  0.0131,\n",
      "         0.0430,  0.0185, -0.0053,  0.0260,  0.0407, -0.0548,  0.0053, -0.0197,\n",
      "         0.0464,  0.0148, -0.0230,  0.0460, -0.0395, -0.0185,  0.0158, -0.0328,\n",
      "         0.0517,  0.0025, -0.0156, -0.0140, -0.0208,  0.0033,  0.0187,  0.0321,\n",
      "        -0.0274, -0.0005, -0.0181, -0.0473,  0.0144,  0.0450,  0.0007,  0.0032,\n",
      "        -0.0279, -0.0084, -0.0177, -0.0591, -0.0235, -0.0027, -0.0555,  0.0550,\n",
      "        -0.0299, -0.0509, -0.0160,  0.0568,  0.0051,  0.0228,  0.0115,  0.0042,\n",
      "         0.0519, -0.0075, -0.0288,  0.0447,  0.0378, -0.0242,  0.0426, -0.0228,\n",
      "         0.0458, -0.0516,  0.0303, -0.0322, -0.0551,  0.0027, -0.0255,  0.0572,\n",
      "        -0.0596,  0.0400,  0.0454,  0.0466, -0.0382,  0.0161, -0.0478, -0.0207])), ('linear3.weight', tensor([[-0.0706,  0.0681, -0.0080,  ..., -0.0186,  0.0047,  0.0409],\n",
      "        [ 0.0089,  0.0618, -0.0299,  ...,  0.0540,  0.0040, -0.0399],\n",
      "        [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0518, -0.0753,  ..., -0.0176,  0.0297, -0.0049],\n",
      "        [-0.0109, -0.0724,  0.0292,  ..., -0.0603,  0.0476,  0.0132],\n",
      "        [-0.0658,  0.0038, -0.0390,  ...,  0.0289, -0.0218, -0.0459]])), ('linear3.bias', tensor([ 0.0145,  0.0002, -0.0470, -0.0006,  0.0558, -0.0591,  0.0525,  0.0646,\n",
      "         0.0103,  0.0019]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302583  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.302579 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.302578  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 2.302572 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.302572  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 2.302562 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.302562  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 2.302545 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.302544  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 2.302516 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.302513  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 2.302474 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.302469  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 2.302416 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.302407  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 2.302339 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.302324  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 2.302238 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.302217  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 2.302108 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.302079  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 2.301941 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.301902  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 2.301732 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.301681  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.301464 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.301398  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.301116 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.301031  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.300664 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.300557  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.300095 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.299960  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 2.299389 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.299222  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 2.298529 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.298332  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 2.297511 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.297287  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 2.296346 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.296102  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.295061 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.294802  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 2.293687 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.293415  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.292261 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.291972  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.290830 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.290508  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 2.289415 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.289042  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.288029 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.287591  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.286679 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.286165  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 2.285375 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.284788  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 2.284129 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.283484  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 2.282953 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.282273  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 2.281851 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.281157  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.280825 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.280131  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 2.279876 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.279184  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 2.279004 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.278307  [  160/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.278208 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-0.2328, -0.1575,  0.1404],\n                        [-0.3092, -0.2124, -0.0475],\n                        [-0.0328,  0.1321, -0.1510]]],\n              \n              \n                      [[[-0.1892, -0.2603,  0.0035],\n                        [-0.1642,  0.0485,  0.1463],\n                        [ 0.3193, -0.2339, -0.2553]]],\n              \n              \n                      [[[-0.1930,  0.1941,  0.2052],\n                        [ 0.2881,  0.2280, -0.2327],\n                        [ 0.0260, -0.1460, -0.0579]]],\n              \n              \n                      [[[ 0.2465, -0.1635, -0.0718],\n                        [-0.2308,  0.3675, -0.2780],\n                        [ 0.2867,  0.2115,  0.2629]]]])),\n             ('conv1.bias',\n              tensor([-2.0875e-01,  2.3210e-01,  3.4513e-01, -1.1122e-07])),\n             ('conv2.weight',\n              tensor([[[[ 0.1285,  0.1293, -0.1389],\n                        [-0.0664,  0.0077, -0.1630],\n                        [-0.0279,  0.0455,  0.1343]],\n              \n                       [[ 0.1357, -0.0934,  0.0344],\n                        [ 0.0233, -0.0153, -0.0677],\n                        [-0.1228,  0.0616,  0.0932]],\n              \n                       [[-0.0399, -0.0994, -0.0681],\n                        [ 0.0071,  0.0797,  0.1694],\n                        [-0.0651, -0.1375, -0.0979]],\n              \n                       [[-0.0945, -0.0603,  0.0412],\n                        [-0.0589, -0.1230, -0.1018],\n                        [ 0.1751, -0.1041,  0.1856]]],\n              \n              \n                      [[[ 0.0510,  0.0343,  0.1636],\n                        [ 0.0559,  0.0953, -0.1076],\n                        [ 0.0381,  0.0672, -0.1269]],\n              \n                       [[ 0.0383, -0.1533, -0.0155],\n                        [-0.1773, -0.1694, -0.0025],\n                        [ 0.0522, -0.1727, -0.0429]],\n              \n                       [[-0.0775,  0.1243,  0.0549],\n                        [ 0.0417,  0.1486,  0.0267],\n                        [-0.1122,  0.0098, -0.0204]],\n              \n                       [[-0.1002, -0.0738, -0.0865],\n                        [-0.0025,  0.1996,  0.2118],\n                        [ 0.0748,  0.2146,  0.1183]]],\n              \n              \n                      [[[ 0.0154,  0.1175, -0.1635],\n                        [-0.0776, -0.0315,  0.1326],\n                        [ 0.0815,  0.0204, -0.0603]],\n              \n                       [[ 0.0975, -0.1333,  0.0701],\n                        [ 0.0633, -0.0995, -0.1419],\n                        [-0.1589, -0.0483,  0.0435]],\n              \n                       [[-0.0155, -0.0939, -0.0870],\n                        [-0.0359, -0.0107, -0.0814],\n                        [ 0.0747, -0.0659, -0.0912]],\n              \n                       [[ 0.0312, -0.0822,  0.0852],\n                        [-0.1027,  0.0658,  0.1524],\n                        [-0.0611,  0.0508, -0.1142]]],\n              \n              \n                      [[[ 0.0886, -0.0288, -0.0476],\n                        [ 0.0636, -0.1219, -0.1228],\n                        [-0.1626,  0.1552, -0.0104]],\n              \n                       [[ 0.0058,  0.0207, -0.0368],\n                        [-0.0953,  0.1192,  0.1983],\n                        [ 0.1159,  0.1339,  0.1547]],\n              \n                       [[ 0.0084,  0.0289, -0.0578],\n                        [ 0.0653,  0.1596,  0.1663],\n                        [ 0.0882, -0.0395, -0.0504]],\n              \n                       [[-0.0899,  0.1067, -0.2048],\n                        [-0.1943,  0.0126, -0.1632],\n                        [ 0.0200, -0.1636,  0.1139]]],\n              \n              \n                      [[[ 0.0872, -0.1264, -0.0872],\n                        [-0.0671,  0.1279, -0.0624],\n                        [-0.0455,  0.0237, -0.1169]],\n              \n                       [[ 0.1601,  0.0060,  0.2135],\n                        [ 0.0896,  0.0561,  0.0763],\n                        [ 0.0430,  0.0300, -0.0120]],\n              \n                       [[-0.0273, -0.1051,  0.1840],\n                        [ 0.0923,  0.1417,  0.0569],\n                        [ 0.0841, -0.0197, -0.0689]],\n              \n                       [[-0.0561, -0.1972, -0.2008],\n                        [-0.0201, -0.1395, -0.1473],\n                        [-0.0627,  0.2058,  0.1229]]],\n              \n              \n                      [[[-0.1290, -0.0769, -0.0430],\n                        [ 0.0620, -0.0631, -0.0678],\n                        [-0.1418,  0.0943, -0.1289]],\n              \n                       [[-0.1351, -0.0888,  0.0164],\n                        [ 0.0940, -0.0872,  0.1078],\n                        [ 0.0699, -0.0047, -0.1067]],\n              \n                       [[-0.0981, -0.1582,  0.0667],\n                        [ 0.0296, -0.0905, -0.0838],\n                        [-0.0352, -0.1126, -0.0832]],\n              \n                       [[-0.1639, -0.0086,  0.0877],\n                        [ 0.1611, -0.1539,  0.1228],\n                        [ 0.1375, -0.1409, -0.1505]]]])),\n             ('conv2.bias',\n              tensor([ 0.1641,  0.1251, -0.1164,  0.0653,  0.0359,  0.0562])),\n             ('linear1.weight',\n              tensor([[-0.0682, -0.0589, -0.0193,  ..., -0.0563,  0.0230, -0.0066],\n                      [-0.0043,  0.0330, -0.0172,  ...,  0.0450,  0.0275, -0.0118],\n                      [ 0.0425,  0.0125,  0.0362,  ..., -0.0501,  0.0610,  0.0487],\n                      ...,\n                      [-0.0460,  0.0267, -0.0495,  ..., -0.0557, -0.0232,  0.0188],\n                      [-0.0410, -0.0245, -0.0914,  ..., -0.0610,  0.0094, -0.0161],\n                      [ 0.0793,  0.0102, -0.0014,  ..., -0.0133,  0.0210, -0.0435]])),\n             ('linear1.bias',\n              tensor([-2.8141e-02,  1.0779e-01, -2.2878e-02,  2.6383e-02, -4.5906e-02,\n                      -3.3767e-02,  2.6141e-02,  4.7857e-02,  6.4505e-02, -3.3085e-02,\n                       3.9504e-03,  7.5413e-02,  1.1124e-02, -6.7145e-02,  5.7564e-02,\n                       5.6125e-02, -5.0841e-03,  8.9196e-02,  3.3509e-02,  4.5435e-02,\n                       6.1025e-02, -5.9463e-02,  4.4755e-02,  7.7884e-02,  2.7466e-02,\n                       3.3016e-02, -3.5316e-02, -4.3031e-02,  4.5660e-02,  9.2635e-02,\n                       2.7366e-02,  8.7084e-02, -4.3211e-02, -4.5873e-02,  7.3352e-02,\n                       3.9879e-02,  8.0986e-02,  7.2014e-02,  5.3832e-02,  9.2858e-02,\n                       5.1246e-02, -5.9343e-02,  8.6216e-02,  3.9325e-02, -4.9558e-02,\n                       9.1718e-02,  3.3145e-02,  2.8630e-02,  6.2941e-02, -2.8283e-02,\n                      -6.6100e-02,  6.7857e-02, -2.5895e-03,  1.9228e-02,  9.8062e-02,\n                       4.1171e-02, -1.1784e-02,  9.0802e-02,  5.2782e-02,  9.6628e-03,\n                       4.6872e-02,  8.8633e-02,  7.5168e-02,  2.1492e-02, -6.2919e-03,\n                       4.3992e-02, -2.6091e-02, -4.4209e-02,  4.0220e-02,  7.1505e-02,\n                       5.3910e-02, -6.1930e-02,  5.2996e-02,  1.2338e-02,  7.1732e-02,\n                       7.9156e-02,  7.1282e-02,  3.8475e-02,  7.9202e-02, -5.4933e-02,\n                       2.1925e-02, -2.1367e-02, -2.2213e-02, -1.2841e-03,  1.7004e-02,\n                       9.6862e-02,  1.1953e-02,  9.0129e-02,  7.0834e-02,  7.9335e-02,\n                       2.1665e-02, -6.4939e-02,  8.6976e-03,  9.0530e-02, -1.8568e-03,\n                       1.0116e-01,  6.3904e-02, -4.9606e-02,  9.4779e-03, -4.9816e-02,\n                      -3.5341e-02,  8.9686e-02,  8.4852e-02, -5.9172e-02,  7.4073e-02,\n                       1.1327e-02,  6.4943e-02,  4.3440e-03, -1.9170e-02,  5.3419e-02,\n                      -9.9126e-03,  1.0601e-01,  3.9538e-02,  4.3683e-02,  4.1565e-02,\n                       3.7695e-02,  7.7326e-02,  5.7255e-02,  6.9776e-02,  9.1926e-02,\n                       1.8652e-02,  2.2678e-05, -4.9746e-02,  5.9719e-02, -4.3652e-02,\n                      -1.4958e-02, -4.5760e-02, -4.4171e-02, -3.3480e-02,  2.0945e-03,\n                       9.8903e-02,  8.0624e-02, -3.1288e-02,  2.5433e-02, -3.8538e-02,\n                       3.5666e-02,  1.0187e-01,  7.8941e-02,  1.8828e-02,  1.2522e-02,\n                       6.9430e-02,  7.4776e-02,  2.0831e-03,  2.5777e-02,  9.4154e-02,\n                      -2.4068e-02,  2.4436e-03, -4.1058e-02, -7.2145e-03,  5.2452e-02,\n                       8.0414e-02,  7.3913e-02,  6.1222e-02, -2.5355e-03,  3.1296e-02,\n                      -2.2538e-02, -1.1920e-02,  4.0638e-02,  5.4279e-02, -5.5693e-02,\n                       3.2406e-02,  8.8622e-02, -3.1302e-02,  6.3007e-02,  3.5473e-02,\n                       5.8268e-02,  8.1827e-02,  2.4658e-02,  3.5842e-02,  7.0307e-02,\n                       2.2969e-03, -5.3028e-02,  7.6708e-02,  9.6837e-02, -6.5602e-02,\n                      -2.8104e-02,  3.4593e-02,  6.8241e-02,  5.1336e-02, -3.5774e-02,\n                       9.6595e-02, -4.7459e-02,  6.0400e-02, -2.3680e-02,  2.8314e-02,\n                      -4.2387e-03, -1.9146e-02,  7.0575e-02, -2.8857e-02,  2.1491e-02,\n                       4.7621e-02,  7.1006e-02,  2.5884e-02,  1.0106e-01,  6.4186e-02,\n                      -1.0377e-02, -4.2990e-02,  6.8823e-02, -1.7632e-02, -5.3386e-02,\n                       8.8019e-02,  4.8664e-02,  8.6766e-02, -2.0383e-02,  6.3919e-02,\n                      -7.4826e-03,  1.8877e-02,  2.8610e-03,  3.4885e-02, -4.7258e-02,\n                       9.9914e-02,  3.8329e-02, -1.5442e-03,  3.4360e-02, -2.7838e-02,\n                       3.1591e-02,  5.5462e-02,  9.5715e-02,  8.4625e-02,  4.1027e-02,\n                       1.5293e-02,  2.5890e-02, -6.5935e-02,  4.7649e-02,  2.9851e-04,\n                       8.3997e-02, -4.9453e-02,  2.7699e-02,  6.3777e-02,  5.6062e-02,\n                       5.4031e-02,  6.2687e-02, -6.1981e-03, -3.2400e-02,  2.8315e-02,\n                       4.7025e-02,  7.1184e-02,  7.9409e-02, -4.9531e-02, -3.2549e-02,\n                       3.6288e-02,  1.3101e-02,  5.9630e-02,  6.6244e-02,  1.0356e-01,\n                       8.0256e-02, -1.5891e-02,  7.0525e-02, -4.7949e-02,  9.5990e-02,\n                      -2.0640e-02,  3.9902e-02, -3.1141e-02, -1.9531e-02, -3.6110e-02,\n                       4.9938e-02,  1.8128e-02, -7.4718e-03, -1.1541e-02,  8.7029e-02])),\n             ('linear2.weight',\n              tensor([[ 0.0052,  0.0184,  0.0343,  ...,  0.0330,  0.0393, -0.0523],\n                      [-0.0398,  0.0599, -0.0484,  ...,  0.0567, -0.0230,  0.0021],\n                      [-0.0758, -0.0154,  0.0229,  ...,  0.0009,  0.0224, -0.0307],\n                      ...,\n                      [-0.0002, -0.0684,  0.0597,  ...,  0.0211,  0.0194,  0.0174],\n                      [ 0.0744, -0.0164,  0.0316,  ..., -0.0445, -0.0247, -0.0537],\n                      [ 0.0049,  0.0865, -0.0451,  ...,  0.0366, -0.0826,  0.0692]])),\n             ('linear2.bias',\n              tensor([ 1.5239e-02,  7.5833e-03,  3.4533e-03,  3.3919e-02,  6.2899e-02,\n                       4.5751e-02,  7.0077e-02,  4.8250e-02,  3.9575e-02, -4.7606e-02,\n                      -2.9077e-02,  8.1586e-02, -3.3725e-04,  4.1363e-02,  5.7282e-02,\n                      -1.7862e-03, -3.6641e-02, -3.5727e-02,  1.8677e-03, -5.9720e-05,\n                       2.8284e-02, -4.8743e-02, -5.5101e-03, -6.5673e-03,  1.5765e-02,\n                      -2.8741e-02,  7.3581e-03,  5.7251e-02, -6.5345e-03, -8.9799e-03,\n                       4.5632e-02,  2.5618e-02,  2.2054e-02,  6.0571e-02,  3.2030e-02,\n                      -2.3447e-02, -4.3855e-02, -1.7114e-02,  3.4281e-02,  5.3591e-02,\n                       3.7894e-02, -2.9225e-02, -1.8921e-02,  3.4680e-02,  5.9620e-02,\n                      -6.0468e-02,  5.4574e-02, -1.6275e-02, -2.5906e-02,  2.1027e-02,\n                       2.9609e-03,  8.3911e-02,  2.3480e-02,  2.3126e-04, -1.3803e-02,\n                      -4.5205e-02,  5.5731e-02, -4.2155e-02,  8.7608e-03, -5.7509e-03,\n                      -5.2056e-02, -1.4101e-02,  8.2498e-02, -9.4665e-03,  2.0277e-02,\n                       9.8062e-02,  3.9726e-02,  6.1881e-02,  4.9897e-02,  5.3008e-02,\n                       5.0302e-02,  6.9232e-02, -2.2782e-02,  1.5072e-02,  6.5483e-02,\n                       1.4381e-02, -4.5494e-02,  4.3797e-03,  7.7188e-03, -4.9000e-02,\n                       6.3549e-02,  5.8792e-02,  5.3524e-02,  8.8977e-02,  9.1453e-02,\n                      -4.4741e-02, -1.2107e-02,  4.7142e-02,  5.1946e-02,  5.8427e-02,\n                       1.1577e-02,  5.8373e-02,  7.0405e-02, -3.6489e-02,  2.2196e-02,\n                      -8.5175e-03,  8.8794e-02,  5.1925e-02, -2.2960e-02,  8.3340e-02,\n                      -3.9544e-02,  9.9469e-04,  5.7531e-02, -7.2469e-03,  9.1121e-02,\n                       3.9486e-02, -1.5626e-02, -7.3357e-03, -2.1555e-02,  3.5321e-02,\n                       5.3757e-02,  3.5755e-02, -3.1172e-03,  3.7597e-02, -1.5498e-02,\n                      -2.8943e-02,  4.1873e-02,  6.3353e-02,  9.8809e-03,  1.7070e-04,\n                      -1.4591e-02,  9.1981e-03,  1.7290e-03, -5.9167e-02,  1.3530e-02,\n                      -2.3773e-03, -5.5463e-02,  9.1348e-02, -3.3745e-02, -5.0851e-02,\n                      -1.9542e-02,  7.8408e-02,  1.9149e-03,  5.0933e-02,  4.9760e-02,\n                       2.8409e-02,  9.2485e-02,  1.7388e-02, -3.8545e-03,  8.3325e-02,\n                       3.1855e-02, -7.6425e-03,  5.6229e-02, -6.9223e-03,  7.1119e-02,\n                      -5.1605e-02,  7.0459e-02, -2.2576e-04, -6.0505e-02,  1.1414e-02,\n                      -2.7916e-02,  7.8046e-02, -5.3815e-02,  6.3994e-02,  7.2612e-02,\n                       8.1113e-02, -3.8196e-02,  2.0434e-02, -3.4928e-02,  1.0242e-02])),\n             ('linear3.weight',\n              tensor([[-0.1187,  0.0680,  0.0227,  ..., -0.0449, -0.0050,  0.0773],\n                      [ 0.0543,  0.0617, -0.0708,  ...,  0.1038,  0.0335, -0.0874],\n                      [-0.0160,  0.0111,  0.0566,  ...,  0.0767, -0.0543, -0.0278],\n                      ...,\n                      [ 0.0708, -0.0519, -0.1038,  ...,  0.0283,  0.0652, -0.0464],\n                      [-0.0095, -0.0723,  0.0647,  ..., -0.1094,  0.0422,  0.0598],\n                      [-0.0601,  0.0037, -0.0347,  ...,  0.0757, -0.0477, -0.0931]])),\n             ('linear3.bias',\n              tensor([ 0.0536,  0.0348, -0.0470,  0.0299,  0.0891, -0.0591,  0.0856,  0.0887,\n                       0.0428,  0.0338]))])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a2_dt.state_dict())\n",
    "df_net_a2_dt = train_test(device, train_dataloader, test_dataloader, net_a2_dt, learning_rate, epochs)\n",
    "df_net_a2_dt.to_csv('NetA2DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2HT_trained.pt')\n",
    "net_a2_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T19:18:03.369057Z",
     "start_time": "2024-06-16T19:12:34.316550Z"
    }
   },
   "id": "d2e02ef68d3a9f05",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca126ac79b44ed2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
