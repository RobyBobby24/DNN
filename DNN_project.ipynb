{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.models.optical_flow.raft import ResidualBlock\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:18:20.960929Z",
     "start_time": "2024-06-12T18:18:18.159954Z"
    }
   },
   "id": "685462a85f2ae1e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:18:22.075421Z",
     "start_time": "2024-06-12T18:18:22.071217Z"
    }
   },
   "id": "8fb89e7978c49f98",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bdd2924b375a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA1(nn.Module):\n",
    "    def __init__(self, num_classes: int, freeze: bool = False):\n",
    "        super(NetA1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(676, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        if freeze:\n",
    "            for param in self.conv1.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T18:18:23.721754Z",
     "start_time": "2024-06-12T18:18:23.716032Z"
    }
   },
   "id": "8e067a677d103ee6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA2(nn.Module):\n",
    "    def __init__(self, num_classes: int, freeze: bool = False):\n",
    "        super(NetA2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=6, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-5)\n",
    "        self.linear1 = nn.Linear(150, 200)\n",
    "        self.linear2 = nn.Linear(200, 100)\n",
    "        self.linear3 = nn.Linear(100, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        if freeze:\n",
    "            for param in self.conv1.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.pool1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T13:32:55.529253Z",
     "start_time": "2024-06-12T13:32:55.525738Z"
    }
   },
   "id": "4f398a6ac2c01f29",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99360d229c0dbc75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1, 3, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization_weights = torch.tensor([\n",
    "    [[[1, 0, 1], [0, 1, 0], [1, 0, 1]]],\n",
    "    [[[1, 1, 0], [0, 0, 1], [1, 1, 0]]],\n",
    "    [[[0, 1, 1], [1, 0, 0], [0, 1, 1]]],\n",
    "    [[[0, 1, 0], [1, 1, 0], [0, 1, 0]]]\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "initialization_biases = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "initialization_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T13:32:55.567381Z",
     "start_time": "2024-06-12T13:32:55.533261Z"
    }
   },
   "id": "3e5ae64e8c459822",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0195, -0.0337, -0.0321,  ..., -0.0025, -0.0069, -0.0187],\n",
      "        [ 0.0010,  0.0261,  0.0348,  ..., -0.0190,  0.0063,  0.0371],\n",
      "        [-0.0273, -0.0210, -0.0034,  ...,  0.0254, -0.0261,  0.0278],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0100,  0.0176,  ..., -0.0079, -0.0256, -0.0278],\n",
      "        [-0.0327,  0.0104, -0.0244,  ...,  0.0367,  0.0215,  0.0012],\n",
      "        [-0.0068,  0.0347,  0.0066,  ...,  0.0278,  0.0149,  0.0346]])), ('linear1.bias', tensor([-0.0050,  0.0207, -0.0220,  0.0372, -0.0149,  0.0130,  0.0200, -0.0066,\n",
      "         0.0198, -0.0099]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0195, -0.0337, -0.0321,  ..., -0.0025, -0.0069, -0.0187],\n",
      "        [ 0.0010,  0.0261,  0.0348,  ..., -0.0190,  0.0063,  0.0371],\n",
      "        [-0.0273, -0.0210, -0.0034,  ...,  0.0254, -0.0261,  0.0278],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0100,  0.0176,  ..., -0.0079, -0.0256, -0.0278],\n",
      "        [-0.0327,  0.0104, -0.0244,  ...,  0.0367,  0.0215,  0.0012],\n",
      "        [-0.0068,  0.0347,  0.0066,  ...,  0.0278,  0.0149,  0.0346]])), ('linear1.bias', tensor([-0.0050,  0.0207, -0.0220,  0.0372, -0.0149,  0.0130,  0.0200, -0.0066,\n",
      "         0.0198, -0.0099]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.2927, -0.1432,  0.2747],\n",
      "          [ 0.1163, -0.1492,  0.1970],\n",
      "          [-0.2993, -0.2226, -0.0555]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2971, -0.0477,  0.0934],\n",
      "          [-0.2647,  0.3319, -0.0030],\n",
      "          [ 0.3074,  0.2674, -0.3014]]],\n",
      "\n",
      "\n",
      "        [[[-0.0546,  0.1789,  0.2904],\n",
      "          [ 0.0404,  0.1238, -0.3019],\n",
      "          [ 0.3023, -0.1766, -0.1893]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2114, -0.0850,  0.1869],\n",
      "          [-0.2506,  0.2259,  0.0907],\n",
      "          [-0.3186, -0.0549, -0.0145]]]])), ('conv1.bias', tensor([ 0.0522,  0.2895, -0.2675,  0.0695])), ('linear1.weight', tensor([[-0.0195, -0.0337, -0.0321,  ..., -0.0025, -0.0069, -0.0187],\n",
      "        [ 0.0010,  0.0261,  0.0348,  ..., -0.0190,  0.0063,  0.0371],\n",
      "        [-0.0273, -0.0210, -0.0034,  ...,  0.0254, -0.0261,  0.0278],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0100,  0.0176,  ..., -0.0079, -0.0256, -0.0278],\n",
      "        [-0.0327,  0.0104, -0.0244,  ...,  0.0367,  0.0215,  0.0012],\n",
      "        [-0.0068,  0.0347,  0.0066,  ...,  0.0278,  0.0149,  0.0346]])), ('linear1.bias', tensor([-0.0050,  0.0207, -0.0220,  0.0372, -0.0149,  0.0130,  0.0200, -0.0066,\n",
      "         0.0198, -0.0099]))])\n"
     ]
    }
   ],
   "source": [
    "net_a1_hf = NetA1(10, True)\n",
    "net_a1_ht = NetA1(10)\n",
    "net_a1_dt = NetA1(10)\n",
    "\n",
    "#set conv1 initialization\n",
    "net_a1_hf.conv1.weight = nn.Parameter(initialization_weights)\n",
    "net_a1_hf.conv1.bias = nn.Parameter(initialization_biases)\n",
    "\n",
    "# set same weights and bias to each layer of each network (except for cov1 of net_a1_dt)\n",
    "net_a1_ht.load_state_dict(net_a1_hf.state_dict())\n",
    "net_a1_dt.linear1.load_state_dict(net_a1_hf.linear1.state_dict())\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1H+_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a1_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a1_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a1_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T14:09:38.536349Z",
     "start_time": "2024-06-12T14:09:38.520225Z"
    }
   },
   "id": "ed80d43768e0ffd6",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Analysys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e527a7ee28f832"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1: \n",
      " \t|W_{conv_a1_hf} - W_{conv_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print( \"Net_A1: \\n\",\n",
    "       \"\\t|W_{conv_a1_hf} - W_{conv_a1_ht}| =\", torch.norm(net_a1_hf.conv1.weight - net_a1_ht.conv1.weight),\"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_ht}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_ht.linear1.weight), \"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_dt}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_dt.linear1.weight))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T14:07:33.668529Z",
     "start_time": "2024-06-12T14:07:33.662264Z"
    }
   },
   "id": "eabba971baa9ea0d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9884bc78c276b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T09:55:17.462469Z",
     "start_time": "2024-06-12T09:55:17.414793Z"
    }
   },
   "id": "e8c282a53727943",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "image, label = train_data[sample_idx]\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T09:41:59.497602Z",
     "start_time": "2024-06-12T09:41:59.489494Z"
    }
   },
   "id": "84fa5b26a60379de",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader= DataLoader(train_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T10:21:54.586088Z",
     "start_time": "2024-06-12T10:21:54.581101Z"
    }
   },
   "id": "be585d7eb8bcae35",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training/Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238fefd45b206a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "      size = len(dataloader.dataset)\n",
    "      num_batches = len(dataloader)\n",
    "      test_loss, correct = 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          pred = model(X)\n",
    "          test_loss += loss_fn(pred, y).item()\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      test_loss /= num_batches\n",
    "      correct /= size\n",
    "      print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "      return (100*correct)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b06878e4ef1f642"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79254d3d93db89e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b437c40fa560695f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b905630a2cd56a7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "134957982e841eca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
