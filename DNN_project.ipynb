{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.models.optical_flow.raft import ResidualBlock\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.429838Z",
     "start_time": "2024-06-14T06:11:12.399172Z"
    }
   },
   "id": "685462a85f2ae1e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.436270Z",
     "start_time": "2024-06-14T06:11:16.431848Z"
    }
   },
   "id": "8fb89e7978c49f98",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bdd2924b375a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA1(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(676, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "            \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.445134Z",
     "start_time": "2024-06-14T06:11:16.438280Z"
    }
   },
   "id": "8e067a677d103ee6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA2(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=6, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(216, 260)\n",
    "        self.linear2 = nn.Linear(260, 160)\n",
    "        self.linear3 = nn.Linear(160, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.455180Z",
     "start_time": "2024-06-14T06:11:16.448144Z"
    }
   },
   "id": "4f398a6ac2c01f29",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99360d229c0dbc75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1, 3, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization_weights = torch.tensor([\n",
    "    [[[1, 0, 1], [0, 1, 0], [1, 0, 1]]],\n",
    "    [[[1, 1, 0], [0, 0, 1], [1, 1, 0]]],\n",
    "    [[[0, 1, 1], [1, 0, 0], [0, 1, 1]]],\n",
    "    [[[0, 1, 0], [1, 1, 0], [0, 1, 0]]]\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "initialization_biases = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "initialization_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.468024Z",
     "start_time": "2024-06-14T06:11:16.457194Z"
    }
   },
   "id": "3e5ae64e8c459822",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0285,  0.0300,  0.0261,  ...,  0.0316, -0.0054, -0.0235],\n",
      "        [-0.0001,  0.0382, -0.0200,  ...,  0.0139, -0.0020,  0.0334],\n",
      "        [-0.0203, -0.0349,  0.0247,  ..., -0.0347,  0.0140, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0161,  0.0051,  ..., -0.0248,  0.0294, -0.0144],\n",
      "        [ 0.0372,  0.0311, -0.0312,  ..., -0.0147,  0.0037,  0.0027],\n",
      "        [-0.0196,  0.0066, -0.0079,  ...,  0.0210, -0.0169,  0.0009]])), ('linear1.bias', tensor([ 0.0192,  0.0374,  0.0258, -0.0071,  0.0304, -0.0120, -0.0228,  0.0204,\n",
      "        -0.0246, -0.0212]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0285,  0.0300,  0.0261,  ...,  0.0316, -0.0054, -0.0235],\n",
      "        [-0.0001,  0.0382, -0.0200,  ...,  0.0139, -0.0020,  0.0334],\n",
      "        [-0.0203, -0.0349,  0.0247,  ..., -0.0347,  0.0140, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0161,  0.0051,  ..., -0.0248,  0.0294, -0.0144],\n",
      "        [ 0.0372,  0.0311, -0.0312,  ..., -0.0147,  0.0037,  0.0027],\n",
      "        [-0.0196,  0.0066, -0.0079,  ...,  0.0210, -0.0169,  0.0009]])), ('linear1.bias', tensor([ 0.0192,  0.0374,  0.0258, -0.0071,  0.0304, -0.0120, -0.0228,  0.0204,\n",
      "        -0.0246, -0.0212]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.1554,  0.0492,  0.3141],\n",
      "          [-0.1258, -0.3250, -0.0167],\n",
      "          [ 0.0201, -0.1168,  0.3246]]],\n",
      "\n",
      "\n",
      "        [[[-0.0118,  0.2718, -0.2517],\n",
      "          [-0.0388, -0.2801,  0.1957],\n",
      "          [ 0.1790,  0.2293,  0.0635]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1669,  0.1119,  0.1287],\n",
      "          [ 0.0206, -0.3305, -0.0627],\n",
      "          [ 0.2295, -0.1768,  0.1198]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1811, -0.2077, -0.1208],\n",
      "          [ 0.0919, -0.1740,  0.1239],\n",
      "          [-0.3191, -0.0752, -0.1332]]]])), ('conv1.bias', tensor([-0.1342,  0.2487,  0.1029, -0.3292])), ('linear1.weight', tensor([[-0.0285,  0.0300,  0.0261,  ...,  0.0316, -0.0054, -0.0235],\n",
      "        [-0.0001,  0.0382, -0.0200,  ...,  0.0139, -0.0020,  0.0334],\n",
      "        [-0.0203, -0.0349,  0.0247,  ..., -0.0347,  0.0140, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0161,  0.0051,  ..., -0.0248,  0.0294, -0.0144],\n",
      "        [ 0.0372,  0.0311, -0.0312,  ..., -0.0147,  0.0037,  0.0027],\n",
      "        [-0.0196,  0.0066, -0.0079,  ...,  0.0210, -0.0169,  0.0009]])), ('linear1.bias', tensor([ 0.0192,  0.0374,  0.0258, -0.0071,  0.0304, -0.0120, -0.0228,  0.0204,\n",
      "        -0.0246, -0.0212]))])\n"
     ]
    }
   ],
   "source": [
    "net_a1_hf = NetA1(10)\n",
    "net_a1_ht = NetA1(10)\n",
    "net_a1_dt = NetA1(10)\n",
    "\n",
    "#set conv1 initialization of net_a1_hf\n",
    "net_a1_hf.conv1.weight = nn.Parameter(initialization_weights)\n",
    "net_a1_hf.conv1.bias = nn.Parameter(initialization_biases)\n",
    "\n",
    "# set same weights and bias to each layer of each network (except for cov1 of net_a1_dt)\n",
    "net_a1_ht.load_state_dict(net_a1_hf.state_dict())\n",
    "net_a1_dt.linear1.load_state_dict(net_a1_hf.linear1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a1_hf\n",
    "net_a1_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_init.pt')\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a1_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a1_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a1_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.495121Z",
     "start_time": "2024-06-14T06:11:16.470036Z"
    }
   },
   "id": "ed80d43768e0ffd6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.0325,  0.0284, -0.0623],\n",
      "          [-0.1249, -0.0975, -0.0574],\n",
      "          [-0.1139,  0.1273,  0.1448]],\n",
      "\n",
      "         [[-0.0991, -0.1663,  0.0272],\n",
      "          [-0.0730, -0.1493, -0.0154],\n",
      "          [-0.0410, -0.0499, -0.0529]],\n",
      "\n",
      "         [[-0.0919, -0.1395, -0.0721],\n",
      "          [ 0.0850, -0.0454, -0.1195],\n",
      "          [-0.1009, -0.1657, -0.0756]],\n",
      "\n",
      "         [[-0.1144,  0.1369, -0.1300],\n",
      "          [ 0.0900,  0.1615, -0.0008],\n",
      "          [ 0.0127, -0.0586, -0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0809,  0.0132, -0.1606],\n",
      "          [ 0.0781, -0.1317, -0.0225],\n",
      "          [ 0.1474, -0.1250,  0.0125]],\n",
      "\n",
      "         [[-0.0346, -0.1193,  0.0190],\n",
      "          [-0.0575, -0.0207, -0.0965],\n",
      "          [-0.0778,  0.1130,  0.0008]],\n",
      "\n",
      "         [[-0.0306, -0.0840, -0.0102],\n",
      "          [-0.1195, -0.1220, -0.0860],\n",
      "          [ 0.0968,  0.1513,  0.1004]],\n",
      "\n",
      "         [[ 0.0602, -0.0723, -0.0724],\n",
      "          [ 0.1535,  0.1232, -0.0509],\n",
      "          [ 0.1217,  0.0015, -0.0050]]],\n",
      "\n",
      "\n",
      "        [[[-0.0610,  0.0304,  0.0468],\n",
      "          [ 0.0843, -0.1370, -0.0962],\n",
      "          [-0.0781,  0.0643, -0.1439]],\n",
      "\n",
      "         [[-0.1540, -0.1068,  0.1349],\n",
      "          [-0.1567,  0.0392, -0.0738],\n",
      "          [ 0.0198, -0.0759, -0.0948]],\n",
      "\n",
      "         [[ 0.1234, -0.0727, -0.0851],\n",
      "          [ 0.0684,  0.0765, -0.0228],\n",
      "          [ 0.0907,  0.0599,  0.0480]],\n",
      "\n",
      "         [[-0.0470,  0.1228,  0.1179],\n",
      "          [-0.1107, -0.1607, -0.1541],\n",
      "          [ 0.1661, -0.1058,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[-0.0410,  0.0713, -0.1394],\n",
      "          [ 0.0548,  0.0346, -0.0403],\n",
      "          [ 0.0538, -0.0518, -0.0610]],\n",
      "\n",
      "         [[-0.0095,  0.0843, -0.0472],\n",
      "          [-0.1273,  0.1123,  0.0875],\n",
      "          [ 0.1615,  0.0843, -0.0878]],\n",
      "\n",
      "         [[ 0.1459, -0.0306, -0.0433],\n",
      "          [-0.0410, -0.1649, -0.0460],\n",
      "          [ 0.0469,  0.1526, -0.0194]],\n",
      "\n",
      "         [[-0.1648,  0.0888, -0.1383],\n",
      "          [-0.1388, -0.1538, -0.0848],\n",
      "          [ 0.0168, -0.1186, -0.0583]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1182, -0.0404,  0.0531],\n",
      "          [-0.1610, -0.1490,  0.0160],\n",
      "          [-0.0912, -0.0494, -0.1327]],\n",
      "\n",
      "         [[ 0.1034,  0.0956,  0.1473],\n",
      "          [ 0.0613, -0.0994, -0.0578],\n",
      "          [ 0.0829, -0.1383, -0.0902]],\n",
      "\n",
      "         [[ 0.0989,  0.0064, -0.0326],\n",
      "          [ 0.0794, -0.0888, -0.1068],\n",
      "          [-0.0046, -0.1513, -0.1542]],\n",
      "\n",
      "         [[-0.0626, -0.0942, -0.0026],\n",
      "          [-0.0396,  0.0142,  0.1031],\n",
      "          [-0.0473, -0.0818, -0.0098]]],\n",
      "\n",
      "\n",
      "        [[[-0.0069, -0.0117,  0.0255],\n",
      "          [-0.0209,  0.0081,  0.1553],\n",
      "          [ 0.0899,  0.1466,  0.1256]],\n",
      "\n",
      "         [[-0.1593, -0.1633, -0.0011],\n",
      "          [ 0.0072,  0.0455, -0.0941],\n",
      "          [-0.0620, -0.1344, -0.1229]],\n",
      "\n",
      "         [[ 0.0810, -0.1419, -0.0080],\n",
      "          [ 0.0387, -0.0982, -0.0490],\n",
      "          [ 0.0565,  0.0350,  0.0597]],\n",
      "\n",
      "         [[-0.0528, -0.1506, -0.0785],\n",
      "          [ 0.0006,  0.1363,  0.0784],\n",
      "          [ 0.1635, -0.0515, -0.0914]]]])), ('conv2.bias', tensor([-0.1224, -0.0264, -0.0845,  0.1049, -0.0654, -0.1082])), ('linear1.weight', tensor([[-0.0206, -0.0641,  0.0034,  ..., -0.0243,  0.0473,  0.0330],\n",
      "        [-0.0453,  0.0414, -0.0348,  ..., -0.0590, -0.0140, -0.0303],\n",
      "        [-0.0419, -0.0392, -0.0187,  ...,  0.0619, -0.0015, -0.0582],\n",
      "        ...,\n",
      "        [-0.0277, -0.0165,  0.0008,  ...,  0.0579, -0.0196,  0.0251],\n",
      "        [-0.0230, -0.0579, -0.0182,  ...,  0.0242,  0.0454, -0.0305],\n",
      "        [-0.0319,  0.0070, -0.0127,  ..., -0.0471, -0.0182, -0.0036]])), ('linear1.bias', tensor([ 0.0148,  0.0232,  0.0260, -0.0172, -0.0278, -0.0074, -0.0153,  0.0482,\n",
      "         0.0332,  0.0598, -0.0679,  0.0283,  0.0190,  0.0652, -0.0353, -0.0567,\n",
      "        -0.0344, -0.0604, -0.0462, -0.0228, -0.0241, -0.0058, -0.0381, -0.0499,\n",
      "        -0.0255,  0.0149, -0.0261,  0.0613, -0.0596, -0.0328, -0.0372,  0.0116,\n",
      "        -0.0155, -0.0431, -0.0047, -0.0657, -0.0584,  0.0586, -0.0176,  0.0495,\n",
      "        -0.0392, -0.0527,  0.0135,  0.0306,  0.0526, -0.0646, -0.0352, -0.0057,\n",
      "         0.0491, -0.0117,  0.0041,  0.0073,  0.0295, -0.0032, -0.0441,  0.0658,\n",
      "         0.0078, -0.0582, -0.0069,  0.0142, -0.0590, -0.0581, -0.0426,  0.0145,\n",
      "         0.0123, -0.0156,  0.0068, -0.0500, -0.0118, -0.0031, -0.0171, -0.0520,\n",
      "        -0.0344, -0.0636,  0.0012, -0.0160, -0.0130,  0.0193, -0.0512,  0.0130,\n",
      "         0.0282, -0.0528,  0.0164,  0.0013, -0.0158,  0.0563,  0.0439,  0.0200,\n",
      "        -0.0365,  0.0184, -0.0159, -0.0342, -0.0546,  0.0447, -0.0247, -0.0002,\n",
      "        -0.0290, -0.0269, -0.0039,  0.0014, -0.0659, -0.0663,  0.0332,  0.0120,\n",
      "         0.0221, -0.0477,  0.0280,  0.0414,  0.0560, -0.0559, -0.0516,  0.0181,\n",
      "        -0.0069,  0.0458, -0.0214, -0.0105,  0.0671,  0.0395,  0.0370, -0.0368,\n",
      "         0.0279,  0.0268,  0.0092, -0.0120,  0.0029, -0.0229, -0.0063, -0.0409,\n",
      "         0.0260, -0.0188,  0.0330,  0.0267,  0.0371,  0.0031, -0.0355,  0.0553,\n",
      "         0.0183, -0.0610, -0.0590, -0.0150,  0.0391, -0.0329,  0.0349, -0.0531,\n",
      "         0.0097, -0.0078,  0.0421,  0.0281, -0.0430,  0.0073,  0.0128, -0.0022,\n",
      "        -0.0566,  0.0185,  0.0343, -0.0058, -0.0504, -0.0644, -0.0614,  0.0600,\n",
      "        -0.0165,  0.0483, -0.0525, -0.0594, -0.0187,  0.0119,  0.0204, -0.0480,\n",
      "        -0.0233, -0.0394,  0.0240,  0.0550, -0.0389,  0.0311,  0.0506,  0.0101,\n",
      "        -0.0282,  0.0204,  0.0562,  0.0510, -0.0073, -0.0590,  0.0235,  0.0025,\n",
      "         0.0254, -0.0137, -0.0311,  0.0356, -0.0423, -0.0479, -0.0558, -0.0082,\n",
      "         0.0548, -0.0664, -0.0625,  0.0090, -0.0472, -0.0112, -0.0320,  0.0245,\n",
      "        -0.0029, -0.0486, -0.0540, -0.0576,  0.0672,  0.0187,  0.0186, -0.0639,\n",
      "        -0.0385,  0.0543,  0.0350, -0.0056,  0.0604, -0.0576,  0.0073, -0.0307,\n",
      "        -0.0395, -0.0469,  0.0071, -0.0314,  0.0094, -0.0433, -0.0628, -0.0679,\n",
      "         0.0639, -0.0641,  0.0351, -0.0634, -0.0624,  0.0016, -0.0668, -0.0475,\n",
      "         0.0074,  0.0403,  0.0399,  0.0379,  0.0409, -0.0099,  0.0402,  0.0446,\n",
      "         0.0572,  0.0134,  0.0097,  0.0375, -0.0279, -0.0515,  0.0461,  0.0314,\n",
      "         0.0381,  0.0388, -0.0522, -0.0443,  0.0565,  0.0091, -0.0038, -0.0484,\n",
      "         0.0505, -0.0130,  0.0039,  0.0194])), ('linear2.weight', tensor([[ 0.0124,  0.0055, -0.0209,  ..., -0.0362,  0.0074, -0.0276],\n",
      "        [ 0.0121,  0.0601,  0.0265,  ..., -0.0538,  0.0411, -0.0039],\n",
      "        [ 0.0559,  0.0324,  0.0414,  ..., -0.0306,  0.0048, -0.0427],\n",
      "        ...,\n",
      "        [-0.0013, -0.0318, -0.0039,  ..., -0.0016, -0.0054,  0.0280],\n",
      "        [ 0.0394, -0.0270, -0.0293,  ..., -0.0117, -0.0044,  0.0567],\n",
      "        [-0.0315, -0.0259,  0.0557,  ..., -0.0147,  0.0472, -0.0079]])), ('linear2.bias', tensor([ 0.0182,  0.0011,  0.0441, -0.0100, -0.0603,  0.0026, -0.0383,  0.0470,\n",
      "        -0.0033, -0.0465,  0.0011,  0.0402,  0.0268,  0.0232,  0.0151,  0.0205,\n",
      "        -0.0291,  0.0003,  0.0591, -0.0351,  0.0351, -0.0219, -0.0260,  0.0504,\n",
      "        -0.0424, -0.0536, -0.0470,  0.0440,  0.0353,  0.0111,  0.0394,  0.0318,\n",
      "        -0.0059, -0.0424,  0.0611,  0.0047, -0.0300,  0.0347, -0.0186, -0.0309,\n",
      "         0.0268,  0.0377,  0.0153, -0.0134, -0.0204,  0.0373, -0.0040, -0.0403,\n",
      "         0.0061, -0.0045, -0.0542, -0.0169, -0.0041,  0.0367, -0.0415, -0.0464,\n",
      "         0.0538,  0.0521,  0.0602, -0.0401, -0.0298, -0.0600, -0.0606, -0.0158,\n",
      "         0.0151, -0.0113, -0.0267, -0.0014,  0.0586, -0.0200, -0.0247,  0.0470,\n",
      "         0.0620, -0.0078,  0.0185,  0.0437,  0.0309,  0.0165, -0.0409, -0.0619,\n",
      "         0.0438,  0.0088, -0.0403, -0.0504,  0.0056, -0.0300,  0.0400, -0.0207,\n",
      "        -0.0289, -0.0310,  0.0540, -0.0611, -0.0250,  0.0609, -0.0117, -0.0117,\n",
      "         0.0236,  0.0178,  0.0336, -0.0340, -0.0144, -0.0125, -0.0164,  0.0326,\n",
      "        -0.0449,  0.0394, -0.0314, -0.0156, -0.0122, -0.0377, -0.0410, -0.0355,\n",
      "         0.0183, -0.0277, -0.0489,  0.0596,  0.0430,  0.0321, -0.0320, -0.0260,\n",
      "        -0.0370,  0.0440, -0.0152, -0.0443,  0.0471, -0.0105, -0.0544, -0.0131,\n",
      "         0.0543, -0.0534, -0.0060, -0.0414, -0.0168,  0.0609, -0.0369, -0.0014,\n",
      "        -0.0546,  0.0244,  0.0222, -0.0390, -0.0386,  0.0146, -0.0258, -0.0462,\n",
      "        -0.0005, -0.0210,  0.0430,  0.0159, -0.0131, -0.0525, -0.0224,  0.0107,\n",
      "         0.0347, -0.0272, -0.0119,  0.0070, -0.0479, -0.0294, -0.0326,  0.0178])), ('linear3.weight', tensor([[ 0.0765,  0.0095,  0.0199,  ...,  0.0642, -0.0430,  0.0373],\n",
      "        [ 0.0082, -0.0006, -0.0244,  ..., -0.0035,  0.0138,  0.0571],\n",
      "        [ 0.0031,  0.0721,  0.0004,  ...,  0.0366, -0.0609, -0.0017],\n",
      "        ...,\n",
      "        [ 0.0498,  0.0092,  0.0020,  ...,  0.0388,  0.0301,  0.0317],\n",
      "        [ 0.0228, -0.0107,  0.0357,  ...,  0.0064,  0.0208,  0.0161],\n",
      "        [ 0.0763, -0.0252,  0.0699,  ...,  0.0393,  0.0048,  0.0011]])), ('linear3.bias', tensor([-0.0009, -0.0156,  0.0297, -0.0279,  0.0658,  0.0657,  0.0239, -0.0635,\n",
      "        -0.0557,  0.0611]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.0325,  0.0284, -0.0623],\n",
      "          [-0.1249, -0.0975, -0.0574],\n",
      "          [-0.1139,  0.1273,  0.1448]],\n",
      "\n",
      "         [[-0.0991, -0.1663,  0.0272],\n",
      "          [-0.0730, -0.1493, -0.0154],\n",
      "          [-0.0410, -0.0499, -0.0529]],\n",
      "\n",
      "         [[-0.0919, -0.1395, -0.0721],\n",
      "          [ 0.0850, -0.0454, -0.1195],\n",
      "          [-0.1009, -0.1657, -0.0756]],\n",
      "\n",
      "         [[-0.1144,  0.1369, -0.1300],\n",
      "          [ 0.0900,  0.1615, -0.0008],\n",
      "          [ 0.0127, -0.0586, -0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0809,  0.0132, -0.1606],\n",
      "          [ 0.0781, -0.1317, -0.0225],\n",
      "          [ 0.1474, -0.1250,  0.0125]],\n",
      "\n",
      "         [[-0.0346, -0.1193,  0.0190],\n",
      "          [-0.0575, -0.0207, -0.0965],\n",
      "          [-0.0778,  0.1130,  0.0008]],\n",
      "\n",
      "         [[-0.0306, -0.0840, -0.0102],\n",
      "          [-0.1195, -0.1220, -0.0860],\n",
      "          [ 0.0968,  0.1513,  0.1004]],\n",
      "\n",
      "         [[ 0.0602, -0.0723, -0.0724],\n",
      "          [ 0.1535,  0.1232, -0.0509],\n",
      "          [ 0.1217,  0.0015, -0.0050]]],\n",
      "\n",
      "\n",
      "        [[[-0.0610,  0.0304,  0.0468],\n",
      "          [ 0.0843, -0.1370, -0.0962],\n",
      "          [-0.0781,  0.0643, -0.1439]],\n",
      "\n",
      "         [[-0.1540, -0.1068,  0.1349],\n",
      "          [-0.1567,  0.0392, -0.0738],\n",
      "          [ 0.0198, -0.0759, -0.0948]],\n",
      "\n",
      "         [[ 0.1234, -0.0727, -0.0851],\n",
      "          [ 0.0684,  0.0765, -0.0228],\n",
      "          [ 0.0907,  0.0599,  0.0480]],\n",
      "\n",
      "         [[-0.0470,  0.1228,  0.1179],\n",
      "          [-0.1107, -0.1607, -0.1541],\n",
      "          [ 0.1661, -0.1058,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[-0.0410,  0.0713, -0.1394],\n",
      "          [ 0.0548,  0.0346, -0.0403],\n",
      "          [ 0.0538, -0.0518, -0.0610]],\n",
      "\n",
      "         [[-0.0095,  0.0843, -0.0472],\n",
      "          [-0.1273,  0.1123,  0.0875],\n",
      "          [ 0.1615,  0.0843, -0.0878]],\n",
      "\n",
      "         [[ 0.1459, -0.0306, -0.0433],\n",
      "          [-0.0410, -0.1649, -0.0460],\n",
      "          [ 0.0469,  0.1526, -0.0194]],\n",
      "\n",
      "         [[-0.1648,  0.0888, -0.1383],\n",
      "          [-0.1388, -0.1538, -0.0848],\n",
      "          [ 0.0168, -0.1186, -0.0583]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1182, -0.0404,  0.0531],\n",
      "          [-0.1610, -0.1490,  0.0160],\n",
      "          [-0.0912, -0.0494, -0.1327]],\n",
      "\n",
      "         [[ 0.1034,  0.0956,  0.1473],\n",
      "          [ 0.0613, -0.0994, -0.0578],\n",
      "          [ 0.0829, -0.1383, -0.0902]],\n",
      "\n",
      "         [[ 0.0989,  0.0064, -0.0326],\n",
      "          [ 0.0794, -0.0888, -0.1068],\n",
      "          [-0.0046, -0.1513, -0.1542]],\n",
      "\n",
      "         [[-0.0626, -0.0942, -0.0026],\n",
      "          [-0.0396,  0.0142,  0.1031],\n",
      "          [-0.0473, -0.0818, -0.0098]]],\n",
      "\n",
      "\n",
      "        [[[-0.0069, -0.0117,  0.0255],\n",
      "          [-0.0209,  0.0081,  0.1553],\n",
      "          [ 0.0899,  0.1466,  0.1256]],\n",
      "\n",
      "         [[-0.1593, -0.1633, -0.0011],\n",
      "          [ 0.0072,  0.0455, -0.0941],\n",
      "          [-0.0620, -0.1344, -0.1229]],\n",
      "\n",
      "         [[ 0.0810, -0.1419, -0.0080],\n",
      "          [ 0.0387, -0.0982, -0.0490],\n",
      "          [ 0.0565,  0.0350,  0.0597]],\n",
      "\n",
      "         [[-0.0528, -0.1506, -0.0785],\n",
      "          [ 0.0006,  0.1363,  0.0784],\n",
      "          [ 0.1635, -0.0515, -0.0914]]]])), ('conv2.bias', tensor([-0.1224, -0.0264, -0.0845,  0.1049, -0.0654, -0.1082])), ('linear1.weight', tensor([[-0.0206, -0.0641,  0.0034,  ..., -0.0243,  0.0473,  0.0330],\n",
      "        [-0.0453,  0.0414, -0.0348,  ..., -0.0590, -0.0140, -0.0303],\n",
      "        [-0.0419, -0.0392, -0.0187,  ...,  0.0619, -0.0015, -0.0582],\n",
      "        ...,\n",
      "        [-0.0277, -0.0165,  0.0008,  ...,  0.0579, -0.0196,  0.0251],\n",
      "        [-0.0230, -0.0579, -0.0182,  ...,  0.0242,  0.0454, -0.0305],\n",
      "        [-0.0319,  0.0070, -0.0127,  ..., -0.0471, -0.0182, -0.0036]])), ('linear1.bias', tensor([ 0.0148,  0.0232,  0.0260, -0.0172, -0.0278, -0.0074, -0.0153,  0.0482,\n",
      "         0.0332,  0.0598, -0.0679,  0.0283,  0.0190,  0.0652, -0.0353, -0.0567,\n",
      "        -0.0344, -0.0604, -0.0462, -0.0228, -0.0241, -0.0058, -0.0381, -0.0499,\n",
      "        -0.0255,  0.0149, -0.0261,  0.0613, -0.0596, -0.0328, -0.0372,  0.0116,\n",
      "        -0.0155, -0.0431, -0.0047, -0.0657, -0.0584,  0.0586, -0.0176,  0.0495,\n",
      "        -0.0392, -0.0527,  0.0135,  0.0306,  0.0526, -0.0646, -0.0352, -0.0057,\n",
      "         0.0491, -0.0117,  0.0041,  0.0073,  0.0295, -0.0032, -0.0441,  0.0658,\n",
      "         0.0078, -0.0582, -0.0069,  0.0142, -0.0590, -0.0581, -0.0426,  0.0145,\n",
      "         0.0123, -0.0156,  0.0068, -0.0500, -0.0118, -0.0031, -0.0171, -0.0520,\n",
      "        -0.0344, -0.0636,  0.0012, -0.0160, -0.0130,  0.0193, -0.0512,  0.0130,\n",
      "         0.0282, -0.0528,  0.0164,  0.0013, -0.0158,  0.0563,  0.0439,  0.0200,\n",
      "        -0.0365,  0.0184, -0.0159, -0.0342, -0.0546,  0.0447, -0.0247, -0.0002,\n",
      "        -0.0290, -0.0269, -0.0039,  0.0014, -0.0659, -0.0663,  0.0332,  0.0120,\n",
      "         0.0221, -0.0477,  0.0280,  0.0414,  0.0560, -0.0559, -0.0516,  0.0181,\n",
      "        -0.0069,  0.0458, -0.0214, -0.0105,  0.0671,  0.0395,  0.0370, -0.0368,\n",
      "         0.0279,  0.0268,  0.0092, -0.0120,  0.0029, -0.0229, -0.0063, -0.0409,\n",
      "         0.0260, -0.0188,  0.0330,  0.0267,  0.0371,  0.0031, -0.0355,  0.0553,\n",
      "         0.0183, -0.0610, -0.0590, -0.0150,  0.0391, -0.0329,  0.0349, -0.0531,\n",
      "         0.0097, -0.0078,  0.0421,  0.0281, -0.0430,  0.0073,  0.0128, -0.0022,\n",
      "        -0.0566,  0.0185,  0.0343, -0.0058, -0.0504, -0.0644, -0.0614,  0.0600,\n",
      "        -0.0165,  0.0483, -0.0525, -0.0594, -0.0187,  0.0119,  0.0204, -0.0480,\n",
      "        -0.0233, -0.0394,  0.0240,  0.0550, -0.0389,  0.0311,  0.0506,  0.0101,\n",
      "        -0.0282,  0.0204,  0.0562,  0.0510, -0.0073, -0.0590,  0.0235,  0.0025,\n",
      "         0.0254, -0.0137, -0.0311,  0.0356, -0.0423, -0.0479, -0.0558, -0.0082,\n",
      "         0.0548, -0.0664, -0.0625,  0.0090, -0.0472, -0.0112, -0.0320,  0.0245,\n",
      "        -0.0029, -0.0486, -0.0540, -0.0576,  0.0672,  0.0187,  0.0186, -0.0639,\n",
      "        -0.0385,  0.0543,  0.0350, -0.0056,  0.0604, -0.0576,  0.0073, -0.0307,\n",
      "        -0.0395, -0.0469,  0.0071, -0.0314,  0.0094, -0.0433, -0.0628, -0.0679,\n",
      "         0.0639, -0.0641,  0.0351, -0.0634, -0.0624,  0.0016, -0.0668, -0.0475,\n",
      "         0.0074,  0.0403,  0.0399,  0.0379,  0.0409, -0.0099,  0.0402,  0.0446,\n",
      "         0.0572,  0.0134,  0.0097,  0.0375, -0.0279, -0.0515,  0.0461,  0.0314,\n",
      "         0.0381,  0.0388, -0.0522, -0.0443,  0.0565,  0.0091, -0.0038, -0.0484,\n",
      "         0.0505, -0.0130,  0.0039,  0.0194])), ('linear2.weight', tensor([[ 0.0124,  0.0055, -0.0209,  ..., -0.0362,  0.0074, -0.0276],\n",
      "        [ 0.0121,  0.0601,  0.0265,  ..., -0.0538,  0.0411, -0.0039],\n",
      "        [ 0.0559,  0.0324,  0.0414,  ..., -0.0306,  0.0048, -0.0427],\n",
      "        ...,\n",
      "        [-0.0013, -0.0318, -0.0039,  ..., -0.0016, -0.0054,  0.0280],\n",
      "        [ 0.0394, -0.0270, -0.0293,  ..., -0.0117, -0.0044,  0.0567],\n",
      "        [-0.0315, -0.0259,  0.0557,  ..., -0.0147,  0.0472, -0.0079]])), ('linear2.bias', tensor([ 0.0182,  0.0011,  0.0441, -0.0100, -0.0603,  0.0026, -0.0383,  0.0470,\n",
      "        -0.0033, -0.0465,  0.0011,  0.0402,  0.0268,  0.0232,  0.0151,  0.0205,\n",
      "        -0.0291,  0.0003,  0.0591, -0.0351,  0.0351, -0.0219, -0.0260,  0.0504,\n",
      "        -0.0424, -0.0536, -0.0470,  0.0440,  0.0353,  0.0111,  0.0394,  0.0318,\n",
      "        -0.0059, -0.0424,  0.0611,  0.0047, -0.0300,  0.0347, -0.0186, -0.0309,\n",
      "         0.0268,  0.0377,  0.0153, -0.0134, -0.0204,  0.0373, -0.0040, -0.0403,\n",
      "         0.0061, -0.0045, -0.0542, -0.0169, -0.0041,  0.0367, -0.0415, -0.0464,\n",
      "         0.0538,  0.0521,  0.0602, -0.0401, -0.0298, -0.0600, -0.0606, -0.0158,\n",
      "         0.0151, -0.0113, -0.0267, -0.0014,  0.0586, -0.0200, -0.0247,  0.0470,\n",
      "         0.0620, -0.0078,  0.0185,  0.0437,  0.0309,  0.0165, -0.0409, -0.0619,\n",
      "         0.0438,  0.0088, -0.0403, -0.0504,  0.0056, -0.0300,  0.0400, -0.0207,\n",
      "        -0.0289, -0.0310,  0.0540, -0.0611, -0.0250,  0.0609, -0.0117, -0.0117,\n",
      "         0.0236,  0.0178,  0.0336, -0.0340, -0.0144, -0.0125, -0.0164,  0.0326,\n",
      "        -0.0449,  0.0394, -0.0314, -0.0156, -0.0122, -0.0377, -0.0410, -0.0355,\n",
      "         0.0183, -0.0277, -0.0489,  0.0596,  0.0430,  0.0321, -0.0320, -0.0260,\n",
      "        -0.0370,  0.0440, -0.0152, -0.0443,  0.0471, -0.0105, -0.0544, -0.0131,\n",
      "         0.0543, -0.0534, -0.0060, -0.0414, -0.0168,  0.0609, -0.0369, -0.0014,\n",
      "        -0.0546,  0.0244,  0.0222, -0.0390, -0.0386,  0.0146, -0.0258, -0.0462,\n",
      "        -0.0005, -0.0210,  0.0430,  0.0159, -0.0131, -0.0525, -0.0224,  0.0107,\n",
      "         0.0347, -0.0272, -0.0119,  0.0070, -0.0479, -0.0294, -0.0326,  0.0178])), ('linear3.weight', tensor([[ 0.0765,  0.0095,  0.0199,  ...,  0.0642, -0.0430,  0.0373],\n",
      "        [ 0.0082, -0.0006, -0.0244,  ..., -0.0035,  0.0138,  0.0571],\n",
      "        [ 0.0031,  0.0721,  0.0004,  ...,  0.0366, -0.0609, -0.0017],\n",
      "        ...,\n",
      "        [ 0.0498,  0.0092,  0.0020,  ...,  0.0388,  0.0301,  0.0317],\n",
      "        [ 0.0228, -0.0107,  0.0357,  ...,  0.0064,  0.0208,  0.0161],\n",
      "        [ 0.0763, -0.0252,  0.0699,  ...,  0.0393,  0.0048,  0.0011]])), ('linear3.bias', tensor([-0.0009, -0.0156,  0.0297, -0.0279,  0.0658,  0.0657,  0.0239, -0.0635,\n",
      "        -0.0557,  0.0611]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.1554,  0.0492,  0.3141],\n",
      "          [-0.1258, -0.3250, -0.0167],\n",
      "          [ 0.0201, -0.1168,  0.3246]]],\n",
      "\n",
      "\n",
      "        [[[-0.0118,  0.2718, -0.2517],\n",
      "          [-0.0388, -0.2801,  0.1957],\n",
      "          [ 0.1790,  0.2293,  0.0635]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1669,  0.1119,  0.1287],\n",
      "          [ 0.0206, -0.3305, -0.0627],\n",
      "          [ 0.2295, -0.1768,  0.1198]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1811, -0.2077, -0.1208],\n",
      "          [ 0.0919, -0.1740,  0.1239],\n",
      "          [-0.3191, -0.0752, -0.1332]]]])), ('conv1.bias', tensor([-0.1342,  0.2487,  0.1029, -0.3292])), ('conv2.weight', tensor([[[[ 0.0325,  0.0284, -0.0623],\n",
      "          [-0.1249, -0.0975, -0.0574],\n",
      "          [-0.1139,  0.1273,  0.1448]],\n",
      "\n",
      "         [[-0.0991, -0.1663,  0.0272],\n",
      "          [-0.0730, -0.1493, -0.0154],\n",
      "          [-0.0410, -0.0499, -0.0529]],\n",
      "\n",
      "         [[-0.0919, -0.1395, -0.0721],\n",
      "          [ 0.0850, -0.0454, -0.1195],\n",
      "          [-0.1009, -0.1657, -0.0756]],\n",
      "\n",
      "         [[-0.1144,  0.1369, -0.1300],\n",
      "          [ 0.0900,  0.1615, -0.0008],\n",
      "          [ 0.0127, -0.0586, -0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0809,  0.0132, -0.1606],\n",
      "          [ 0.0781, -0.1317, -0.0225],\n",
      "          [ 0.1474, -0.1250,  0.0125]],\n",
      "\n",
      "         [[-0.0346, -0.1193,  0.0190],\n",
      "          [-0.0575, -0.0207, -0.0965],\n",
      "          [-0.0778,  0.1130,  0.0008]],\n",
      "\n",
      "         [[-0.0306, -0.0840, -0.0102],\n",
      "          [-0.1195, -0.1220, -0.0860],\n",
      "          [ 0.0968,  0.1513,  0.1004]],\n",
      "\n",
      "         [[ 0.0602, -0.0723, -0.0724],\n",
      "          [ 0.1535,  0.1232, -0.0509],\n",
      "          [ 0.1217,  0.0015, -0.0050]]],\n",
      "\n",
      "\n",
      "        [[[-0.0610,  0.0304,  0.0468],\n",
      "          [ 0.0843, -0.1370, -0.0962],\n",
      "          [-0.0781,  0.0643, -0.1439]],\n",
      "\n",
      "         [[-0.1540, -0.1068,  0.1349],\n",
      "          [-0.1567,  0.0392, -0.0738],\n",
      "          [ 0.0198, -0.0759, -0.0948]],\n",
      "\n",
      "         [[ 0.1234, -0.0727, -0.0851],\n",
      "          [ 0.0684,  0.0765, -0.0228],\n",
      "          [ 0.0907,  0.0599,  0.0480]],\n",
      "\n",
      "         [[-0.0470,  0.1228,  0.1179],\n",
      "          [-0.1107, -0.1607, -0.1541],\n",
      "          [ 0.1661, -0.1058,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[-0.0410,  0.0713, -0.1394],\n",
      "          [ 0.0548,  0.0346, -0.0403],\n",
      "          [ 0.0538, -0.0518, -0.0610]],\n",
      "\n",
      "         [[-0.0095,  0.0843, -0.0472],\n",
      "          [-0.1273,  0.1123,  0.0875],\n",
      "          [ 0.1615,  0.0843, -0.0878]],\n",
      "\n",
      "         [[ 0.1459, -0.0306, -0.0433],\n",
      "          [-0.0410, -0.1649, -0.0460],\n",
      "          [ 0.0469,  0.1526, -0.0194]],\n",
      "\n",
      "         [[-0.1648,  0.0888, -0.1383],\n",
      "          [-0.1388, -0.1538, -0.0848],\n",
      "          [ 0.0168, -0.1186, -0.0583]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1182, -0.0404,  0.0531],\n",
      "          [-0.1610, -0.1490,  0.0160],\n",
      "          [-0.0912, -0.0494, -0.1327]],\n",
      "\n",
      "         [[ 0.1034,  0.0956,  0.1473],\n",
      "          [ 0.0613, -0.0994, -0.0578],\n",
      "          [ 0.0829, -0.1383, -0.0902]],\n",
      "\n",
      "         [[ 0.0989,  0.0064, -0.0326],\n",
      "          [ 0.0794, -0.0888, -0.1068],\n",
      "          [-0.0046, -0.1513, -0.1542]],\n",
      "\n",
      "         [[-0.0626, -0.0942, -0.0026],\n",
      "          [-0.0396,  0.0142,  0.1031],\n",
      "          [-0.0473, -0.0818, -0.0098]]],\n",
      "\n",
      "\n",
      "        [[[-0.0069, -0.0117,  0.0255],\n",
      "          [-0.0209,  0.0081,  0.1553],\n",
      "          [ 0.0899,  0.1466,  0.1256]],\n",
      "\n",
      "         [[-0.1593, -0.1633, -0.0011],\n",
      "          [ 0.0072,  0.0455, -0.0941],\n",
      "          [-0.0620, -0.1344, -0.1229]],\n",
      "\n",
      "         [[ 0.0810, -0.1419, -0.0080],\n",
      "          [ 0.0387, -0.0982, -0.0490],\n",
      "          [ 0.0565,  0.0350,  0.0597]],\n",
      "\n",
      "         [[-0.0528, -0.1506, -0.0785],\n",
      "          [ 0.0006,  0.1363,  0.0784],\n",
      "          [ 0.1635, -0.0515, -0.0914]]]])), ('conv2.bias', tensor([-0.1224, -0.0264, -0.0845,  0.1049, -0.0654, -0.1082])), ('linear1.weight', tensor([[-0.0206, -0.0641,  0.0034,  ..., -0.0243,  0.0473,  0.0330],\n",
      "        [-0.0453,  0.0414, -0.0348,  ..., -0.0590, -0.0140, -0.0303],\n",
      "        [-0.0419, -0.0392, -0.0187,  ...,  0.0619, -0.0015, -0.0582],\n",
      "        ...,\n",
      "        [-0.0277, -0.0165,  0.0008,  ...,  0.0579, -0.0196,  0.0251],\n",
      "        [-0.0230, -0.0579, -0.0182,  ...,  0.0242,  0.0454, -0.0305],\n",
      "        [-0.0319,  0.0070, -0.0127,  ..., -0.0471, -0.0182, -0.0036]])), ('linear1.bias', tensor([ 0.0148,  0.0232,  0.0260, -0.0172, -0.0278, -0.0074, -0.0153,  0.0482,\n",
      "         0.0332,  0.0598, -0.0679,  0.0283,  0.0190,  0.0652, -0.0353, -0.0567,\n",
      "        -0.0344, -0.0604, -0.0462, -0.0228, -0.0241, -0.0058, -0.0381, -0.0499,\n",
      "        -0.0255,  0.0149, -0.0261,  0.0613, -0.0596, -0.0328, -0.0372,  0.0116,\n",
      "        -0.0155, -0.0431, -0.0047, -0.0657, -0.0584,  0.0586, -0.0176,  0.0495,\n",
      "        -0.0392, -0.0527,  0.0135,  0.0306,  0.0526, -0.0646, -0.0352, -0.0057,\n",
      "         0.0491, -0.0117,  0.0041,  0.0073,  0.0295, -0.0032, -0.0441,  0.0658,\n",
      "         0.0078, -0.0582, -0.0069,  0.0142, -0.0590, -0.0581, -0.0426,  0.0145,\n",
      "         0.0123, -0.0156,  0.0068, -0.0500, -0.0118, -0.0031, -0.0171, -0.0520,\n",
      "        -0.0344, -0.0636,  0.0012, -0.0160, -0.0130,  0.0193, -0.0512,  0.0130,\n",
      "         0.0282, -0.0528,  0.0164,  0.0013, -0.0158,  0.0563,  0.0439,  0.0200,\n",
      "        -0.0365,  0.0184, -0.0159, -0.0342, -0.0546,  0.0447, -0.0247, -0.0002,\n",
      "        -0.0290, -0.0269, -0.0039,  0.0014, -0.0659, -0.0663,  0.0332,  0.0120,\n",
      "         0.0221, -0.0477,  0.0280,  0.0414,  0.0560, -0.0559, -0.0516,  0.0181,\n",
      "        -0.0069,  0.0458, -0.0214, -0.0105,  0.0671,  0.0395,  0.0370, -0.0368,\n",
      "         0.0279,  0.0268,  0.0092, -0.0120,  0.0029, -0.0229, -0.0063, -0.0409,\n",
      "         0.0260, -0.0188,  0.0330,  0.0267,  0.0371,  0.0031, -0.0355,  0.0553,\n",
      "         0.0183, -0.0610, -0.0590, -0.0150,  0.0391, -0.0329,  0.0349, -0.0531,\n",
      "         0.0097, -0.0078,  0.0421,  0.0281, -0.0430,  0.0073,  0.0128, -0.0022,\n",
      "        -0.0566,  0.0185,  0.0343, -0.0058, -0.0504, -0.0644, -0.0614,  0.0600,\n",
      "        -0.0165,  0.0483, -0.0525, -0.0594, -0.0187,  0.0119,  0.0204, -0.0480,\n",
      "        -0.0233, -0.0394,  0.0240,  0.0550, -0.0389,  0.0311,  0.0506,  0.0101,\n",
      "        -0.0282,  0.0204,  0.0562,  0.0510, -0.0073, -0.0590,  0.0235,  0.0025,\n",
      "         0.0254, -0.0137, -0.0311,  0.0356, -0.0423, -0.0479, -0.0558, -0.0082,\n",
      "         0.0548, -0.0664, -0.0625,  0.0090, -0.0472, -0.0112, -0.0320,  0.0245,\n",
      "        -0.0029, -0.0486, -0.0540, -0.0576,  0.0672,  0.0187,  0.0186, -0.0639,\n",
      "        -0.0385,  0.0543,  0.0350, -0.0056,  0.0604, -0.0576,  0.0073, -0.0307,\n",
      "        -0.0395, -0.0469,  0.0071, -0.0314,  0.0094, -0.0433, -0.0628, -0.0679,\n",
      "         0.0639, -0.0641,  0.0351, -0.0634, -0.0624,  0.0016, -0.0668, -0.0475,\n",
      "         0.0074,  0.0403,  0.0399,  0.0379,  0.0409, -0.0099,  0.0402,  0.0446,\n",
      "         0.0572,  0.0134,  0.0097,  0.0375, -0.0279, -0.0515,  0.0461,  0.0314,\n",
      "         0.0381,  0.0388, -0.0522, -0.0443,  0.0565,  0.0091, -0.0038, -0.0484,\n",
      "         0.0505, -0.0130,  0.0039,  0.0194])), ('linear2.weight', tensor([[ 0.0124,  0.0055, -0.0209,  ..., -0.0362,  0.0074, -0.0276],\n",
      "        [ 0.0121,  0.0601,  0.0265,  ..., -0.0538,  0.0411, -0.0039],\n",
      "        [ 0.0559,  0.0324,  0.0414,  ..., -0.0306,  0.0048, -0.0427],\n",
      "        ...,\n",
      "        [-0.0013, -0.0318, -0.0039,  ..., -0.0016, -0.0054,  0.0280],\n",
      "        [ 0.0394, -0.0270, -0.0293,  ..., -0.0117, -0.0044,  0.0567],\n",
      "        [-0.0315, -0.0259,  0.0557,  ..., -0.0147,  0.0472, -0.0079]])), ('linear2.bias', tensor([ 0.0182,  0.0011,  0.0441, -0.0100, -0.0603,  0.0026, -0.0383,  0.0470,\n",
      "        -0.0033, -0.0465,  0.0011,  0.0402,  0.0268,  0.0232,  0.0151,  0.0205,\n",
      "        -0.0291,  0.0003,  0.0591, -0.0351,  0.0351, -0.0219, -0.0260,  0.0504,\n",
      "        -0.0424, -0.0536, -0.0470,  0.0440,  0.0353,  0.0111,  0.0394,  0.0318,\n",
      "        -0.0059, -0.0424,  0.0611,  0.0047, -0.0300,  0.0347, -0.0186, -0.0309,\n",
      "         0.0268,  0.0377,  0.0153, -0.0134, -0.0204,  0.0373, -0.0040, -0.0403,\n",
      "         0.0061, -0.0045, -0.0542, -0.0169, -0.0041,  0.0367, -0.0415, -0.0464,\n",
      "         0.0538,  0.0521,  0.0602, -0.0401, -0.0298, -0.0600, -0.0606, -0.0158,\n",
      "         0.0151, -0.0113, -0.0267, -0.0014,  0.0586, -0.0200, -0.0247,  0.0470,\n",
      "         0.0620, -0.0078,  0.0185,  0.0437,  0.0309,  0.0165, -0.0409, -0.0619,\n",
      "         0.0438,  0.0088, -0.0403, -0.0504,  0.0056, -0.0300,  0.0400, -0.0207,\n",
      "        -0.0289, -0.0310,  0.0540, -0.0611, -0.0250,  0.0609, -0.0117, -0.0117,\n",
      "         0.0236,  0.0178,  0.0336, -0.0340, -0.0144, -0.0125, -0.0164,  0.0326,\n",
      "        -0.0449,  0.0394, -0.0314, -0.0156, -0.0122, -0.0377, -0.0410, -0.0355,\n",
      "         0.0183, -0.0277, -0.0489,  0.0596,  0.0430,  0.0321, -0.0320, -0.0260,\n",
      "        -0.0370,  0.0440, -0.0152, -0.0443,  0.0471, -0.0105, -0.0544, -0.0131,\n",
      "         0.0543, -0.0534, -0.0060, -0.0414, -0.0168,  0.0609, -0.0369, -0.0014,\n",
      "        -0.0546,  0.0244,  0.0222, -0.0390, -0.0386,  0.0146, -0.0258, -0.0462,\n",
      "        -0.0005, -0.0210,  0.0430,  0.0159, -0.0131, -0.0525, -0.0224,  0.0107,\n",
      "         0.0347, -0.0272, -0.0119,  0.0070, -0.0479, -0.0294, -0.0326,  0.0178])), ('linear3.weight', tensor([[ 0.0765,  0.0095,  0.0199,  ...,  0.0642, -0.0430,  0.0373],\n",
      "        [ 0.0082, -0.0006, -0.0244,  ..., -0.0035,  0.0138,  0.0571],\n",
      "        [ 0.0031,  0.0721,  0.0004,  ...,  0.0366, -0.0609, -0.0017],\n",
      "        ...,\n",
      "        [ 0.0498,  0.0092,  0.0020,  ...,  0.0388,  0.0301,  0.0317],\n",
      "        [ 0.0228, -0.0107,  0.0357,  ...,  0.0064,  0.0208,  0.0161],\n",
      "        [ 0.0763, -0.0252,  0.0699,  ...,  0.0393,  0.0048,  0.0011]])), ('linear3.bias', tensor([-0.0009, -0.0156,  0.0297, -0.0279,  0.0658,  0.0657,  0.0239, -0.0635,\n",
      "        -0.0557,  0.0611]))])\n"
     ]
    }
   ],
   "source": [
    "net_a2_hf = NetA2(10)\n",
    "net_a2_ht = NetA2(10)\n",
    "net_a2_dt = NetA2(10)\n",
    "\n",
    "#set conv1 initialization of net_a2_hf\n",
    "net_a2_hf.conv1.weight = nn.Parameter(initialization_weights)\n",
    "net_a2_hf.conv1.bias = nn.Parameter(initialization_biases)\n",
    "\n",
    "# set same weights and bias to each layer of each network (except for cov1 of net_a1_dt)\n",
    "net_a2_ht.load_state_dict(net_a2_hf.state_dict())\n",
    "net_a2_dt.load_state_dict(net_a2_hf.state_dict())\n",
    "\n",
    "#set conv1 initialization\n",
    "net_a2_dt.conv1.load_state_dict(net_a1_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a2_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA2HF_init.pt')\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA2HT_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA2DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a2_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a2_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a2_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.539336Z",
     "start_time": "2024-06-14T06:11:16.497132Z"
    }
   },
   "id": "5945b11a44c08847",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Analysys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e527a7ee28f832"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1: \n",
      " \t|W_{conv_a1_hf} - W_{conv_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A2: \n",
      " \t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A1 Vs Net_A2: \n",
      " \t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| = tensor(0.) \n",
      " \t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n"
     ]
    }
   ],
   "source": [
    "print( \"Net_A1: \\n\",\n",
    "       \"\\t|W_{conv_a1_hf} - W_{conv_a1_ht}| =\", torch.norm(net_a1_hf.conv1.weight - net_a1_ht.conv1.weight),\"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_ht}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_ht.linear1.weight), \"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_dt}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_dt.linear1.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| =\", torch.norm(net_a2_hf.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| =\", torch.norm(net_a2_hf.conv2.weight - net_a2_ht.conv2.weight),\"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_ht.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_dt.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_ht.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_dt.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_ht.linear3.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_dt.linear3.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A1 Vs Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| =\", torch.norm(net_a1_hf.conv1.weight - net_a2_hf.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| =\", torch.norm(net_a1_ht.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| =\", torch.norm(net_a1_dt.conv1.weight - net_a2_dt.conv1.weight),\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.556925Z",
     "start_time": "2024-06-14T06:11:16.541351Z"
    }
   },
   "id": "eabba971baa9ea0d",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9884bc78c276b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.609515Z",
     "start_time": "2024-06-14T06:11:16.558934Z"
    }
   },
   "id": "e8c282a53727943",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "image, label = train_data[sample_idx]\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.620986Z",
     "start_time": "2024-06-14T06:11:16.612523Z"
    }
   },
   "id": "84fa5b26a60379de",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader= DataLoader(train_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.628298Z",
     "start_time": "2024-06-14T06:11:16.622996Z"
    }
   },
   "id": "be585d7eb8bcae35",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training/Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238fefd45b206a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "      size = len(dataloader.dataset)\n",
    "      num_batches = len(dataloader)\n",
    "      test_loss, correct = 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          pred = model(X)\n",
    "          test_loss += loss_fn(pred, y).item()\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      test_loss /= num_batches\n",
    "      correct /= size\n",
    "      print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "      return 100*correct, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.638551Z",
     "start_time": "2024-06-14T06:11:16.630307Z"
    }
   },
   "id": "1b06878e4ef1f642",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79254d3d93db89e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:16.645038Z",
     "start_time": "2024-06-14T06:11:16.640563Z"
    }
   },
   "id": "b437c40fa560695f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_test(device, train_dataloader, test_dataloader, net, learning_rate, epochs):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, net, loss_fn, optimizer)\n",
    "        acc, loss = test_loop(device, test_dataloader, net, loss_fn)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": [ i for i in range(epochs)],\n",
    "            \"loss\": losses,\n",
    "            \"accuracy\": accuracies\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:11:20.524977Z",
     "start_time": "2024-06-14T06:11:20.517921Z"
    }
   },
   "id": "a166ce4ccf0cb4a4",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1 -> HF Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcdec5977207972"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[ 0.0104, -0.0263, -0.0227,  ..., -0.0365,  0.0185,  0.0133],\n",
      "        [-0.0147, -0.0146,  0.0140,  ..., -0.0009,  0.0057, -0.0075],\n",
      "        [-0.0164, -0.0220,  0.0229,  ...,  0.0109,  0.0119,  0.0116],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0225, -0.0024,  ..., -0.0003, -0.0223, -0.0032],\n",
      "        [ 0.0315,  0.0013,  0.0172,  ...,  0.0241,  0.0051,  0.0278],\n",
      "        [ 0.0120,  0.0103, -0.0237,  ..., -0.0320, -0.0291, -0.0278]])), ('linear1.bias', tensor([-0.0365,  0.0243, -0.0267, -0.0056, -0.0325,  0.0328,  0.0208,  0.0131,\n",
      "        -0.0117,  0.0210]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303091  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 2.250945 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.249686  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.246645 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.245772  [  128/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(net_a1_hf\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[1;32m----> 2\u001B[0m df_net_a1_hf \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet_a1_hf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m df_net_a1_hf\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNetA1HF_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m net_a1_hf\u001B[38;5;241m.\u001B[39mstate_dict()\n",
      "Cell \u001B[1;32mIn[14], line 10\u001B[0m, in \u001B[0;36mtrain_test\u001B[1;34m(device, train_dataloader, test_dataloader, net, learning_rate, epochs)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m     \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     acc, loss \u001B[38;5;241m=\u001B[39m test_loop(device, test_dataloader, net, loss_fn)\n\u001B[0;32m     12\u001B[0m     accuracies\u001B[38;5;241m.\u001B[39mappend(acc)\n",
      "Cell \u001B[1;32mIn[12], line 3\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(device, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_loop\u001B[39m(device, dataloader, model, loss_fn, optimizer):\n\u001B[0;32m      2\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[1;32m----> 3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Compute prediction and loss\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\transforms\\functional.py:175\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    173\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[1;32m--> 175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(net_a1_hf.state_dict())\n",
    "df_net_a1_hf = train_test(device, train_dataloader, test_dataloader, net_a1_hf, learning_rate, epochs)\n",
    "df_net_a1_hf.to_csv('NetA1HF_results.csv', index=False)\n",
    "net_a1_hf.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:10:49.659683Z",
     "start_time": "2024-06-14T06:10:35.233659Z"
    }
   },
   "id": "98b5cb92a0191d1e",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> HT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae1a1c117cd5c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a1_ht.state_dict())\n",
    "df_net_a1_ht = train_test(device, train_dataloader, test_dataloader, net_a1_ht, learning_rate, epochs)\n",
    "df_net_a1_ht.to_csv('NetA1HT_results.csv', index=False)\n",
    "net_a1_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T06:10:49.661730Z"
    }
   },
   "id": "57d9aed7097ab0b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> DT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e7c98ba5a28137"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[-0.1554,  0.0492,  0.3141],\n",
      "          [-0.1258, -0.3250, -0.0167],\n",
      "          [ 0.0201, -0.1168,  0.3246]]],\n",
      "\n",
      "\n",
      "        [[[-0.0118,  0.2718, -0.2517],\n",
      "          [-0.0388, -0.2801,  0.1957],\n",
      "          [ 0.1790,  0.2293,  0.0635]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1669,  0.1119,  0.1287],\n",
      "          [ 0.0206, -0.3305, -0.0627],\n",
      "          [ 0.2295, -0.1768,  0.1198]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1811, -0.2077, -0.1208],\n",
      "          [ 0.0919, -0.1740,  0.1239],\n",
      "          [-0.3191, -0.0752, -0.1332]]]])), ('conv1.bias', tensor([-0.1342,  0.2487,  0.1029, -0.3292])), ('linear1.weight', tensor([[-0.0285,  0.0300,  0.0261,  ...,  0.0316, -0.0054, -0.0235],\n",
      "        [-0.0001,  0.0382, -0.0200,  ...,  0.0139, -0.0020,  0.0334],\n",
      "        [-0.0203, -0.0349,  0.0247,  ..., -0.0347,  0.0140, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0161,  0.0051,  ..., -0.0248,  0.0294, -0.0144],\n",
      "        [ 0.0372,  0.0311, -0.0312,  ..., -0.0147,  0.0037,  0.0027],\n",
      "        [-0.0196,  0.0066, -0.0079,  ...,  0.0210, -0.0169,  0.0009]])), ('linear1.bias', tensor([ 0.0192,  0.0374,  0.0258, -0.0071,  0.0304, -0.0120, -0.0228,  0.0204,\n",
      "        -0.0246, -0.0212]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302592  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 2.279576 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.281748  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.270810 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.273932  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 2.264883 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.267241  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.261115 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.262058  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 2.260138 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.259986  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 2.259513 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.258599  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 2.259004 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.257759  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.258550 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.257245  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 2.258142 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.256811  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 2.257786 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.256505  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 2.257476 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.256299  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.257212 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.256160  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.256993 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.256057  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.256818 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.255973  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.256676 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.255897  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.256557 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.255829  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.256455 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.255768  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.256367 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.255713  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.256290 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.255664  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.256221 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.255621  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.256157 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.255585  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.256097 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.255556  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.256041 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.255534  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.255988 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.255517  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.255939 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.255503  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.255891 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.255493  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.255845 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.255485  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.255801 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.255479  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.255758 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.255474  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.255716 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.255470  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.255675 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.255467  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.255635 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.255465  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.255597 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.255464  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.255561 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.255464  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255527 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.255464  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255494 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.255465  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255464 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.255466  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255436 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.255468  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255410 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.255470  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255385 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.255472  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255362 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.255474  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255339 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.255476  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255318 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.255477  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.255297 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.255478  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255276 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.255480  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255257 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.255480  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.255237 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.255480  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.255218 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.255480  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.255199 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.255479  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.255180 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-0.0564,  0.3738,  1.0812],\n                        [ 0.1055,  0.0993,  0.9112],\n                        [ 0.2583,  0.3009,  1.1307]]],\n              \n              \n                      [[[ 0.1899,  0.5847, -0.1145],\n                        [ 0.2957,  0.2213,  0.6387],\n                        [ 0.5168,  0.6410,  0.4810]]],\n              \n              \n                      [[[ 0.7863,  0.4414,  0.6641],\n                        [ 0.8376,  0.0954,  0.6608],\n                        [ 0.7549, -0.0214,  0.6964]]],\n              \n              \n                      [[[ 0.1811, -0.2077, -0.1208],\n                        [ 0.0919, -0.1740,  0.1239],\n                        [-0.3191, -0.0752, -0.1332]]]])),\n             ('conv1.bias',\n              tensor([-6.8523e-02,  1.0596e+00,  4.8831e-04, -3.2920e-01])),\n             ('linear1.weight',\n              tensor([[ 0.0663, -0.0055, -0.0764,  ...,  0.0316, -0.0054, -0.0235],\n                      [-0.0267, -0.1301, -0.3792,  ...,  0.0139, -0.0020,  0.0334],\n                      [-0.0203, -0.0349,  0.0247,  ..., -0.0347,  0.0140, -0.0029],\n                      ...,\n                      [ 0.0017, -0.0174,  0.0037,  ..., -0.0248,  0.0294, -0.0144],\n                      [ 0.0372,  0.0311, -0.0312,  ..., -0.0147,  0.0037,  0.0027],\n                      [ 0.0048, -0.0151, -0.0818,  ...,  0.0210, -0.0169,  0.0009]])),\n             ('linear1.bias',\n              tensor([ 0.2174,  0.2776,  0.0210,  0.2257,  0.0722,  0.3418,  0.8606,  0.0188,\n                      -0.0266,  0.4492]))])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_dt.state_dict())\n",
    "df_net_a1_dt = train_test(device, train_dataloader, test_dataloader, net_a1_dt, learning_rate, epochs)\n",
    "df_net_a1_dt.to_csv('NetA1DT_results.csv', index=False)\n",
    "net_a1_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T06:18:19.465308Z",
     "start_time": "2024-06-14T06:11:35.817658Z"
    }
   },
   "id": "5fc9595e6f55df0b",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HF Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aabaf6906661353"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_hf.state_dict())\n",
    "df_net_a2_hf = train_test(device, train_dataloader, test_dataloader, net_a2_hf, learning_rate, epochs)\n",
    "df_net_a2_hf.to_csv('NetA2HF_results.csv', index=False)\n",
    "net_a2_hf.state_dict() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T06:10:49.664741Z"
    }
   },
   "id": "f0485488dfe86d3f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c29cd600b2295f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_ht.state_dict())\n",
    "df_net_a2_ht = train_test(device, train_dataloader, test_dataloader, net_a2_ht, learning_rate, epochs)\n",
    "df_net_a2_ht.to_csv('NetA2HT_results.csv', index=False)\n",
    "net_a2_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc170d6165469498",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> DT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ae93a49f80a7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_dt.state_dict())\n",
    "df_net_a2_dt = train_test(device, train_dataloader, test_dataloader, net_a2_dt, learning_rate, epochs)\n",
    "df_net_a2_dt.to_csv('NetA2DT_results.csv', index=False)\n",
    "net_a2_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T06:10:49.667740Z"
    }
   },
   "id": "d2e02ef68d3a9f05",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
