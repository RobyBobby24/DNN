{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.models.optical_flow.raft import ResidualBlock\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.210570Z",
     "start_time": "2024-06-18T10:12:18.725382Z"
    }
   },
   "id": "685462a85f2ae1e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.216169Z",
     "start_time": "2024-06-18T10:12:22.212578Z"
    }
   },
   "id": "8fb89e7978c49f98",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bdd2924b375a67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA1(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(576, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "            \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.228477Z",
     "start_time": "2024-06-18T10:12:22.217237Z"
    }
   },
   "id": "8e067a677d103ee6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetA2(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(NetA2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=12, kernel_size=3, stride=2)\n",
    "        self.flatten = nn.Flatten(start_dim=-3)\n",
    "        self.linear1 = nn.Linear(300, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def freeze(self, layer: str):\n",
    "        for param in getattr(self, layer).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.237919Z",
     "start_time": "2024-06-18T10:12:22.230541Z"
    }
   },
   "id": "4f398a6ac2c01f29",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99360d229c0dbc75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1, 5, 5])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialization_weights = torch.tensor([\n",
    "    [[\n",
    "        [1, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 1, 0], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [0, 1, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 1]\n",
    "    ]],\n",
    "    [[\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [0, 0, 1, 0, 0]\n",
    "    ]],\n",
    "    [[\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [1, 0, 0, 0, 1], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [0, 1, 1, 1, 0]\n",
    "    ]],\n",
    "    [[\n",
    "        [1, 1, 0, 1, 1], \n",
    "        [0, 1, 0, 1, 0], \n",
    "        [0, 0, 1, 0, 0], \n",
    "        [1, 1, 0, 1, 1],\n",
    "        [1, 1, 0, 1, 1]\n",
    "    ]]], dtype=torch.float32)\n",
    "\n",
    "initialization_biases = torch.tensor([0,0,0,0], dtype=torch.float32)\n",
    "initialization_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.250639Z",
     "start_time": "2024-06-18T10:12:22.238982Z"
    }
   },
   "id": "3e5ae64e8c459822",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.0061, -0.1554, -0.0142,  0.0578, -0.0099],\n",
      "          [ 0.1869,  0.1770, -0.1435,  0.1801,  0.1628],\n",
      "          [ 0.1702, -0.0947,  0.1717, -0.0842,  0.1247],\n",
      "          [ 0.0238,  0.0544,  0.0687,  0.1856,  0.0196],\n",
      "          [ 0.0523,  0.0625,  0.0194,  0.0104,  0.1586]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1630,  0.0413, -0.0511, -0.1957, -0.0891],\n",
      "          [-0.1879,  0.0403, -0.1168, -0.0925,  0.1643],\n",
      "          [ 0.0493,  0.1674,  0.1800,  0.1824,  0.0387],\n",
      "          [-0.0117, -0.1641,  0.0288, -0.1827, -0.0479],\n",
      "          [ 0.0218, -0.0633, -0.0475, -0.1878,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1177, -0.1837,  0.1865, -0.0738, -0.0184],\n",
      "          [ 0.0237,  0.1294,  0.1114,  0.1400,  0.0315],\n",
      "          [ 0.0940, -0.0617, -0.0338, -0.0947, -0.1626],\n",
      "          [ 0.0766,  0.0023,  0.0311,  0.1917,  0.1184],\n",
      "          [ 0.1952,  0.1259, -0.0296, -0.1338,  0.0884]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0724,  0.0605, -0.1065, -0.0611, -0.0419],\n",
      "          [-0.1390, -0.0909,  0.0681, -0.1637,  0.0339],\n",
      "          [-0.1531,  0.1662, -0.1162,  0.1599, -0.0344],\n",
      "          [-0.1465, -0.0736, -0.0137,  0.1574, -0.0215],\n",
      "          [ 0.1790, -0.0872,  0.1049,  0.0193,  0.0299]]]])), ('conv1.bias', tensor([ 0.1321, -0.0655, -0.0929,  0.0429])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n"
     ]
    }
   ],
   "source": [
    "net_a1_hf = NetA1(10)\n",
    "net_a1_ht = NetA1(10)\n",
    "net_a1_dt = NetA1(10)\n",
    "\n",
    "#set conv1 initialization of net_a1_hf\n",
    "net_a1_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a1_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a1_ht.load_state_dict(net_a1_hf.state_dict())\n",
    "for name, param in net_a1_hf.state_dict().items():\n",
    "    if \"conv1\" not in name:\n",
    "        net_a1_dt.state_dict()[name].copy_(param)\n",
    "\n",
    "#set conv1 initialization\n",
    "#net_a1_dt.conv1.load_state_dict(net_a2_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a1_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_init.pt')\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_init.pt')\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a1_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a1_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a1_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.274912Z",
     "start_time": "2024-06-18T10:12:22.251705Z"
    }
   },
   "id": "2d60961b8d0a8bc7",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1_HF: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1518, -0.0377,  0.1249],\n",
      "          [-0.0100, -0.1487,  0.0391],\n",
      "          [-0.1228,  0.1132, -0.0737]],\n",
      "\n",
      "         [[-0.0040, -0.0954, -0.1017],\n",
      "          [ 0.1362,  0.0255, -0.1470],\n",
      "          [-0.0381, -0.0117,  0.0466]],\n",
      "\n",
      "         [[ 0.0319,  0.0124, -0.1047],\n",
      "          [-0.0964, -0.0315,  0.0061],\n",
      "          [-0.0346,  0.0237,  0.0561]],\n",
      "\n",
      "         [[-0.1640,  0.0097,  0.1342],\n",
      "          [ 0.0255, -0.0511, -0.0716],\n",
      "          [-0.1100,  0.0205,  0.0216]]],\n",
      "\n",
      "\n",
      "        [[[-0.0389,  0.0102,  0.0679],\n",
      "          [ 0.0359,  0.0830, -0.1491],\n",
      "          [ 0.1260, -0.1300, -0.0153]],\n",
      "\n",
      "         [[-0.0946, -0.0965, -0.1040],\n",
      "          [ 0.0090,  0.0904,  0.1590],\n",
      "          [ 0.1180, -0.1597,  0.1555]],\n",
      "\n",
      "         [[-0.0260,  0.0207, -0.0976],\n",
      "          [-0.0525,  0.0711, -0.0511],\n",
      "          [-0.0899,  0.0119,  0.0755]],\n",
      "\n",
      "         [[ 0.0441,  0.1637, -0.1303],\n",
      "          [ 0.1034,  0.0384, -0.1419],\n",
      "          [ 0.1600,  0.0319, -0.1161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0798,  0.0988,  0.0132],\n",
      "          [ 0.0810, -0.1636,  0.0996],\n",
      "          [-0.0379,  0.0433,  0.1582]],\n",
      "\n",
      "         [[-0.1436, -0.1092, -0.0718],\n",
      "          [ 0.0768, -0.0347,  0.1154],\n",
      "          [-0.0850,  0.1057,  0.1478]],\n",
      "\n",
      "         [[ 0.0451, -0.1337,  0.0361],\n",
      "          [ 0.1170,  0.0857, -0.0100],\n",
      "          [-0.1445,  0.1088, -0.0726]],\n",
      "\n",
      "         [[-0.1381,  0.0773,  0.1400],\n",
      "          [ 0.1583,  0.0366, -0.1547],\n",
      "          [-0.0305,  0.0773,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1513, -0.0850, -0.0475],\n",
      "          [ 0.0019, -0.0261,  0.0249],\n",
      "          [ 0.0756,  0.0299,  0.1320]],\n",
      "\n",
      "         [[-0.1435,  0.0879,  0.1333],\n",
      "          [-0.1408, -0.1238, -0.1189],\n",
      "          [ 0.1580,  0.0445,  0.1471]],\n",
      "\n",
      "         [[-0.1446, -0.1364, -0.1047],\n",
      "          [-0.0881, -0.0236, -0.0295],\n",
      "          [-0.0934,  0.0408,  0.0733]],\n",
      "\n",
      "         [[-0.0991, -0.0325, -0.0805],\n",
      "          [ 0.0139, -0.1579, -0.1488],\n",
      "          [ 0.0398, -0.1449, -0.0617]]],\n",
      "\n",
      "\n",
      "        [[[-0.1090,  0.1399, -0.1253],\n",
      "          [-0.1549,  0.1320,  0.1445],\n",
      "          [ 0.0180, -0.0368, -0.0870]],\n",
      "\n",
      "         [[ 0.1379,  0.0878,  0.0761],\n",
      "          [ 0.1032, -0.0537,  0.1039],\n",
      "          [ 0.1325,  0.0757,  0.0921]],\n",
      "\n",
      "         [[-0.1363,  0.1648, -0.0822],\n",
      "          [-0.1281, -0.0285,  0.0003],\n",
      "          [-0.0942,  0.0503, -0.0420]],\n",
      "\n",
      "         [[ 0.0194, -0.0237, -0.1151],\n",
      "          [ 0.1521, -0.1559,  0.0093],\n",
      "          [ 0.0274, -0.0812, -0.1523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0311,  0.0711,  0.0515],\n",
      "          [-0.1309,  0.1466,  0.0431],\n",
      "          [ 0.0786, -0.0175,  0.1469]],\n",
      "\n",
      "         [[ 0.1227,  0.1405,  0.1267],\n",
      "          [ 0.1657, -0.0699,  0.0274],\n",
      "          [ 0.1185, -0.1070, -0.0855]],\n",
      "\n",
      "         [[ 0.0275, -0.0293, -0.0995],\n",
      "          [ 0.1093, -0.1078,  0.1532],\n",
      "          [ 0.0171,  0.0958, -0.1060]],\n",
      "\n",
      "         [[ 0.1013,  0.0228,  0.1601],\n",
      "          [ 0.0962,  0.1461,  0.0093],\n",
      "          [ 0.1347,  0.1429, -0.1522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0859, -0.1321,  0.1002],\n",
      "          [ 0.1038,  0.1370,  0.1242],\n",
      "          [-0.1526,  0.1455,  0.1296]],\n",
      "\n",
      "         [[-0.1333,  0.1242,  0.0865],\n",
      "          [ 0.0887,  0.0154,  0.0190],\n",
      "          [-0.0599, -0.1358,  0.0817]],\n",
      "\n",
      "         [[-0.1122,  0.0853, -0.0671],\n",
      "          [ 0.1066,  0.0040, -0.0420],\n",
      "          [ 0.0128, -0.1329,  0.0146]],\n",
      "\n",
      "         [[ 0.1168, -0.0891,  0.0562],\n",
      "          [ 0.1476, -0.0054,  0.0818],\n",
      "          [-0.0672, -0.1401, -0.1643]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893,  0.0464,  0.1150],\n",
      "          [ 0.0311,  0.1285,  0.0288],\n",
      "          [-0.0565, -0.0942,  0.0682]],\n",
      "\n",
      "         [[-0.0023,  0.0231,  0.0418],\n",
      "          [ 0.1206, -0.1640,  0.0080],\n",
      "          [-0.0236,  0.0674,  0.1320]],\n",
      "\n",
      "         [[-0.1453,  0.1468, -0.0836],\n",
      "          [-0.1412,  0.0860, -0.1551],\n",
      "          [ 0.1181,  0.0454,  0.1532]],\n",
      "\n",
      "         [[-0.0770,  0.0083, -0.0158],\n",
      "          [-0.0284,  0.0335, -0.0067],\n",
      "          [-0.1286,  0.1629,  0.0188]]],\n",
      "\n",
      "\n",
      "        [[[-0.1556, -0.1562,  0.0559],\n",
      "          [ 0.0663, -0.0118,  0.0820],\n",
      "          [-0.1099, -0.0277,  0.1154]],\n",
      "\n",
      "         [[-0.1092,  0.0893, -0.0582],\n",
      "          [ 0.0080,  0.1640,  0.0887],\n",
      "          [ 0.1373,  0.1052, -0.0953]],\n",
      "\n",
      "         [[ 0.0175,  0.1626, -0.0476],\n",
      "          [ 0.0459,  0.0813, -0.0941],\n",
      "          [ 0.1165,  0.0350, -0.1572]],\n",
      "\n",
      "         [[ 0.0493, -0.1505, -0.0528],\n",
      "          [ 0.0054,  0.1412, -0.0423],\n",
      "          [-0.0360,  0.1500,  0.1583]]],\n",
      "\n",
      "\n",
      "        [[[-0.0693,  0.0622,  0.1566],\n",
      "          [-0.0018, -0.1420,  0.0784],\n",
      "          [-0.0739, -0.0408,  0.1492]],\n",
      "\n",
      "         [[-0.1150,  0.1161,  0.0402],\n",
      "          [ 0.1274,  0.0941,  0.0629],\n",
      "          [ 0.0488,  0.0396,  0.1660]],\n",
      "\n",
      "         [[-0.1071,  0.0956, -0.1135],\n",
      "          [ 0.0225, -0.1317, -0.0448],\n",
      "          [ 0.0724, -0.1254,  0.0306]],\n",
      "\n",
      "         [[ 0.0681, -0.1408, -0.0381],\n",
      "          [ 0.1591,  0.0394, -0.0787],\n",
      "          [ 0.0755,  0.1179, -0.1603]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.1457, -0.0071],\n",
      "          [-0.1274, -0.1198, -0.0278],\n",
      "          [ 0.0968,  0.0864,  0.0325]],\n",
      "\n",
      "         [[-0.1607, -0.0931, -0.0062],\n",
      "          [ 0.0359, -0.1553,  0.0473],\n",
      "          [-0.1180, -0.0693,  0.1443]],\n",
      "\n",
      "         [[-0.0235, -0.1033, -0.1291],\n",
      "          [ 0.0615,  0.0980,  0.0757],\n",
      "          [-0.0489, -0.0668, -0.0824]],\n",
      "\n",
      "         [[ 0.0249,  0.0799,  0.1139],\n",
      "          [ 0.1360, -0.0858, -0.0431],\n",
      "          [-0.1233, -0.0465,  0.1503]]],\n",
      "\n",
      "\n",
      "        [[[-0.1586,  0.0867, -0.0577],\n",
      "          [ 0.1389,  0.0589, -0.1383],\n",
      "          [ 0.0326, -0.1345,  0.1604]],\n",
      "\n",
      "         [[-0.0664, -0.1348, -0.0349],\n",
      "          [-0.0002, -0.0256, -0.0162],\n",
      "          [ 0.1602,  0.1361,  0.0357]],\n",
      "\n",
      "         [[-0.0229, -0.0997,  0.1227],\n",
      "          [ 0.0545,  0.0696,  0.0240],\n",
      "          [ 0.0757,  0.0066,  0.0005]],\n",
      "\n",
      "         [[-0.1605, -0.0713, -0.0299],\n",
      "          [-0.0843,  0.0250, -0.0971],\n",
      "          [-0.1280,  0.1464, -0.1098]]]])), ('conv2.bias', tensor([ 0.1457,  0.1590,  0.1079,  0.0935, -0.1109, -0.0072, -0.0451,  0.0436,\n",
      "         0.1550,  0.0383,  0.1300,  0.0760])), ('linear1.weight', tensor([[ 1.5908e-02,  1.5656e-02,  4.6909e-02,  ..., -2.4763e-02,\n",
      "         -4.5605e-02, -6.0834e-06],\n",
      "        [ 3.8864e-02, -2.0766e-02,  5.0544e-02,  ...,  4.2443e-02,\n",
      "         -3.7946e-02, -1.1646e-02],\n",
      "        [-4.3416e-02, -2.7498e-02,  3.2318e-02,  ...,  4.7365e-02,\n",
      "          2.3062e-02,  2.8593e-02],\n",
      "        ...,\n",
      "        [ 1.8522e-02, -1.9695e-02, -5.4902e-02,  ...,  1.1535e-02,\n",
      "         -9.5615e-03,  2.9643e-02],\n",
      "        [ 2.4093e-02,  1.4357e-02, -3.5899e-02,  ..., -5.4755e-02,\n",
      "          4.6055e-02,  4.1239e-02],\n",
      "        [ 5.1365e-02, -3.0765e-02,  2.6570e-02,  ..., -1.2703e-02,\n",
      "          1.7119e-02, -2.4224e-02]])), ('linear1.bias', tensor([ 0.0252,  0.0300,  0.0190, -0.0107,  0.0018, -0.0247,  0.0454, -0.0023,\n",
      "         0.0309, -0.0527]))])\n",
      "Net_A1_HT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1518, -0.0377,  0.1249],\n",
      "          [-0.0100, -0.1487,  0.0391],\n",
      "          [-0.1228,  0.1132, -0.0737]],\n",
      "\n",
      "         [[-0.0040, -0.0954, -0.1017],\n",
      "          [ 0.1362,  0.0255, -0.1470],\n",
      "          [-0.0381, -0.0117,  0.0466]],\n",
      "\n",
      "         [[ 0.0319,  0.0124, -0.1047],\n",
      "          [-0.0964, -0.0315,  0.0061],\n",
      "          [-0.0346,  0.0237,  0.0561]],\n",
      "\n",
      "         [[-0.1640,  0.0097,  0.1342],\n",
      "          [ 0.0255, -0.0511, -0.0716],\n",
      "          [-0.1100,  0.0205,  0.0216]]],\n",
      "\n",
      "\n",
      "        [[[-0.0389,  0.0102,  0.0679],\n",
      "          [ 0.0359,  0.0830, -0.1491],\n",
      "          [ 0.1260, -0.1300, -0.0153]],\n",
      "\n",
      "         [[-0.0946, -0.0965, -0.1040],\n",
      "          [ 0.0090,  0.0904,  0.1590],\n",
      "          [ 0.1180, -0.1597,  0.1555]],\n",
      "\n",
      "         [[-0.0260,  0.0207, -0.0976],\n",
      "          [-0.0525,  0.0711, -0.0511],\n",
      "          [-0.0899,  0.0119,  0.0755]],\n",
      "\n",
      "         [[ 0.0441,  0.1637, -0.1303],\n",
      "          [ 0.1034,  0.0384, -0.1419],\n",
      "          [ 0.1600,  0.0319, -0.1161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0798,  0.0988,  0.0132],\n",
      "          [ 0.0810, -0.1636,  0.0996],\n",
      "          [-0.0379,  0.0433,  0.1582]],\n",
      "\n",
      "         [[-0.1436, -0.1092, -0.0718],\n",
      "          [ 0.0768, -0.0347,  0.1154],\n",
      "          [-0.0850,  0.1057,  0.1478]],\n",
      "\n",
      "         [[ 0.0451, -0.1337,  0.0361],\n",
      "          [ 0.1170,  0.0857, -0.0100],\n",
      "          [-0.1445,  0.1088, -0.0726]],\n",
      "\n",
      "         [[-0.1381,  0.0773,  0.1400],\n",
      "          [ 0.1583,  0.0366, -0.1547],\n",
      "          [-0.0305,  0.0773,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1513, -0.0850, -0.0475],\n",
      "          [ 0.0019, -0.0261,  0.0249],\n",
      "          [ 0.0756,  0.0299,  0.1320]],\n",
      "\n",
      "         [[-0.1435,  0.0879,  0.1333],\n",
      "          [-0.1408, -0.1238, -0.1189],\n",
      "          [ 0.1580,  0.0445,  0.1471]],\n",
      "\n",
      "         [[-0.1446, -0.1364, -0.1047],\n",
      "          [-0.0881, -0.0236, -0.0295],\n",
      "          [-0.0934,  0.0408,  0.0733]],\n",
      "\n",
      "         [[-0.0991, -0.0325, -0.0805],\n",
      "          [ 0.0139, -0.1579, -0.1488],\n",
      "          [ 0.0398, -0.1449, -0.0617]]],\n",
      "\n",
      "\n",
      "        [[[-0.1090,  0.1399, -0.1253],\n",
      "          [-0.1549,  0.1320,  0.1445],\n",
      "          [ 0.0180, -0.0368, -0.0870]],\n",
      "\n",
      "         [[ 0.1379,  0.0878,  0.0761],\n",
      "          [ 0.1032, -0.0537,  0.1039],\n",
      "          [ 0.1325,  0.0757,  0.0921]],\n",
      "\n",
      "         [[-0.1363,  0.1648, -0.0822],\n",
      "          [-0.1281, -0.0285,  0.0003],\n",
      "          [-0.0942,  0.0503, -0.0420]],\n",
      "\n",
      "         [[ 0.0194, -0.0237, -0.1151],\n",
      "          [ 0.1521, -0.1559,  0.0093],\n",
      "          [ 0.0274, -0.0812, -0.1523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0311,  0.0711,  0.0515],\n",
      "          [-0.1309,  0.1466,  0.0431],\n",
      "          [ 0.0786, -0.0175,  0.1469]],\n",
      "\n",
      "         [[ 0.1227,  0.1405,  0.1267],\n",
      "          [ 0.1657, -0.0699,  0.0274],\n",
      "          [ 0.1185, -0.1070, -0.0855]],\n",
      "\n",
      "         [[ 0.0275, -0.0293, -0.0995],\n",
      "          [ 0.1093, -0.1078,  0.1532],\n",
      "          [ 0.0171,  0.0958, -0.1060]],\n",
      "\n",
      "         [[ 0.1013,  0.0228,  0.1601],\n",
      "          [ 0.0962,  0.1461,  0.0093],\n",
      "          [ 0.1347,  0.1429, -0.1522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0859, -0.1321,  0.1002],\n",
      "          [ 0.1038,  0.1370,  0.1242],\n",
      "          [-0.1526,  0.1455,  0.1296]],\n",
      "\n",
      "         [[-0.1333,  0.1242,  0.0865],\n",
      "          [ 0.0887,  0.0154,  0.0190],\n",
      "          [-0.0599, -0.1358,  0.0817]],\n",
      "\n",
      "         [[-0.1122,  0.0853, -0.0671],\n",
      "          [ 0.1066,  0.0040, -0.0420],\n",
      "          [ 0.0128, -0.1329,  0.0146]],\n",
      "\n",
      "         [[ 0.1168, -0.0891,  0.0562],\n",
      "          [ 0.1476, -0.0054,  0.0818],\n",
      "          [-0.0672, -0.1401, -0.1643]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893,  0.0464,  0.1150],\n",
      "          [ 0.0311,  0.1285,  0.0288],\n",
      "          [-0.0565, -0.0942,  0.0682]],\n",
      "\n",
      "         [[-0.0023,  0.0231,  0.0418],\n",
      "          [ 0.1206, -0.1640,  0.0080],\n",
      "          [-0.0236,  0.0674,  0.1320]],\n",
      "\n",
      "         [[-0.1453,  0.1468, -0.0836],\n",
      "          [-0.1412,  0.0860, -0.1551],\n",
      "          [ 0.1181,  0.0454,  0.1532]],\n",
      "\n",
      "         [[-0.0770,  0.0083, -0.0158],\n",
      "          [-0.0284,  0.0335, -0.0067],\n",
      "          [-0.1286,  0.1629,  0.0188]]],\n",
      "\n",
      "\n",
      "        [[[-0.1556, -0.1562,  0.0559],\n",
      "          [ 0.0663, -0.0118,  0.0820],\n",
      "          [-0.1099, -0.0277,  0.1154]],\n",
      "\n",
      "         [[-0.1092,  0.0893, -0.0582],\n",
      "          [ 0.0080,  0.1640,  0.0887],\n",
      "          [ 0.1373,  0.1052, -0.0953]],\n",
      "\n",
      "         [[ 0.0175,  0.1626, -0.0476],\n",
      "          [ 0.0459,  0.0813, -0.0941],\n",
      "          [ 0.1165,  0.0350, -0.1572]],\n",
      "\n",
      "         [[ 0.0493, -0.1505, -0.0528],\n",
      "          [ 0.0054,  0.1412, -0.0423],\n",
      "          [-0.0360,  0.1500,  0.1583]]],\n",
      "\n",
      "\n",
      "        [[[-0.0693,  0.0622,  0.1566],\n",
      "          [-0.0018, -0.1420,  0.0784],\n",
      "          [-0.0739, -0.0408,  0.1492]],\n",
      "\n",
      "         [[-0.1150,  0.1161,  0.0402],\n",
      "          [ 0.1274,  0.0941,  0.0629],\n",
      "          [ 0.0488,  0.0396,  0.1660]],\n",
      "\n",
      "         [[-0.1071,  0.0956, -0.1135],\n",
      "          [ 0.0225, -0.1317, -0.0448],\n",
      "          [ 0.0724, -0.1254,  0.0306]],\n",
      "\n",
      "         [[ 0.0681, -0.1408, -0.0381],\n",
      "          [ 0.1591,  0.0394, -0.0787],\n",
      "          [ 0.0755,  0.1179, -0.1603]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.1457, -0.0071],\n",
      "          [-0.1274, -0.1198, -0.0278],\n",
      "          [ 0.0968,  0.0864,  0.0325]],\n",
      "\n",
      "         [[-0.1607, -0.0931, -0.0062],\n",
      "          [ 0.0359, -0.1553,  0.0473],\n",
      "          [-0.1180, -0.0693,  0.1443]],\n",
      "\n",
      "         [[-0.0235, -0.1033, -0.1291],\n",
      "          [ 0.0615,  0.0980,  0.0757],\n",
      "          [-0.0489, -0.0668, -0.0824]],\n",
      "\n",
      "         [[ 0.0249,  0.0799,  0.1139],\n",
      "          [ 0.1360, -0.0858, -0.0431],\n",
      "          [-0.1233, -0.0465,  0.1503]]],\n",
      "\n",
      "\n",
      "        [[[-0.1586,  0.0867, -0.0577],\n",
      "          [ 0.1389,  0.0589, -0.1383],\n",
      "          [ 0.0326, -0.1345,  0.1604]],\n",
      "\n",
      "         [[-0.0664, -0.1348, -0.0349],\n",
      "          [-0.0002, -0.0256, -0.0162],\n",
      "          [ 0.1602,  0.1361,  0.0357]],\n",
      "\n",
      "         [[-0.0229, -0.0997,  0.1227],\n",
      "          [ 0.0545,  0.0696,  0.0240],\n",
      "          [ 0.0757,  0.0066,  0.0005]],\n",
      "\n",
      "         [[-0.1605, -0.0713, -0.0299],\n",
      "          [-0.0843,  0.0250, -0.0971],\n",
      "          [-0.1280,  0.1464, -0.1098]]]])), ('conv2.bias', tensor([ 0.1457,  0.1590,  0.1079,  0.0935, -0.1109, -0.0072, -0.0451,  0.0436,\n",
      "         0.1550,  0.0383,  0.1300,  0.0760])), ('linear1.weight', tensor([[ 1.5908e-02,  1.5656e-02,  4.6909e-02,  ..., -2.4763e-02,\n",
      "         -4.5605e-02, -6.0834e-06],\n",
      "        [ 3.8864e-02, -2.0766e-02,  5.0544e-02,  ...,  4.2443e-02,\n",
      "         -3.7946e-02, -1.1646e-02],\n",
      "        [-4.3416e-02, -2.7498e-02,  3.2318e-02,  ...,  4.7365e-02,\n",
      "          2.3062e-02,  2.8593e-02],\n",
      "        ...,\n",
      "        [ 1.8522e-02, -1.9695e-02, -5.4902e-02,  ...,  1.1535e-02,\n",
      "         -9.5615e-03,  2.9643e-02],\n",
      "        [ 2.4093e-02,  1.4357e-02, -3.5899e-02,  ..., -5.4755e-02,\n",
      "          4.6055e-02,  4.1239e-02],\n",
      "        [ 5.1365e-02, -3.0765e-02,  2.6570e-02,  ..., -1.2703e-02,\n",
      "          1.7119e-02, -2.4224e-02]])), ('linear1.bias', tensor([ 0.0252,  0.0300,  0.0190, -0.0107,  0.0018, -0.0247,  0.0454, -0.0023,\n",
      "         0.0309, -0.0527]))])\n",
      "Net_A1_DT: \n",
      " \t OrderedDict([('conv1.weight', tensor([[[[-0.0061, -0.1554, -0.0142,  0.0578, -0.0099],\n",
      "          [ 0.1869,  0.1770, -0.1435,  0.1801,  0.1628],\n",
      "          [ 0.1702, -0.0947,  0.1717, -0.0842,  0.1247],\n",
      "          [ 0.0238,  0.0544,  0.0687,  0.1856,  0.0196],\n",
      "          [ 0.0523,  0.0625,  0.0194,  0.0104,  0.1586]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1630,  0.0413, -0.0511, -0.1957, -0.0891],\n",
      "          [-0.1879,  0.0403, -0.1168, -0.0925,  0.1643],\n",
      "          [ 0.0493,  0.1674,  0.1800,  0.1824,  0.0387],\n",
      "          [-0.0117, -0.1641,  0.0288, -0.1827, -0.0479],\n",
      "          [ 0.0218, -0.0633, -0.0475, -0.1878,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1177, -0.1837,  0.1865, -0.0738, -0.0184],\n",
      "          [ 0.0237,  0.1294,  0.1114,  0.1400,  0.0315],\n",
      "          [ 0.0940, -0.0617, -0.0338, -0.0947, -0.1626],\n",
      "          [ 0.0766,  0.0023,  0.0311,  0.1917,  0.1184],\n",
      "          [ 0.1952,  0.1259, -0.0296, -0.1338,  0.0884]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0724,  0.0605, -0.1065, -0.0611, -0.0419],\n",
      "          [-0.1390, -0.0909,  0.0681, -0.1637,  0.0339],\n",
      "          [-0.1531,  0.1662, -0.1162,  0.1599, -0.0344],\n",
      "          [-0.1465, -0.0736, -0.0137,  0.1574, -0.0215],\n",
      "          [ 0.1790, -0.0872,  0.1049,  0.0193,  0.0299]]]])), ('conv1.bias', tensor([ 0.1321, -0.0655, -0.0929,  0.0429])), ('conv2.weight', tensor([[[[ 0.1518, -0.0377,  0.1249],\n",
      "          [-0.0100, -0.1487,  0.0391],\n",
      "          [-0.1228,  0.1132, -0.0737]],\n",
      "\n",
      "         [[-0.0040, -0.0954, -0.1017],\n",
      "          [ 0.1362,  0.0255, -0.1470],\n",
      "          [-0.0381, -0.0117,  0.0466]],\n",
      "\n",
      "         [[ 0.0319,  0.0124, -0.1047],\n",
      "          [-0.0964, -0.0315,  0.0061],\n",
      "          [-0.0346,  0.0237,  0.0561]],\n",
      "\n",
      "         [[-0.1640,  0.0097,  0.1342],\n",
      "          [ 0.0255, -0.0511, -0.0716],\n",
      "          [-0.1100,  0.0205,  0.0216]]],\n",
      "\n",
      "\n",
      "        [[[-0.0389,  0.0102,  0.0679],\n",
      "          [ 0.0359,  0.0830, -0.1491],\n",
      "          [ 0.1260, -0.1300, -0.0153]],\n",
      "\n",
      "         [[-0.0946, -0.0965, -0.1040],\n",
      "          [ 0.0090,  0.0904,  0.1590],\n",
      "          [ 0.1180, -0.1597,  0.1555]],\n",
      "\n",
      "         [[-0.0260,  0.0207, -0.0976],\n",
      "          [-0.0525,  0.0711, -0.0511],\n",
      "          [-0.0899,  0.0119,  0.0755]],\n",
      "\n",
      "         [[ 0.0441,  0.1637, -0.1303],\n",
      "          [ 0.1034,  0.0384, -0.1419],\n",
      "          [ 0.1600,  0.0319, -0.1161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0798,  0.0988,  0.0132],\n",
      "          [ 0.0810, -0.1636,  0.0996],\n",
      "          [-0.0379,  0.0433,  0.1582]],\n",
      "\n",
      "         [[-0.1436, -0.1092, -0.0718],\n",
      "          [ 0.0768, -0.0347,  0.1154],\n",
      "          [-0.0850,  0.1057,  0.1478]],\n",
      "\n",
      "         [[ 0.0451, -0.1337,  0.0361],\n",
      "          [ 0.1170,  0.0857, -0.0100],\n",
      "          [-0.1445,  0.1088, -0.0726]],\n",
      "\n",
      "         [[-0.1381,  0.0773,  0.1400],\n",
      "          [ 0.1583,  0.0366, -0.1547],\n",
      "          [-0.0305,  0.0773,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1513, -0.0850, -0.0475],\n",
      "          [ 0.0019, -0.0261,  0.0249],\n",
      "          [ 0.0756,  0.0299,  0.1320]],\n",
      "\n",
      "         [[-0.1435,  0.0879,  0.1333],\n",
      "          [-0.1408, -0.1238, -0.1189],\n",
      "          [ 0.1580,  0.0445,  0.1471]],\n",
      "\n",
      "         [[-0.1446, -0.1364, -0.1047],\n",
      "          [-0.0881, -0.0236, -0.0295],\n",
      "          [-0.0934,  0.0408,  0.0733]],\n",
      "\n",
      "         [[-0.0991, -0.0325, -0.0805],\n",
      "          [ 0.0139, -0.1579, -0.1488],\n",
      "          [ 0.0398, -0.1449, -0.0617]]],\n",
      "\n",
      "\n",
      "        [[[-0.1090,  0.1399, -0.1253],\n",
      "          [-0.1549,  0.1320,  0.1445],\n",
      "          [ 0.0180, -0.0368, -0.0870]],\n",
      "\n",
      "         [[ 0.1379,  0.0878,  0.0761],\n",
      "          [ 0.1032, -0.0537,  0.1039],\n",
      "          [ 0.1325,  0.0757,  0.0921]],\n",
      "\n",
      "         [[-0.1363,  0.1648, -0.0822],\n",
      "          [-0.1281, -0.0285,  0.0003],\n",
      "          [-0.0942,  0.0503, -0.0420]],\n",
      "\n",
      "         [[ 0.0194, -0.0237, -0.1151],\n",
      "          [ 0.1521, -0.1559,  0.0093],\n",
      "          [ 0.0274, -0.0812, -0.1523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0311,  0.0711,  0.0515],\n",
      "          [-0.1309,  0.1466,  0.0431],\n",
      "          [ 0.0786, -0.0175,  0.1469]],\n",
      "\n",
      "         [[ 0.1227,  0.1405,  0.1267],\n",
      "          [ 0.1657, -0.0699,  0.0274],\n",
      "          [ 0.1185, -0.1070, -0.0855]],\n",
      "\n",
      "         [[ 0.0275, -0.0293, -0.0995],\n",
      "          [ 0.1093, -0.1078,  0.1532],\n",
      "          [ 0.0171,  0.0958, -0.1060]],\n",
      "\n",
      "         [[ 0.1013,  0.0228,  0.1601],\n",
      "          [ 0.0962,  0.1461,  0.0093],\n",
      "          [ 0.1347,  0.1429, -0.1522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0859, -0.1321,  0.1002],\n",
      "          [ 0.1038,  0.1370,  0.1242],\n",
      "          [-0.1526,  0.1455,  0.1296]],\n",
      "\n",
      "         [[-0.1333,  0.1242,  0.0865],\n",
      "          [ 0.0887,  0.0154,  0.0190],\n",
      "          [-0.0599, -0.1358,  0.0817]],\n",
      "\n",
      "         [[-0.1122,  0.0853, -0.0671],\n",
      "          [ 0.1066,  0.0040, -0.0420],\n",
      "          [ 0.0128, -0.1329,  0.0146]],\n",
      "\n",
      "         [[ 0.1168, -0.0891,  0.0562],\n",
      "          [ 0.1476, -0.0054,  0.0818],\n",
      "          [-0.0672, -0.1401, -0.1643]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893,  0.0464,  0.1150],\n",
      "          [ 0.0311,  0.1285,  0.0288],\n",
      "          [-0.0565, -0.0942,  0.0682]],\n",
      "\n",
      "         [[-0.0023,  0.0231,  0.0418],\n",
      "          [ 0.1206, -0.1640,  0.0080],\n",
      "          [-0.0236,  0.0674,  0.1320]],\n",
      "\n",
      "         [[-0.1453,  0.1468, -0.0836],\n",
      "          [-0.1412,  0.0860, -0.1551],\n",
      "          [ 0.1181,  0.0454,  0.1532]],\n",
      "\n",
      "         [[-0.0770,  0.0083, -0.0158],\n",
      "          [-0.0284,  0.0335, -0.0067],\n",
      "          [-0.1286,  0.1629,  0.0188]]],\n",
      "\n",
      "\n",
      "        [[[-0.1556, -0.1562,  0.0559],\n",
      "          [ 0.0663, -0.0118,  0.0820],\n",
      "          [-0.1099, -0.0277,  0.1154]],\n",
      "\n",
      "         [[-0.1092,  0.0893, -0.0582],\n",
      "          [ 0.0080,  0.1640,  0.0887],\n",
      "          [ 0.1373,  0.1052, -0.0953]],\n",
      "\n",
      "         [[ 0.0175,  0.1626, -0.0476],\n",
      "          [ 0.0459,  0.0813, -0.0941],\n",
      "          [ 0.1165,  0.0350, -0.1572]],\n",
      "\n",
      "         [[ 0.0493, -0.1505, -0.0528],\n",
      "          [ 0.0054,  0.1412, -0.0423],\n",
      "          [-0.0360,  0.1500,  0.1583]]],\n",
      "\n",
      "\n",
      "        [[[-0.0693,  0.0622,  0.1566],\n",
      "          [-0.0018, -0.1420,  0.0784],\n",
      "          [-0.0739, -0.0408,  0.1492]],\n",
      "\n",
      "         [[-0.1150,  0.1161,  0.0402],\n",
      "          [ 0.1274,  0.0941,  0.0629],\n",
      "          [ 0.0488,  0.0396,  0.1660]],\n",
      "\n",
      "         [[-0.1071,  0.0956, -0.1135],\n",
      "          [ 0.0225, -0.1317, -0.0448],\n",
      "          [ 0.0724, -0.1254,  0.0306]],\n",
      "\n",
      "         [[ 0.0681, -0.1408, -0.0381],\n",
      "          [ 0.1591,  0.0394, -0.0787],\n",
      "          [ 0.0755,  0.1179, -0.1603]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.1457, -0.0071],\n",
      "          [-0.1274, -0.1198, -0.0278],\n",
      "          [ 0.0968,  0.0864,  0.0325]],\n",
      "\n",
      "         [[-0.1607, -0.0931, -0.0062],\n",
      "          [ 0.0359, -0.1553,  0.0473],\n",
      "          [-0.1180, -0.0693,  0.1443]],\n",
      "\n",
      "         [[-0.0235, -0.1033, -0.1291],\n",
      "          [ 0.0615,  0.0980,  0.0757],\n",
      "          [-0.0489, -0.0668, -0.0824]],\n",
      "\n",
      "         [[ 0.0249,  0.0799,  0.1139],\n",
      "          [ 0.1360, -0.0858, -0.0431],\n",
      "          [-0.1233, -0.0465,  0.1503]]],\n",
      "\n",
      "\n",
      "        [[[-0.1586,  0.0867, -0.0577],\n",
      "          [ 0.1389,  0.0589, -0.1383],\n",
      "          [ 0.0326, -0.1345,  0.1604]],\n",
      "\n",
      "         [[-0.0664, -0.1348, -0.0349],\n",
      "          [-0.0002, -0.0256, -0.0162],\n",
      "          [ 0.1602,  0.1361,  0.0357]],\n",
      "\n",
      "         [[-0.0229, -0.0997,  0.1227],\n",
      "          [ 0.0545,  0.0696,  0.0240],\n",
      "          [ 0.0757,  0.0066,  0.0005]],\n",
      "\n",
      "         [[-0.1605, -0.0713, -0.0299],\n",
      "          [-0.0843,  0.0250, -0.0971],\n",
      "          [-0.1280,  0.1464, -0.1098]]]])), ('conv2.bias', tensor([ 0.1457,  0.1590,  0.1079,  0.0935, -0.1109, -0.0072, -0.0451,  0.0436,\n",
      "         0.1550,  0.0383,  0.1300,  0.0760])), ('linear1.weight', tensor([[ 1.5908e-02,  1.5656e-02,  4.6909e-02,  ..., -2.4763e-02,\n",
      "         -4.5605e-02, -6.0834e-06],\n",
      "        [ 3.8864e-02, -2.0766e-02,  5.0544e-02,  ...,  4.2443e-02,\n",
      "         -3.7946e-02, -1.1646e-02],\n",
      "        [-4.3416e-02, -2.7498e-02,  3.2318e-02,  ...,  4.7365e-02,\n",
      "          2.3062e-02,  2.8593e-02],\n",
      "        ...,\n",
      "        [ 1.8522e-02, -1.9695e-02, -5.4902e-02,  ...,  1.1535e-02,\n",
      "         -9.5615e-03,  2.9643e-02],\n",
      "        [ 2.4093e-02,  1.4357e-02, -3.5899e-02,  ..., -5.4755e-02,\n",
      "          4.6055e-02,  4.1239e-02],\n",
      "        [ 5.1365e-02, -3.0765e-02,  2.6570e-02,  ..., -1.2703e-02,\n",
      "          1.7119e-02, -2.4224e-02]])), ('linear1.bias', tensor([ 0.0252,  0.0300,  0.0190, -0.0107,  0.0018, -0.0247,  0.0454, -0.0023,\n",
      "         0.0309, -0.0527]))])\n"
     ]
    }
   ],
   "source": [
    "net_a2_hf = NetA2(10)\n",
    "net_a2_ht = NetA2(10)\n",
    "net_a2_dt = NetA2(10)\n",
    "\n",
    "#set conv1 initialization of net_a2_hf\n",
    "net_a2_hf.conv1.weight = nn.Parameter(copy.deepcopy(initialization_weights))\n",
    "net_a2_hf.conv1.bias = nn.Parameter(copy.deepcopy(initialization_biases))\n",
    "\n",
    "# set same weights and bias to each layer of each network\n",
    "net_a2_ht.load_state_dict(net_a2_hf.state_dict())\n",
    "for name, param in net_a2_hf.state_dict().items():\n",
    "    if \"conv1\" not in name:\n",
    "        net_a2_dt.state_dict()[name].copy_(param)\n",
    "\n",
    "#set conv1 initialization\n",
    "net_a2_dt.conv1.load_state_dict(net_a1_dt.conv1.state_dict())\n",
    "\n",
    "#freeze conv1 layer of net_a2_hf\n",
    "net_a2_hf.freeze(\"conv1\")\n",
    "\n",
    "#save weights and bias of nat_a1_h* and net_a1_dt\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_init.pt')\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_init.pt')\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2DT_init.pt')\n",
    "\n",
    "\n",
    "# print weights and bias\n",
    "print(\"Net_A1_HF: \\n \\t\", net_a2_hf.state_dict())\n",
    "print(\"Net_A1_HT: \\n \\t\", net_a2_ht.state_dict())\n",
    "print(\"Net_A1_DT: \\n \\t\", net_a2_dt.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.305318Z",
     "start_time": "2024-06-18T10:12:22.275935Z"
    }
   },
   "id": "8dc214f47bd76048",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminary Analysys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e527a7ee28f832"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1: \n",
      " \t|W_{conv_a1_hf} - W_{conv_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear_a1_hf} - W_{linear_a1_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      "\n",
      "Net_A2: \n",
      " \t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      ",\n",
      "       \"\t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_ht.linear2.weight), \"\n",
      "\",\n",
      "       \"\t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_dt.linear2.weight), \"\n",
      "\",\n",
      "       \"\t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_ht.linear3.weight), \"\n",
      "\",\n",
      "       \"\t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_dt.linear3.weight), \"\n",
      "\"\n",
      "       \n",
      "Net_A1 Vs Net_A2: \n",
      " \t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| = tensor(0.) \n",
      " \t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n",
      " \t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| = tensor(0., grad_fn=<LinalgVectorNormBackward0>) \n"
     ]
    }
   ],
   "source": [
    "print( \"Net_A1: \\n\",\n",
    "       \"\\t|W_{conv_a1_hf} - W_{conv_a1_ht}| =\", torch.norm(net_a1_hf.conv1.weight - net_a1_ht.conv1.weight),\"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_ht}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_ht.linear1.weight), \"\\n\",\n",
    "      \"\\t|W_{linear_a1_hf} - W_{linear_a1_dt}| =\", torch.norm(net_a1_hf.linear1.weight - net_a1_dt.linear1.weight), \"\\n\")\n",
    "\n",
    "print( \"Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a2_hf} - W_{conv1_a2_ht}| =\", torch.norm(net_a2_hf.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv2_a2_hf} - W_{conv2_a2_ht}| =\", torch.norm(net_a2_hf.conv2.weight - net_a2_ht.conv2.weight),\"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_ht}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_ht.linear1.weight), \"\\n\",\n",
    "       \"\\t|W_{linear1_a2_hf} - W_{linear1_a2_dt}| =\", torch.norm(net_a2_hf.linear1.weight - net_a2_dt.linear1.weight), \"\\n\"\"\"\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_ht}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_ht.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear2_a2_hf} - W_{linear2_a2_dt}| =\", torch.norm(net_a2_hf.linear2.weight - net_a2_dt.linear2.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_ht}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_ht.linear3.weight), \"\\n\",\n",
    "       \"\\t|W_{linear3_a2_hf} - W_{linear3_a2_dt}| =\", torch.norm(net_a2_hf.linear3.weight - net_a2_dt.linear3.weight), \"\\n\"\n",
    "       \"\"\")\n",
    "\n",
    "print( \"Net_A1 Vs Net_A2: \\n\",\n",
    "       \"\\t|W_{conv1_a1_hf} - W_{conv1_a2_hf}| =\", torch.norm(net_a1_hf.conv1.weight - net_a2_hf.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_ht} - W_{conv2_a2_ht}| =\", torch.norm(net_a1_ht.conv1.weight - net_a2_ht.conv1.weight),\"\\n\",\n",
    "       \"\\t|W_{conv1_a1_dt} - W_{conv2_a2_dt}| =\", torch.norm(net_a1_dt.conv1.weight - net_a2_dt.conv1.weight),\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.316266Z",
     "start_time": "2024-06-18T10:12:22.307327Z"
    }
   },
   "id": "eabba971baa9ea0d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A1HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A2HF:\n",
      "\t False\n",
      "\t False\n",
      "Net_A1HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2HT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A1DT:\n",
      "\t True\n",
      "\t True\n",
      "Net_A2DT:\n",
      "\t True\n",
      "\t True\n"
     ]
    }
   ],
   "source": [
    "print(\"Net_A1HF:\")\n",
    "for param in net_a1_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HF:\")\n",
    "for param in net_a2_hf.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1HT:\")\n",
    "for param in net_a1_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2HT:\")\n",
    "for param in net_a2_ht.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A1DT:\")\n",
    "for param in net_a1_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)\n",
    "print(\"Net_A2DT:\")\n",
    "for param in net_a2_dt.conv1.parameters():\n",
    "    print(\"\\t\",param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.323457Z",
     "start_time": "2024-06-18T10:12:22.318278Z"
    }
   },
   "id": "527076a732d29933",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net_a1_ht.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a1_ht.linear1.weight is net_a1_hf.linear1.weight)\n",
    "print(net_a1_hf.linear1.weight is net_a1_dt.linear1.weight)\n",
    "print(net_a2_hf.conv1.weight is net_a1_hf.conv1.weight)\n",
    "print(net_a2_ht.conv1.weight is net_a1_ht.conv1.weight)\n",
    "print(net_a2_dt.conv1.weight is net_a1_dt.conv1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_ht.linear1.weight)\n",
    "print(net_a2_hf.linear1.weight is net_a2_dt.linear1.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.331554Z",
     "start_time": "2024-06-18T10:12:22.325819Z"
    }
   },
   "id": "7f3e08ca6623fd25",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9884bc78c276b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.379382Z",
     "start_time": "2024-06-18T10:12:22.332562Z"
    }
   },
   "id": "e8c282a53727943",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "image, label = train_data[sample_idx]\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.390087Z",
     "start_time": "2024-06-18T10:12:22.380453Z"
    }
   },
   "id": "84fa5b26a60379de",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader= DataLoader(train_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.395813Z",
     "start_time": "2024-06-18T10:12:22.391096Z"
    }
   },
   "id": "be585d7eb8bcae35",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training/Test Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238fefd45b206a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test_loop(device, dataloader, model, loss_fn):\n",
    "      size = len(dataloader.dataset)\n",
    "      num_batches = len(dataloader)\n",
    "      test_loss, correct = 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          pred = model(X)\n",
    "          test_loss += loss_fn(pred, y).item()\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "      test_loss /= num_batches\n",
    "      correct /= size\n",
    "      print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "      return 100*correct, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.404828Z",
     "start_time": "2024-06-18T10:12:22.396823Z"
    }
   },
   "id": "1b06878e4ef1f642",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79254d3d93db89e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 5e-4#4.1\n",
    "epochs = 60"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.411255Z",
     "start_time": "2024-06-18T10:12:22.406838Z"
    }
   },
   "id": "b437c40fa560695f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_test(device, train_dataloader, test_dataloader, net, learning_rate, epochs):\n",
    "    net.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    times=[]\n",
    "    \n",
    "    time_s = time.time()\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, net, loss_fn, optimizer)\n",
    "        acc, loss = test_loop(device, test_dataloader, net, loss_fn)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "        times.append(time.time() - time_s)\n",
    "    print(\"Done!\")\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": [ i for i in range(epochs)],\n",
    "            \"times\": times,\n",
    "            \"loss\": losses,\n",
    "            \"accuracy\": accuracies\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:12:22.417838Z",
     "start_time": "2024-06-18T10:12:22.412777Z"
    }
   },
   "id": "a166ce4ccf0cb4a4",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1 -> HF Train "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcdec5977207972"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301509  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.254974 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.255678  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.254472 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.255539  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.254262 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.255520  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.254160 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.255492  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.254044 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.255539  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.253882 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.255542  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.253731 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.255537  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.253640 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.255531  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.253599 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.255521  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.253563 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.255521  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253561 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.255517  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.253579 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.255510  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.253614 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.255492  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.253664 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.255486  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253695 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.255469  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.253732 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.255398  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.253810 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.255411  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253862 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.255294  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253863 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.255238  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253860 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.255312  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.253831 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.255110  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.253812 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.255088  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.253801 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.255025  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.253765 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.255050  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253751 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.255051  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253789 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.255114  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253754 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.255091  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253802 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.255056  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.253809 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.255043  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.253847 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.255023  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.253934 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.255017  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.253934 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.255011  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.253923 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.254934  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.253923 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.254972  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.253846 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.254962  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.253775 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.254961  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.253761 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.254960  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.253856 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.254946  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.253785 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.254933  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 2.253718 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.254930  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.253847 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.254919  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.253771 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.254915  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.253847 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.254886  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.253698 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.254878  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253820 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.254871  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253676 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.254858  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.253943 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.254846  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253948 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.254835  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.253651 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.254827  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253655 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.254821  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253664 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.254817  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253678 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.254817  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.253668 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.254820  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253615 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.254832  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253574 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.254847  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 2.253542 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.254852  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253578 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.254843  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253587 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.254828  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 2.253603 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.254812  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253648 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[1., 0., 0., 0., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [0., 1., 0., 1., 0.],\n                        [1., 0., 0., 0., 1.]]],\n              \n              \n                      [[[0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 0., 1., 0., 0.]]],\n              \n              \n                      [[[0., 1., 1., 1., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 0., 0., 0., 1.],\n                        [1., 1., 0., 1., 1.],\n                        [0., 1., 1., 1., 0.]]],\n              \n              \n                      [[[1., 1., 0., 1., 1.],\n                        [0., 1., 0., 1., 0.],\n                        [0., 0., 1., 0., 0.],\n                        [1., 1., 0., 1., 1.],\n                        [1., 1., 0., 1., 1.]]]])),\n             ('conv1.bias', tensor([0., 0., 0., 0.])),\n             ('linear1.weight',\n              tensor([[-1.2231e-02,  2.6658e-03,  2.8865e-02,  ..., -1.9144e-02,\n                       -5.8173e-04, -3.7382e-02],\n                      [-7.4004e-02, -8.7806e-01,  8.3700e-02,  ..., -1.8210e-02,\n                       -1.8596e-01, -6.2993e-01],\n                      [ 9.1455e-03, -1.7131e-01,  6.5400e-02,  ...,  2.0171e-01,\n                       -2.0752e-01,  2.9299e-01],\n                      ...,\n                      [-3.2235e-02, -4.1271e-03, -5.3068e-02,  ...,  9.0036e-03,\n                        2.0242e-02, -1.6757e-01],\n                      [ 2.5662e-02,  1.1304e-02, -1.7231e-03,  ...,  3.8199e-02,\n                        1.1323e-02, -1.8668e-02],\n                      [ 1.4586e-01, -1.5546e-02,  5.8942e-02,  ...,  1.2703e-01,\n                        8.0537e-02,  2.8634e-03]])),\n             ('linear1.bias',\n              tensor([-0.0197,  7.8084,  0.6569,  2.8344,  0.2884,  3.4343, -0.0431,  4.9496,\n                      -0.0351,  4.8936]))])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_hf.state_dict())\n",
    "df_net_a1_hf = train_test(device, train_dataloader, test_dataloader, net_a1_hf, learning_rate, epochs)\n",
    "df_net_a1_hf.to_csv('NetA1HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_hf.state_dict()}, 'NetA1HF_trained.pt')\n",
    "net_a1_hf.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:20:47.231327Z",
     "start_time": "2024-06-18T10:12:25.651321Z"
    }
   },
   "id": "98b5cb92a0191d1e",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> HT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ae1a1c117cd5c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301509  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 2.254866 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.255673  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.254366 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.255560  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.254111 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.255553  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.254053 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.255589  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.253906 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.255588  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.253870 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.255574  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.253645 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.255564  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.253589 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.255548  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.253526 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.255541  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.253493 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.255529  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.253454 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.255516  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.253458 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.255488  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.253428 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.255477  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.253537 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.255501  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.253348 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.255405  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.253469 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.255407  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.253530 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.255410  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.253275 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.255355  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.253414 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.255341  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253521 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.255285  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253500 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.255202  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.253274 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.255089  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253306 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.255133  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253398 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.255102  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.253481 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.255040  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.253371 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.255046  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.253469 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.255022  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 2.253434 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.255231  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.253459 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.254998  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.253477 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.254997  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.253409 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.254993  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253447 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.254971  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253437 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.254955  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.253397 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.255001  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.253386 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.254951  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.253409 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.254938  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253356 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.254926  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.253396 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.254911  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.253403 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.254966  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.253433 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.254974  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.253432 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.254973  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.253449 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.254958  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.253456 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.254945  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.253468 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.254924  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.253460 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.254911  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.253457 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.254897  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.253455 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.254884  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253460 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.254858  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.253469 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.254854  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253476 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.254871  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.253516 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.254841  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.253461 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.254848  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 2.253431 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.254803  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 2.253427 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.254802  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 2.253411 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.254799  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 2.253399 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.254779  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 2.253375 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.254798  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.253398 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.254775  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 2.253325 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.254756  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 2.253324 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 0.7176, -0.5781, -0.6060, -0.0475,  1.1054],\n                        [-0.1733,  0.9834, -0.1711,  1.4371, -0.1028],\n                        [-0.5196,  0.2502,  0.8477,  0.5163, -0.2029],\n                        [-0.2545,  0.9754, -0.3562,  1.4367, -0.1005],\n                        [ 1.2646,  0.1536, -0.0976,  0.3039,  1.5294]]],\n              \n              \n                      [[[-0.4497, -0.4623,  1.6874, -0.0393, -0.2903],\n                        [ 0.7817,  0.6986,  0.9122,  1.5241,  1.0825],\n                        [-0.1048, -0.0579,  1.9054,  0.6129, -0.1130],\n                        [ 0.8026,  0.7876,  0.7397,  1.5373,  1.0518],\n                        [-0.2522, -0.1370,  1.6614,  0.3832,  0.0592]]],\n              \n              \n                      [[[-0.2018,  1.3166,  1.0381,  1.3409, -0.1870],\n                        [ 1.2004,  1.0302, -0.4551,  1.0690,  1.0940],\n                        [ 1.1427, -0.0579, -0.6418, -0.0068,  1.1490],\n                        [ 1.1082,  0.9236, -0.5081,  1.2911,  1.4286],\n                        [-0.0521,  1.0311,  0.9740,  1.4438,  0.2847]]],\n              \n              \n                      [[[ 1.1279,  1.3414, -0.4848,  1.3775,  1.1129],\n                        [-0.1182,  1.1245, -0.8141,  0.8432, -0.2556],\n                        [-0.2584,  0.1512,  0.2813, -0.1914, -0.0428],\n                        [ 1.3054,  1.2440, -0.7702,  1.0846,  1.3773],\n                        [ 1.3742,  1.3320, -0.4463,  1.2669,  1.7469]]]])),\n             ('conv1.bias', tensor([ 0.0014, -0.6926,  0.5214, -0.0050])),\n             ('linear1.weight',\n              tensor([[-1.2231e-02,  2.6677e-03,  2.8866e-02,  ..., -1.9145e-02,\n                       -5.8336e-04, -3.7401e-02],\n                      [-2.3119e-02, -8.2252e-01,  4.9647e-02,  ..., -8.8297e-03,\n                       -1.4416e-01, -7.0522e-01],\n                      [-2.1489e-01, -7.4050e-02,  1.6544e-02,  ...,  1.8018e-01,\n                       -2.3515e-01,  2.3778e-01],\n                      ...,\n                      [ 1.0631e-01, -4.5389e-02, -6.6059e-02,  ...,  1.7865e-01,\n                        1.9107e-01,  1.9051e-01],\n                      [ 2.5664e-02,  1.1304e-02, -1.7231e-03,  ...,  3.8199e-02,\n                        1.1323e-02, -1.8713e-02],\n                      [ 2.7085e-01,  1.6085e-01,  5.3082e-02,  ...,  1.0249e-01,\n                        3.3976e-02,  2.1865e-02]])),\n             ('linear1.bias',\n              tensor([-0.0197,  6.9394,  0.6143,  2.4327, -0.0778,  3.1482, -0.0431,  4.4921,\n                      -0.0351,  4.7947]))])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_ht.state_dict())\n",
    "df_net_a1_ht = train_test(device, train_dataloader, test_dataloader, net_a1_ht, learning_rate, epochs)\n",
    "df_net_a1_ht.to_csv('NetA1HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_ht.state_dict()}, 'NetA1HT_trained.pt')\n",
    "net_a1_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:27:00.366690Z",
     "start_time": "2024-06-18T10:20:47.232374Z"
    }
   },
   "id": "57d9aed7097ab0b0",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA1-> DT train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e7c98ba5a28137"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[-0.0061, -0.1554, -0.0142,  0.0578, -0.0099],\n",
      "          [ 0.1869,  0.1770, -0.1435,  0.1801,  0.1628],\n",
      "          [ 0.1702, -0.0947,  0.1717, -0.0842,  0.1247],\n",
      "          [ 0.0238,  0.0544,  0.0687,  0.1856,  0.0196],\n",
      "          [ 0.0523,  0.0625,  0.0194,  0.0104,  0.1586]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1630,  0.0413, -0.0511, -0.1957, -0.0891],\n",
      "          [-0.1879,  0.0403, -0.1168, -0.0925,  0.1643],\n",
      "          [ 0.0493,  0.1674,  0.1800,  0.1824,  0.0387],\n",
      "          [-0.0117, -0.1641,  0.0288, -0.1827, -0.0479],\n",
      "          [ 0.0218, -0.0633, -0.0475, -0.1878,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1177, -0.1837,  0.1865, -0.0738, -0.0184],\n",
      "          [ 0.0237,  0.1294,  0.1114,  0.1400,  0.0315],\n",
      "          [ 0.0940, -0.0617, -0.0338, -0.0947, -0.1626],\n",
      "          [ 0.0766,  0.0023,  0.0311,  0.1917,  0.1184],\n",
      "          [ 0.1952,  0.1259, -0.0296, -0.1338,  0.0884]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0724,  0.0605, -0.1065, -0.0611, -0.0419],\n",
      "          [-0.1390, -0.0909,  0.0681, -0.1637,  0.0339],\n",
      "          [-0.1531,  0.1662, -0.1162,  0.1599, -0.0344],\n",
      "          [-0.1465, -0.0736, -0.0137,  0.1574, -0.0215],\n",
      "          [ 0.1790, -0.0872,  0.1049,  0.0193,  0.0299]]]])), ('conv1.bias', tensor([ 0.1321, -0.0655, -0.0929,  0.0429])), ('linear1.weight', tensor([[-0.0109,  0.0062,  0.0268,  ..., -0.0219, -0.0045, -0.0344],\n",
      "        [-0.0116,  0.0373,  0.0330,  ...,  0.0226,  0.0077,  0.0011],\n",
      "        [-0.0132, -0.0024, -0.0067,  ...,  0.0403, -0.0240,  0.0265],\n",
      "        ...,\n",
      "        [-0.0260,  0.0160,  0.0219,  ..., -0.0409,  0.0295, -0.0053],\n",
      "        [ 0.0281,  0.0138,  0.0008,  ...,  0.0408,  0.0139, -0.0168],\n",
      "        [-0.0146,  0.0268,  0.0032,  ...,  0.0094,  0.0027,  0.0327]])), ('linear1.bias', tensor([-0.0145, -0.0305, -0.0306, -0.0284, -0.0153,  0.0234, -0.0402, -0.0384,\n",
      "        -0.0325, -0.0328]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302817  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 2.263321 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.262670  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.262298 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.262154  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.261972 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.262098  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 2.261850 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.262034  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 2.261771 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.262007  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.261707 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.261996  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 2.261656 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.261986  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 2.261621 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.261978  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.261595 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.261972  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.261571 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.261966  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 2.261544 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.261959  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.261515 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.261949  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 2.261487 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.261939  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 2.261455 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.261928  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.261426 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.261917  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.261389 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.261903  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.261340 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.261882  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 2.261300 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.261856  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 2.261277 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.261832  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.261253 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.261811  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 2.261232 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.261790  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 2.261218 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.261770  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 2.261218 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.261750  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.261217 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.261728  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.261306 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.261702  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.261324 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.261685  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.261327 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.261670  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 2.261328 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.261657  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.261325 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.261645  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.261319 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.261635  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.261311 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.261626  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.261303 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.261616  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.261296 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.261607  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.261288 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.261598  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.261281 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.261591  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.261276 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.261584  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.261270 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.261577  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.261267 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.261572  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.261265 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.261566  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.261261 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.261561  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.261255 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.261558  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.261248 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.261555  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 2.261241 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.261552  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.261234 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.261550  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 2.261230 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.261548  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.261228 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.261547  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.261230 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.261547  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.261233 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.261547  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.261238 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.261547  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.261244 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.261546  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.261250 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.261545  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.261255 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.261544  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.261256 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.261544  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.261255 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.261542  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.261247 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.261542  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 2.261241 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.261540  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.261238 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.261537  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 2.261232 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.261535  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 2.261224 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.261532  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.261213 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-0.3240, -0.4770, -0.1767,  0.4256,  0.3182],\n                        [ 0.7024,  0.3412, -0.0836,  0.6190,  0.6251],\n                        [ 0.4918, -0.0852, -0.0355,  0.2151,  0.5686],\n                        [ 0.1675, -0.0704, -0.1758,  0.3755,  0.2207],\n                        [ 0.2447,  0.0588, -0.0694,  0.4799,  0.6172]]],\n              \n              \n                      [[[ 0.7177,  0.9911,  0.0063, -0.7684,  0.1485],\n                        [ 0.2821,  0.7993,  0.0987, -0.1572,  0.9992],\n                        [ 0.2418,  0.9996,  0.5280,  0.3445,  0.9843],\n                        [-0.0828,  0.1330, -0.0860, -0.4443,  0.3603],\n                        [ 0.0342,  0.0854, -0.2356, -0.5914,  0.1753]]],\n              \n              \n                      [[[ 0.2906, -0.1114,  0.5505,  0.2307, -0.0313],\n                        [ 0.5189,  0.2588,  0.5592,  0.5711,  0.0024],\n                        [ 0.3384, -0.0882, -0.2907, -0.0444, -0.1639],\n                        [ 0.5591,  0.0189, -0.1714,  0.5358,  0.3494],\n                        [ 0.8210,  0.3834, -0.1159,  0.2902,  0.4490]]],\n              \n              \n                      [[[-0.6574, -0.5959, -0.4543, -0.1160,  0.1480],\n                        [-0.7799, -0.7597,  0.0614,  0.1277,  0.3749],\n                        [-0.8849, -0.2266,  0.0073,  0.7530,  0.2172],\n                        [-0.5351, -0.2635,  0.4459,  0.9732,  0.6042],\n                        [ 0.0895, -0.1496,  0.4500,  0.7663,  0.5479]]]])),\n             ('conv1.bias', tensor([ 1.8524, -0.6448, -0.2600,  0.6393])),\n             ('linear1.weight',\n              tensor([[ 0.1813, -0.0240, -0.1449,  ..., -0.3418, -0.3436,  0.5534],\n                      [ 1.2610, -0.8243,  0.1145,  ...,  0.0402,  0.5265,  1.3142],\n                      [-0.0879,  0.0647,  0.0965,  ...,  0.0345, -0.0649, -0.2368],\n                      ...,\n                      [ 0.9084,  0.6966,  0.0444,  ..., -0.2063, -0.1911, -0.2102],\n                      [ 0.0247,  0.0104, -0.0026,  ...,  0.0376,  0.0107, -0.0201],\n                      [ 0.9547,  0.3615,  0.0162,  ...,  0.0845, -0.1635, -0.2482]])),\n             ('linear1.bias',\n              tensor([ 1.4190,  0.6749,  1.0665, -0.0361,  0.3967,  0.0162, -0.0437,  0.7803,\n                      -0.0361,  3.7115]))])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net_a1_dt.state_dict())\n",
    "df_net_a1_dt = train_test(device, train_dataloader, test_dataloader, net_a1_dt, learning_rate, epochs)\n",
    "df_net_a1_dt.to_csv('NetA1DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a1_dt.state_dict()}, 'NetA1DT_trained.pt')\n",
    "net_a1_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:33:04.165924Z",
     "start_time": "2024-06-18T10:27:00.368700Z"
    }
   },
   "id": "5fc9595e6f55df0b",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HF Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aabaf6906661353"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[1., 0., 0., 0., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1., 1., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [0., 1., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 1., 1.],\n",
      "          [0., 1., 0., 1., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [1., 1., 0., 1., 1.],\n",
      "          [1., 1., 0., 1., 1.]]]])), ('conv1.bias', tensor([0., 0., 0., 0.])), ('conv2.weight', tensor([[[[ 0.1518, -0.0377,  0.1249],\n",
      "          [-0.0100, -0.1487,  0.0391],\n",
      "          [-0.1228,  0.1132, -0.0737]],\n",
      "\n",
      "         [[-0.0040, -0.0954, -0.1017],\n",
      "          [ 0.1362,  0.0255, -0.1470],\n",
      "          [-0.0381, -0.0117,  0.0466]],\n",
      "\n",
      "         [[ 0.0319,  0.0124, -0.1047],\n",
      "          [-0.0964, -0.0315,  0.0061],\n",
      "          [-0.0346,  0.0237,  0.0561]],\n",
      "\n",
      "         [[-0.1640,  0.0097,  0.1342],\n",
      "          [ 0.0255, -0.0511, -0.0716],\n",
      "          [-0.1100,  0.0205,  0.0216]]],\n",
      "\n",
      "\n",
      "        [[[-0.0389,  0.0102,  0.0679],\n",
      "          [ 0.0359,  0.0830, -0.1491],\n",
      "          [ 0.1260, -0.1300, -0.0153]],\n",
      "\n",
      "         [[-0.0946, -0.0965, -0.1040],\n",
      "          [ 0.0090,  0.0904,  0.1590],\n",
      "          [ 0.1180, -0.1597,  0.1555]],\n",
      "\n",
      "         [[-0.0260,  0.0207, -0.0976],\n",
      "          [-0.0525,  0.0711, -0.0511],\n",
      "          [-0.0899,  0.0119,  0.0755]],\n",
      "\n",
      "         [[ 0.0441,  0.1637, -0.1303],\n",
      "          [ 0.1034,  0.0384, -0.1419],\n",
      "          [ 0.1600,  0.0319, -0.1161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0798,  0.0988,  0.0132],\n",
      "          [ 0.0810, -0.1636,  0.0996],\n",
      "          [-0.0379,  0.0433,  0.1582]],\n",
      "\n",
      "         [[-0.1436, -0.1092, -0.0718],\n",
      "          [ 0.0768, -0.0347,  0.1154],\n",
      "          [-0.0850,  0.1057,  0.1478]],\n",
      "\n",
      "         [[ 0.0451, -0.1337,  0.0361],\n",
      "          [ 0.1170,  0.0857, -0.0100],\n",
      "          [-0.1445,  0.1088, -0.0726]],\n",
      "\n",
      "         [[-0.1381,  0.0773,  0.1400],\n",
      "          [ 0.1583,  0.0366, -0.1547],\n",
      "          [-0.0305,  0.0773,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1513, -0.0850, -0.0475],\n",
      "          [ 0.0019, -0.0261,  0.0249],\n",
      "          [ 0.0756,  0.0299,  0.1320]],\n",
      "\n",
      "         [[-0.1435,  0.0879,  0.1333],\n",
      "          [-0.1408, -0.1238, -0.1189],\n",
      "          [ 0.1580,  0.0445,  0.1471]],\n",
      "\n",
      "         [[-0.1446, -0.1364, -0.1047],\n",
      "          [-0.0881, -0.0236, -0.0295],\n",
      "          [-0.0934,  0.0408,  0.0733]],\n",
      "\n",
      "         [[-0.0991, -0.0325, -0.0805],\n",
      "          [ 0.0139, -0.1579, -0.1488],\n",
      "          [ 0.0398, -0.1449, -0.0617]]],\n",
      "\n",
      "\n",
      "        [[[-0.1090,  0.1399, -0.1253],\n",
      "          [-0.1549,  0.1320,  0.1445],\n",
      "          [ 0.0180, -0.0368, -0.0870]],\n",
      "\n",
      "         [[ 0.1379,  0.0878,  0.0761],\n",
      "          [ 0.1032, -0.0537,  0.1039],\n",
      "          [ 0.1325,  0.0757,  0.0921]],\n",
      "\n",
      "         [[-0.1363,  0.1648, -0.0822],\n",
      "          [-0.1281, -0.0285,  0.0003],\n",
      "          [-0.0942,  0.0503, -0.0420]],\n",
      "\n",
      "         [[ 0.0194, -0.0237, -0.1151],\n",
      "          [ 0.1521, -0.1559,  0.0093],\n",
      "          [ 0.0274, -0.0812, -0.1523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0311,  0.0711,  0.0515],\n",
      "          [-0.1309,  0.1466,  0.0431],\n",
      "          [ 0.0786, -0.0175,  0.1469]],\n",
      "\n",
      "         [[ 0.1227,  0.1405,  0.1267],\n",
      "          [ 0.1657, -0.0699,  0.0274],\n",
      "          [ 0.1185, -0.1070, -0.0855]],\n",
      "\n",
      "         [[ 0.0275, -0.0293, -0.0995],\n",
      "          [ 0.1093, -0.1078,  0.1532],\n",
      "          [ 0.0171,  0.0958, -0.1060]],\n",
      "\n",
      "         [[ 0.1013,  0.0228,  0.1601],\n",
      "          [ 0.0962,  0.1461,  0.0093],\n",
      "          [ 0.1347,  0.1429, -0.1522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0859, -0.1321,  0.1002],\n",
      "          [ 0.1038,  0.1370,  0.1242],\n",
      "          [-0.1526,  0.1455,  0.1296]],\n",
      "\n",
      "         [[-0.1333,  0.1242,  0.0865],\n",
      "          [ 0.0887,  0.0154,  0.0190],\n",
      "          [-0.0599, -0.1358,  0.0817]],\n",
      "\n",
      "         [[-0.1122,  0.0853, -0.0671],\n",
      "          [ 0.1066,  0.0040, -0.0420],\n",
      "          [ 0.0128, -0.1329,  0.0146]],\n",
      "\n",
      "         [[ 0.1168, -0.0891,  0.0562],\n",
      "          [ 0.1476, -0.0054,  0.0818],\n",
      "          [-0.0672, -0.1401, -0.1643]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893,  0.0464,  0.1150],\n",
      "          [ 0.0311,  0.1285,  0.0288],\n",
      "          [-0.0565, -0.0942,  0.0682]],\n",
      "\n",
      "         [[-0.0023,  0.0231,  0.0418],\n",
      "          [ 0.1206, -0.1640,  0.0080],\n",
      "          [-0.0236,  0.0674,  0.1320]],\n",
      "\n",
      "         [[-0.1453,  0.1468, -0.0836],\n",
      "          [-0.1412,  0.0860, -0.1551],\n",
      "          [ 0.1181,  0.0454,  0.1532]],\n",
      "\n",
      "         [[-0.0770,  0.0083, -0.0158],\n",
      "          [-0.0284,  0.0335, -0.0067],\n",
      "          [-0.1286,  0.1629,  0.0188]]],\n",
      "\n",
      "\n",
      "        [[[-0.1556, -0.1562,  0.0559],\n",
      "          [ 0.0663, -0.0118,  0.0820],\n",
      "          [-0.1099, -0.0277,  0.1154]],\n",
      "\n",
      "         [[-0.1092,  0.0893, -0.0582],\n",
      "          [ 0.0080,  0.1640,  0.0887],\n",
      "          [ 0.1373,  0.1052, -0.0953]],\n",
      "\n",
      "         [[ 0.0175,  0.1626, -0.0476],\n",
      "          [ 0.0459,  0.0813, -0.0941],\n",
      "          [ 0.1165,  0.0350, -0.1572]],\n",
      "\n",
      "         [[ 0.0493, -0.1505, -0.0528],\n",
      "          [ 0.0054,  0.1412, -0.0423],\n",
      "          [-0.0360,  0.1500,  0.1583]]],\n",
      "\n",
      "\n",
      "        [[[-0.0693,  0.0622,  0.1566],\n",
      "          [-0.0018, -0.1420,  0.0784],\n",
      "          [-0.0739, -0.0408,  0.1492]],\n",
      "\n",
      "         [[-0.1150,  0.1161,  0.0402],\n",
      "          [ 0.1274,  0.0941,  0.0629],\n",
      "          [ 0.0488,  0.0396,  0.1660]],\n",
      "\n",
      "         [[-0.1071,  0.0956, -0.1135],\n",
      "          [ 0.0225, -0.1317, -0.0448],\n",
      "          [ 0.0724, -0.1254,  0.0306]],\n",
      "\n",
      "         [[ 0.0681, -0.1408, -0.0381],\n",
      "          [ 0.1591,  0.0394, -0.0787],\n",
      "          [ 0.0755,  0.1179, -0.1603]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.1457, -0.0071],\n",
      "          [-0.1274, -0.1198, -0.0278],\n",
      "          [ 0.0968,  0.0864,  0.0325]],\n",
      "\n",
      "         [[-0.1607, -0.0931, -0.0062],\n",
      "          [ 0.0359, -0.1553,  0.0473],\n",
      "          [-0.1180, -0.0693,  0.1443]],\n",
      "\n",
      "         [[-0.0235, -0.1033, -0.1291],\n",
      "          [ 0.0615,  0.0980,  0.0757],\n",
      "          [-0.0489, -0.0668, -0.0824]],\n",
      "\n",
      "         [[ 0.0249,  0.0799,  0.1139],\n",
      "          [ 0.1360, -0.0858, -0.0431],\n",
      "          [-0.1233, -0.0465,  0.1503]]],\n",
      "\n",
      "\n",
      "        [[[-0.1586,  0.0867, -0.0577],\n",
      "          [ 0.1389,  0.0589, -0.1383],\n",
      "          [ 0.0326, -0.1345,  0.1604]],\n",
      "\n",
      "         [[-0.0664, -0.1348, -0.0349],\n",
      "          [-0.0002, -0.0256, -0.0162],\n",
      "          [ 0.1602,  0.1361,  0.0357]],\n",
      "\n",
      "         [[-0.0229, -0.0997,  0.1227],\n",
      "          [ 0.0545,  0.0696,  0.0240],\n",
      "          [ 0.0757,  0.0066,  0.0005]],\n",
      "\n",
      "         [[-0.1605, -0.0713, -0.0299],\n",
      "          [-0.0843,  0.0250, -0.0971],\n",
      "          [-0.1280,  0.1464, -0.1098]]]])), ('conv2.bias', tensor([ 0.1457,  0.1590,  0.1079,  0.0935, -0.1109, -0.0072, -0.0451,  0.0436,\n",
      "         0.1550,  0.0383,  0.1300,  0.0760])), ('linear1.weight', tensor([[ 1.5908e-02,  1.5656e-02,  4.6909e-02,  ..., -2.4763e-02,\n",
      "         -4.5605e-02, -6.0834e-06],\n",
      "        [ 3.8864e-02, -2.0766e-02,  5.0544e-02,  ...,  4.2443e-02,\n",
      "         -3.7946e-02, -1.1646e-02],\n",
      "        [-4.3416e-02, -2.7498e-02,  3.2318e-02,  ...,  4.7365e-02,\n",
      "          2.3062e-02,  2.8593e-02],\n",
      "        ...,\n",
      "        [ 1.8522e-02, -1.9695e-02, -5.4902e-02,  ...,  1.1535e-02,\n",
      "         -9.5615e-03,  2.9643e-02],\n",
      "        [ 2.4093e-02,  1.4357e-02, -3.5899e-02,  ..., -5.4755e-02,\n",
      "          4.6055e-02,  4.1239e-02],\n",
      "        [ 5.1365e-02, -3.0765e-02,  2.6570e-02,  ..., -1.2703e-02,\n",
      "          1.7119e-02, -2.4224e-02]])), ('linear1.bias', tensor([ 0.0252,  0.0300,  0.0190, -0.0107,  0.0018, -0.0247,  0.0454, -0.0023,\n",
      "         0.0309, -0.0527]))])\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.134775  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.248243 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.178421  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.196363 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.129720  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.163418 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.098804  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.143032 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.081110  [  128/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(net_a2_hf\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[1;32m----> 2\u001B[0m df_net_a2_hf \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet_a2_hf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m df_net_a2_hf\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNetA2HF_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minitialization\u001B[39m\u001B[38;5;124m'\u001B[39m: net_a2_hf\u001B[38;5;241m.\u001B[39mstate_dict()}, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNetA2HF_trained.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[16], line 13\u001B[0m, in \u001B[0;36mtrain_test\u001B[1;34m(device, train_dataloader, test_dataloader, net, learning_rate, epochs)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m     \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     acc, loss \u001B[38;5;241m=\u001B[39m test_loop(device, test_dataloader, net, loss_fn)\n\u001B[0;32m     15\u001B[0m     accuracies\u001B[38;5;241m.\u001B[39mappend(acc)\n",
      "Cell \u001B[1;32mIn[14], line 3\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(device, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_loop\u001B[39m(device, dataloader, model, loss_fn, optimizer):\n\u001B[0;32m      2\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[1;32m----> 3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Compute prediction and loss\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    142\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    148\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\ambienti-virtuali-python\\deep-neural-network\\Lib\\site-packages\\torchvision\\transforms\\functional.py:175\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    173\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[1;32m--> 175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(net_a2_hf.state_dict())\n",
    "df_net_a2_hf = train_test(device, train_dataloader, test_dataloader, net_a2_hf, learning_rate, epochs)\n",
    "df_net_a2_hf.to_csv('NetA2HF_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_hf.state_dict()}, 'NetA2HF_trained.pt')\n",
    "net_a2_hf.state_dict() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T10:33:43.551270Z",
     "start_time": "2024-06-18T10:33:04.168940Z"
    }
   },
   "id": "f0485488dfe86d3f",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> HT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c29cd600b2295f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_ht.state_dict())\n",
    "df_net_a2_ht = train_test(device, train_dataloader, test_dataloader, net_a2_ht, learning_rate, epochs)\n",
    "df_net_a2_ht.to_csv('NetA2HT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_ht.state_dict()}, 'NetA2HT_trained.pt')\n",
    "net_a2_ht.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T10:33:43.553935Z"
    }
   },
   "id": "bc170d6165469498",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NetA2-> DT Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ae93a49f80a7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(net_a2_dt.state_dict())\n",
    "df_net_a2_dt = train_test(device, train_dataloader, test_dataloader, net_a2_dt, learning_rate, epochs)\n",
    "df_net_a2_dt.to_csv('NetA2DT_results.csv', index=False)\n",
    "torch.save({'initialization': net_a2_dt.state_dict()}, 'NetA2HT_trained.pt')\n",
    "net_a2_dt.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T10:33:43.555911Z"
    }
   },
   "id": "d2e02ef68d3a9f05",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca126ac79b44ed2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
